{
  "metainfo" : {
    "migrationVersions" : {
      "gemBuilderVersion" : "v1"
    },
    "functions" : { },
    "codeGenConfiguration" : {
      "editableConfig" : true,
      "plibVersion" : {
        "mavenVersion" : "7.1.97"
      }
    },
    "id" : "1",
    "uri" : "pipelines/dxf_pipeline",
    "language" : "scala",
    "fabricId" : "81",
    "frontEndLanguage" : "sql",
    "mode" : "batch",
    "udfs" : {
      "language" : "scala",
      "udfs" : [ ],
      "functionPackageName" : "hd_migration_team.dxf_framework.functions",
      "sharedFunctionPackageNames" : [ ],
      "initialCode" : "import java.text.SimpleDateFormat\nimport pureconfig._\nimport pureconfig.generic.auto._\nimport scala.language.implicitConversions\n\nimplicit val columnReader = ConfigReader[String].map(col(_))\n\nimplicit def eitherReader[A,B](implicit convA: ConfigReader[A], convB: ConfigReader[B]): ConfigReader[Either[A, B]] = {\n    new ConfigReader[Either[A, B]] {\n        override def from(cur: ConfigCursor): ConfigReader.Result[Either[A, B]] = {\n            convA.from(cur) match {\n                case Left(aErr) => convB.from(cur) match {\n                        case Left(bErr) => Left(aErr ++ bErr)\n                        case Right(bType) => Right(Right[A, B](bType))\n                    }\n                case Right(aType) => Right(Left[A, B](aType))\n            }\n        }\n    }\n}\n\ncase class SingleOrList[A](value: List[A])\nimplicit def solToList[A](sol:SingleOrList[A]): List[A] = sol.value\n\nimplicit def singleOrListReader[A](implicit convA: ConfigReader[A], convL: ConfigReader[List[A]]):ConfigReader[SingleOrList[A]] = {\n    ConfigReader[Either[A, List[A]]].map {\n        case Left(a) => SingleOrList(List(a))\n        case Right(aList) => SingleOrList(aList)\n    }\n}"
    },
    "udafs" : {
      "language" : "scala",
      "code" : "package udfs\n\nimport org.apache.spark.sql.expressions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\n\n/**\n  * Here you can define your custom aggregate functions.\n  *\n  * Make sure to register your `udafs` in the register_udafs function below.\n  *\n  * Example:\n  *\n  * object GeometricMean extends UserDefinedAggregateFunction {\n  *   // This is the input fields for your aggregate function.\n  *   override def inputSchema: org.apache.spark.sql.types.StructType =\n  *     StructType(StructField(\"value\", DoubleType) :: Nil)\n  *\n  *   // This is the internal fields you keep for computing your aggregate.\n  *   override def bufferSchema: StructType = StructType(\n  *     StructField(\"count\", LongType) ::\n  *     StructField(\"product\", DoubleType) :: Nil\n  *   )\n  *\n  *   // This is the output type of your aggregatation function.\n  *   override def dataType: DataType = DoubleType\n  *\n  *   override def deterministic: Boolean = true\n  *\n  *   // This is the initial value for your buffer schema.\n  *   override def initialize(buffer: MutableAggregationBuffer): Unit = {\n  *     buffer(0) = 0L\n  *     buffer(1) = 1.0\n  *   }\n  *\n  *   // This is how to update your buffer schema given an input.\n  *   override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n  *     buffer(0) = buffer.getAs[Long](0) + 1\n  *     buffer(1) = buffer.getAs[Double](1) * input.getAs[Double](0)\n  *   }\n  *\n  *   // This is how to merge two objects with the bufferSchema type.\n  *   override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n  *     buffer1(0) = buffer1.getAs[Long](0) + buffer2.getAs[Long](0)\n  *     buffer1(1) = buffer1.getAs[Double](1) * buffer2.getAs[Double](1)\n  *   }\n  *\n  *   // This is where you output the final value, given the final value of your bufferSchema.\n  *   override def evaluate(buffer: Row): Any = {\n  *     math.pow(buffer.getDouble(1), 1.toDouble / buffer.getLong(0))\n  *   }\n  * }\n  *\n  */\n\n\nobject UDAFs {\n  /**\n    * Registers UDAFs with Spark SQL\n    */\n  def registerUDAFs(spark: SparkSession): Unit = {\n    /**\n      * Example:\n      *\n      * spark.udf.register(\"gm\", GeometricMean)\n      *\n      */\n\n\n  }\n}\n"
    },
    "configuration" : {
      "common" : {
        "type" : "record",
        "fields" : [ {
          "name" : "pipeline_name",
          "kind" : {
            "type" : "string",
            "value" : "pipeline_name"
          },
          "optional" : false,
          "comment" : "Name of the pipeline. This is used for auditing, event and consumption logs.",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "source_table",
          "kind" : {
            "type" : "string",
            "value" : "source_table"
          },
          "optional" : false,
          "comment" : "Source table name. Used for auditing purpose.",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "target_table_db",
          "kind" : {
            "type" : "string",
            "value" : "orx_idw_dm_prd"
          },
          "optional" : false,
          "comment" : "Database name for target delta table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "target_table",
          "kind" : {
            "type" : "string",
            "value" : "target_table"
          },
          "optional" : false,
          "comment" : "Name of target delta table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "source_path",
          "kind" : {
            "type" : "string",
            "value" : "srcs = [{\n    \"fmt\" : \"parquet\",\n    \"src\" : \"dbfs:/path_to_file\"\n}]"
          },
          "optional" : false,
          "comment" : "Path from where source data needs to be read",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "read_incremental_files_flag",
          "kind" : {
            "type" : "string",
            "value" : "true"
          },
          "optional" : false,
          "comment" : "Flag to identify if only incremental files need to be read from directory",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "reformat_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Json specifying target column name and expression",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "reformat_rules_join",
          "kind" : {
            "type" : "string",
            "value" : "joins = []"
          },
          "optional" : false,
          "comment" : "Config to define join conditions on base tables",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "lookups",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Option config to define lookups. Lookups in spark remain are broadcasted in memory. Avoid using it for very large tables.",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "primary_key",
          "kind" : {
            "type" : "string",
            "value" : "pk"
          },
          "optional" : false,
          "comment" : "Comma separated list of natural key of table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "length_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Length rules if any to reject the record",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "source_filter",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Optional filter condition to filter out the data at source",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "temp_output_flag",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "Setting true would make the output as temp output and would overwrite the table instead of CDC",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "dedup_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Option setting to provide deduplication rules, by default deduplication would be done on primary key if no rules i sgiven",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "final_table_schema",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "final schema of table as per delta table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "hash_cols",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Comma separated columns if any which needs to be removed from duplicate row check",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_service_col",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "SK column of the table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "optional_sk_config",
          "kind" : {
            "type" : "string",
            "value" : "sks = []"
          },
          "optional" : false,
          "comment" : "FSK config of the table which needs to be configured for making the call to SK service",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "load_ready_insert_path",
          "kind" : {
            "type" : "string",
            "value" : "dbfs:/mnt/prophecy_dev/load_ready"
          },
          "optional" : false,
          "comment" : "Path where load ready insert files are created",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "load_ready_update_path",
          "kind" : {
            "type" : "string",
            "value" : "dbfs:/mnt/prophecy_dev/load_ready"
          },
          "optional" : false,
          "comment" : "Path where load ready update files are created",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "single_load_ready_path",
          "kind" : {
            "type" : "string",
            "value" : "dbfs:/mnt/prophecy_dev/single_load_ready"
          },
          "optional" : false,
          "comment" : "Path where single load ready files are stored",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "rollup_groupby_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Group by columns for Produce block in abinitio",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "rollup_agg_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Aggregate rules for Produce block in abinitio",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "target_filter",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Rows to filter if any just before writing to target",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "orderby_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Comma separated order by rules",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "normalize_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Column which needs to be normalised",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "reject_record_threshold",
          "kind" : {
            "type" : "int",
            "value" : 0
          },
          "optional" : false,
          "comment" : "Threshold value of reject records in a run beyond which pipeline should fail. Default is 0.",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "repartition_flag",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true data at source would be reparitioned",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "repartition_cols",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Column to repartition on",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "decrypt_cols",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Columns to decrypt if any",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "enable_sk_service",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "Flag to enable/disable SK service api calls",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_service_self_join",
          "kind" : {
            "type" : "string",
            "value" : "true"
          },
          "optional" : false,
          "comment" : "Flag to enable self join on table to reduce the number of requests to SK service API ",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "generate_load_ready_files",
          "kind" : {
            "type" : "string",
            "value" : "true"
          },
          "optional" : false,
          "comment" : "Flag to enable/disable load ready files",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "create_placeholders",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "Flag to enable/disable creation of placeholder records",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_load_metadata_table",
          "kind" : {
            "type" : "string",
            "value" : "temp.incremental_load_metadata_table"
          },
          "optional" : false,
          "comment" : "Watermark table where last processed timestamp for each pipeline would be stored",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "audit_summary",
          "kind" : {
            "type" : "string",
            "value" : "temp.audit_summary"
          },
          "optional" : false,
          "comment" : "Table name where audit summary needs to be stored",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "disable_audit",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true then audit columns won't be added to data",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "disable_reject_record_summary",
          "kind" : {
            "type" : "string",
            "value" : "true"
          },
          "optional" : false,
          "comment" : "If true then reject record summary won't be populated",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "disable_audit_summary",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true then audit summary won't be populated",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "default_decimal_types",
          "kind" : {
            "type" : "string",
            "value" : "true"
          },
          "optional" : false,
          "comment" : "if false then decimal type columns wold not be converted to int/long at the source based on data",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "rec_stat_cd",
          "kind" : {
            "type" : "string",
            "value" : "1"
          },
          "optional" : false,
          "comment" : "default value for rec_stat_cd. Set to 1.",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "uid",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Constant unique identifer for datamart",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "audit_cols",
          "kind" : {
            "type" : "string",
            "value" : "insert_ts, update_ts, insert_uid, update_uid, run_id, rec_stat_cd"
          },
          "optional" : false,
          "comment" : "Audit columns list to be excluded from duplicate row check while performing CDC",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "default_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Advanced default rules in key value json",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "error_summary_table",
          "kind" : {
            "type" : "string",
            "value" : "temp.hd_error_summary"
          },
          "optional" : false,
          "comment" : "Table in which error summary would be stored",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "error_table_prefix",
          "kind" : {
            "type" : "string",
            "value" : "temp.error_"
          },
          "optional" : false,
          "comment" : "Prefix for table in which rejected records would be saved",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "src_env_sk",
          "kind" : {
            "type" : "string",
            "value" : "690"
          },
          "optional" : false,
          "comment" : "default value of src_env_sk",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_service_parallel_pool",
          "kind" : {
            "type" : "int",
            "value" : 5
          },
          "optional" : false,
          "comment" : "Parallel calls for SK service at a time",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "pk_sk_info",
          "kind" : {
            "type" : "string",
            "value" : "{}"
          },
          "optional" : false,
          "comment" : "Placeholder config containing mapping of all table for the data mart",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "disable_dedup_rules",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "if true, deduplication rules would be disabled",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_service_batch_size",
          "kind" : {
            "type" : "int",
            "value" : 30000
          },
          "optional" : false,
          "comment" : "Number of key's to be sent to SK service in one api call",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "apply_defaults",
          "kind" : {
            "type" : "string",
            "value" : "true"
          },
          "optional" : false,
          "comment" : "If false default rules as per data types won't be applied",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_service_base_url",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Base URL for sk service",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_stream_max_rows",
          "kind" : {
            "type" : "int",
            "value" : 1000
          },
          "optional" : false,
          "comment" : "Max number of rows to stream from SK service api for placeholder records creation",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "default_source_blank_nulls",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "List of columns which needs to be defaulted before reformat rules are applied",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_stream_parallel_pool",
          "kind" : {
            "type" : "int",
            "value" : 1
          },
          "optional" : false,
          "comment" : "Parallel threads to be spawned for get stream calls to SK service used to create placeholder records",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "read_from_multiple_temp_tables",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "read input data from multiple tables",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "target_write_type",
          "kind" : {
            "type" : "string",
            "value" : "scd1"
          },
          "optional" : false,
          "comment" : "Strategy to write into target. SCD0, SCD1 etc",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "join_persist_flag",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "When set to true, join's are persisted in disk after each stage. Make it true when for large tables.",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "skip_delta_synapse_write_common_dimension",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "Skip writing to delta table and synapse for common dimensions",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "list_tables_skip_delta_synapse_write_common_dimension_placeholder",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "List of tables for which delta and synapse write needs to be skipped",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "data_mart",
          "kind" : {
            "type" : "string",
            "value" : "home_delivery"
          },
          "optional" : false,
          "comment" : "Data mart name",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "event_log_table_name",
          "kind" : {
            "type" : "string",
            "value" : "temp.event_log"
          },
          "optional" : false,
          "comment" : "Table name where events would be stored for synapse consumption",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "decrypt_scope",
          "kind" : {
            "type" : "string",
            "value" : "idwshared-dbscope"
          },
          "optional" : false,
          "comment" : "Decryption secret scope",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "decrypt_EncKey",
          "kind" : {
            "type" : "string",
            "value" : "EncKey"
          },
          "optional" : false,
          "comment" : "Decryption secret key",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "decrypt_InitVec",
          "kind" : {
            "type" : "string",
            "value" : "InitVec"
          },
          "optional" : false,
          "comment" : "Decryption secret init vector",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "api_scope",
          "kind" : {
            "type" : "string",
            "value" : "idwshared-dbscope"
          },
          "optional" : false,
          "comment" : "Databricks secrets scope for SK service api key",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "api_key",
          "kind" : {
            "type" : "string",
            "value" : "databricks-user-key"
          },
          "optional" : false,
          "comment" : "Databricks secrets key for SK service api key",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "dedup_columns",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "In case source data has duplicates on primary key, you can give comma separated list on which dedup should happen",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "source_join_hint",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Join hint for source dataframe",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "custom_run_id_suffix",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Custom run_id HHmmss suffix",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "add_audit_sec_flag",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "if true it will add sec_flg audit column to final output",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "generate_only_load_ready_files",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If only load ready files needs to be generated, make this flag true",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "encrypt_cols",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Columns to encrypt if any",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "encrypt_scope",
          "kind" : {
            "type" : "string",
            "value" : "idwshared-dbscope"
          },
          "optional" : false,
          "comment" : "Encryption secret scope",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "encrypt_EncKey",
          "kind" : {
            "type" : "string",
            "value" : "EncKey"
          },
          "optional" : false,
          "comment" : "Encryption secret key",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "encrypt_InitVec",
          "kind" : {
            "type" : "string",
            "value" : "InitVec"
          },
          "optional" : false,
          "comment" : "Encryption secret init vector",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "length_rules_from_metadata_table",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true length rules would be taken from metadata table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "length_rules_metadata_table",
          "kind" : {
            "type" : "string",
            "value" : "temp.synapse_schema_metadata"
          },
          "optional" : false,
          "comment" : "Metadata table name for schema and length rules",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "post_delta_sql_update",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Run a SQL query before generation of load ready files",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "trigger_file_content_data_mart_prefix",
          "kind" : {
            "type" : "string",
            "value" : "hd"
          },
          "optional" : false,
          "comment" : "Prefix to add in trigger file content. For e.g for home delivery it's hd",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "dedup_filter",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Filter to apply to data before deduplication",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "run_id_from_filename",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "Populate run_id column in data with timestamp in file name ",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ff3_encrypt_scope",
          "kind" : {
            "type" : "string",
            "value" : "idwshared-dbscope"
          },
          "optional" : false,
          "comment" : "Databricks secrets scope for ff3 encrypt",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ff3_encrypt_key",
          "kind" : {
            "type" : "string",
            "value" : "ff3EncKey"
          },
          "optional" : false,
          "comment" : "Databricks secrets key for ff3 encrypt key",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ff3_encrypt_tweak",
          "kind" : {
            "type" : "string",
            "value" : "ff3InitVec"
          },
          "optional" : false,
          "comment" : "Databricks secrets key for ff3 encrypt tweak",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "optional_filter_rules",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Optional filter before reformat rules",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "optional_filter_rules_join",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Join if any required for optional filter",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "hex_cleanse_pattern",
          "kind" : {
            "type" : "string",
            "value" : "[\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f\\xc7\\xff\\xa0\\xa4\\xa6\\xa8\\xb4\\xb8\\xbc\\xbd\\xbe]"
          },
          "optional" : false,
          "comment" : "Hex characters to clean in source",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "hex_replace_value",
          "kind" : {
            "type" : "string",
            "value" : "  "
          },
          "optional" : false,
          "comment" : "Value to replace hex characters with",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_table_strategy",
          "kind" : {
            "type" : "string",
            "value" : "now"
          },
          "optional" : false,
          "comment" : "End time strategy to pick incremental data. Currently supported: now, today and today-n (where n is number of days).)",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_table_strategy_delta_hours",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Delta hours to add to end time strategy to pick incremental data. e.g.: for 6:30 AM we can give 06:30:00",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_table_watermark_col",
          "kind" : {
            "type" : "string",
            "value" : "UPDATE_TIMESTAMP"
          },
          "optional" : false,
          "comment" : "Column on which incremental data needs to be picked up. Can be spark sql expression as well",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_table_cutoff_date",
          "kind" : {
            "type" : "string",
            "value" : "1900-01-01"
          },
          "optional" : false,
          "comment" : "Cutoff date for table when entry not present in incremental metadata table. During initial load what data to consider",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_condition_strategy",
          "kind" : {
            "type" : "string",
            "value" : ">,<="
          },
          "optional" : false,
          "comment" : "Incremental comparison strategy for columns (incremental_col lower_cond last_load_timestamp and incremental_col upper_cond strategy_timestamp). Possible values for lower_cond, upper_cond can be  >, < or >=,<= or >=,< or >,<=",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "survivorship_flag",
          "kind" : {
            "type" : "boolean",
            "value" : true
          },
          "optional" : false,
          "comment" : "enable/disable survivorship rules",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "survivorship_override_tables",
          "kind" : {
            "type" : "string",
            "value" : "d_pharmacy_affiliation, d_pharmacy_payment_center, d_pharmacy, d_pharmacy_relationship, d_pharmacy_rltnshp_dtl, d_pharmacy_network, d_pharmacy_network_rcex1p, d_pharmacy_network_rcphdp, d_pharmacy_super_network, d_pharmacy_super_network_rcex1p, d_product"
          },
          "optional" : false,
          "comment" : "comma separated list of exception tables for survivorship rule",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "survivorship_lookup",
          "kind" : {
            "type" : "string",
            "value" : "{\r\n    \"lookup_src_envrt_id\" : {\r\n        \"fmt\" : \"parquet\",\r\n        \"key-cols\" : [\r\n            \"src_env_sk\",\r\n            \"tgt_tbl_nm\"\r\n        ],\r\n        \"src\" : \"dbfs:/mnt/shared/survivorship_lookup/lookup_src_envrt_id.parquet\",\r\n        \"val-cols\" : [\r\n            \"src_env_rnk\"\r\n        ]\r\n    }\r\n}"
          },
          "optional" : false,
          "comment" : "Lookup for survivorship rule",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "spark_configs",
          "kind" : {
            "type" : "string",
            "value" : "spark.sql.shuffle.partitions=800,spark.sql.legacy.allowUntypedScalaUDF=true,spark.sql.legacy.timeParserPolicy=LEGACY"
          },
          "optional" : false,
          "comment" : "spark configs to set at start of the pipeline",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "remove_partition_cols_in_load_ready",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "comma separated list of partition cols. eg: col1, col2",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "start_ts_query_repr",
          "kind" : {
            "type" : "string",
            "value" : "##START_TS##"
          },
          "optional" : false,
          "comment" : "start timestamp placeholder within source query",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "end_ts_query_repr",
          "kind" : {
            "type" : "string",
            "value" : "##END_TS##"
          },
          "optional" : false,
          "comment" : "end timestamp placeholder within source query",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_query_replace_strategy",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "flag for enabling/disabling incremental source query representation(start_ts_query_repr and end_ts_query_repr)",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_service_logic_from_metadata_table",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "flag to decide to enable new NON-API sk logic ",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sql_server_sk_metadata_table",
          "kind" : {
            "type" : "string",
            "value" : "metadata.TableProperties"
          },
          "optional" : false,
          "comment" : "metadata table which defines whether to use old or new SK logic per table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sql_server_lookup_table_detail",
          "kind" : {
            "type" : "string",
            "value" : "metadata.TableDetail"
          },
          "optional" : false,
          "comment" : "lookup table for tableId",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sql_server_max_sk_table",
          "kind" : {
            "type" : "string",
            "value" : "metadata.TableMaxSk"
          },
          "optional" : false,
          "comment" : "table in which max sk for new non-api SK service would be stored",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_lock_file_path",
          "kind" : {
            "type" : "string",
            "value" : "dbfs:/prophecy/non_api_sk_temp_locks/"
          },
          "optional" : false,
          "comment" : "file path at which locks would be created for new non-api sk logic",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_lock_retry_count",
          "kind" : {
            "type" : "string",
            "value" : "50"
          },
          "optional" : false,
          "comment" : "Number of times to retry new non-api sk lock",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_scope",
          "kind" : {
            "type" : "string",
            "value" : "idwshared-dbscope"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_user_key",
          "kind" : {
            "type" : "string",
            "value" : "dwUserName"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_password_key",
          "kind" : {
            "type" : "string",
            "value" : "dwPassword"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_url_key",
          "kind" : {
            "type" : "string",
            "value" : "dwServerURL"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_db_name_key",
          "kind" : {
            "type" : "string",
            "value" : "dwDatabaseName"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_fs_access_key",
          "kind" : {
            "type" : "string",
            "value" : "SAAccessKey"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_fs_access_url_key",
          "kind" : {
            "type" : "string",
            "value" : "dwStorageAccessURL"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "synapse_temp_dir_key",
          "kind" : {
            "type" : "string",
            "value" : "dwPolybaseWorkDirectory"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "max_sk_counter_reset_for_main_table",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ht2_flag",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "flag for enabling HT2 logic",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "scd2_columns",
          "kind" : {
            "type" : "string",
            "value" : "startTimestamp : column_from_dt, endTimestamp : column_thru_dt"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "pk_sk_col_from_metadata_table",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "encryption_to_uppercase",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "merge_logic_including_sk",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "merge logic includes sk column of table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "metadata_scope",
          "kind" : {
            "type" : "string",
            "value" : "idwshared-dbscope"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "metadata_dbname_key",
          "kind" : {
            "type" : "string",
            "value" : "sqlDatabase"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "metadata_url_key",
          "kind" : {
            "type" : "string",
            "value" : "dwServerURL"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "metadata_user_key",
          "kind" : {
            "type" : "string",
            "value" : "dwUserName"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "metadata_password_key",
          "kind" : {
            "type" : "string",
            "value" : "dwPassword"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "source_zoneId",
          "kind" : {
            "type" : "string",
            "value" : "America/Chicago"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "incremental_watermark_max_ts_expr",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "scd2_exclude_source_list",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "skip_placeholder_tables_delta_load_ready",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "skip delta write and load ready file generation for all FSK tables",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "enable_round_robin_partioning",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "This enables round robin partitioning for source dataset, reffering the value from spark.sql.shuffle.partitions",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "create_ht2_encrypt_file",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true then will create encrypted ht2_idw file",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ht2_load_ready_insert_path",
          "kind" : {
            "type" : "string",
            "value" : "dbfs:/mnt/prophecy_dev/ht2_load_ready"
          },
          "optional" : false,
          "comment" : "Base location where insertload ready spark part file for ecrypted ht2_idw file is generated",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ht2_single_load_ready_path",
          "kind" : {
            "type" : "string",
            "value" : "dbfs:/mnt/prophecy_dev/ht2_inbound"
          },
          "optional" : false,
          "comment" : "Base location where single load ready ecrypted ht2_idw file is generated",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ht2_extra_columns_reformat",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "if true then ht2_idw encrypted file will contain all columns which are output of reforamt rules",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ht2_encrypt_columns_meta_table",
          "kind" : {
            "type" : "string",
            "value" : "metadata.ht2_encrypt_fields"
          },
          "optional" : false,
          "comment" : "Delta table to identify column names which needs to be encrypted",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ht2_nullify_columns_meta_table",
          "kind" : {
            "type" : "string",
            "value" : "metadata.ht2_nullable_fields"
          },
          "optional" : false,
          "comment" : "Delta table to identify column names which needs to be nullified",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "optimize_write_flag",
          "kind" : {
            "type" : "string",
            "value" : "true"
          },
          "optional" : false,
          "comment" : "Flag to turn or or off optimize write flag for delta table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "skip_main_table_load_ready_files",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true then main table load ready files would not be generated. Default: false",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_table_name_override",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "If value is not None then this value would be used for locking and SK generation counter",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "default_to_decimal_type",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true then final table would be defaulted to decimal as per final table schema instead of double",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_placeholder_skip_final_override",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "If true then final_output will be updated else always will refer the input dataframe. Default: false",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "partition_column_explicit",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Used for adding partition column explicitly while performing delta merge",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "sk_service_base_url_key",
          "kind" : {
            "type" : "string",
            "value" : "skServiceBaseURL"
          },
          "optional" : false,
          "comment" : "Uses sk_service_base_url_key to fetch URL from key vault",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "update_uid_key",
          "kind" : {
            "type" : "string",
            "value" : "updateID"
          },
          "optional" : false,
          "comment" : "Key to fetch UID from key vault",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "accumulated_process_flag",
          "kind" : {
            "type" : "string",
            "value" : "false"
          },
          "optional" : false,
          "comment" : "Flag to be turned on for accumulated process for common dimensions",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "accumulated_processed_files_table",
          "kind" : {
            "type" : "string",
            "value" : "temp.accumulated_processed_files_table"
          },
          "optional" : false,
          "comment" : "Delta table where accumulated processed files list is maintained",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "survivorship_lookup_column",
          "kind" : {
            "type" : "string",
            "value" : "src_env_sk"
          },
          "optional" : false,
          "comment" : "Lookup column used for fetching src_env_rank",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "debug_flag",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "Print DFs from apply_reformat_rules onwards",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "final_output_debug_flag",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "{rint Intermediate Final Output DF from SK Service Placeholder (Very compute intensive)",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "debug_filter",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "Filter Condition to Select records While printing intermediate DFs",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "debug_col_list",
          "kind" : {
            "type" : "string",
            "value" : "None"
          },
          "optional" : false,
          "comment" : "List of Columns to Select While printing intermediate DFs",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "ph_source_column_nulls_if_not_present",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "Any columns not present in source that are present in final_table_schema would be filled in as NULL if this flag is enabled.",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "get_insert_uid_from_source_df",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "For placeholders, insert_uid should come from source, not secrets or hardcoded",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "max_retries_tbl_sk_gen",
          "kind" : {
            "type" : "int",
            "value" : 1
          },
          "optional" : false,
          "comment" : "The Maximum number of tries for regenerating SKs in case any NULLs get generated",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "retry_log_file_folder",
          "kind" : {
            "type" : "string",
            "value" : "dbfs:/mnt/prophecy_dev/sk_gen_final_output/"
          },
          "optional" : false,
          "comment" : "File path for saving final_output DF in case of NULL SK Generation",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "enable_negative_one_self_join_sk",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "flag for handling -1 as sk col present in main target table",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "enable_optional_input_file_header",
          "kind" : {
            "type" : "boolean",
            "value" : true
          },
          "optional" : false,
          "comment" : "enable/disable optional header for csv, txt and dat files",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        }, {
          "name" : "enable_read_schema_from_target_table",
          "kind" : {
            "type" : "boolean",
            "value" : false
          },
          "optional" : false,
          "comment" : "enforce final table schema while reading input files",
          "isWorkflowNodeConfiguration" : false,
          "isReferenced" : false
        } ]
      },
      "oldCommon" : {
        "type" : "record",
        "fields" : [ ]
      },
      "fabrics" : { },
      "instances" : { },
      "selected" : "default",
      "nonEditable" : [ ],
      "isSubscribedPipelineWithPipelineConfigs" : false
    },
    "sparkConf" : [ {
      "name" : "spark.sql.legacy.timeParserPolicy",
      "value" : "LEGACY"
    }, {
      "name" : "spark.sql.legacy.allowUntypedScalaUDF",
      "value" : "true"
    } ],
    "hadoopConf" : [ ],
    "codeMode" : "sparse",
    "buildSystem" : "maven",
    "externalDependencies" : [ ],
    "dependentProjectExternalDependencies" : [ ],
    "isImported" : false,
    "interimMode" : "Full",
    "interimModeEnabled" : false,
    "visualCodeInterimMode" : "Disabled",
    "recordsLimit" : {
      "enabled" : false,
      "value" : 1000
    },
    "topLevelPackage" : "io.prophecy.pipelines.dxf_framework",
    "configurationVersion" : "v2"
  },
  "connections" : [ {
    "id" : "niw0EU5buxiwIt_F9HGj8",
    "source" : "RBBqUr4dpZNVnm5tWNW8D$$NdUDYQv2OrgDNHD2VJ0cB",
    "sourcePort" : "JSu35un4kuT_9Yc2pJAJG$$X6YMlCwsxv3WQNiZmzgJr",
    "target" : "Ij_U4Axq1dHCVC-2pIWvC$$BAYL6HtkUYT8c0-H07jNw",
    "targetPort" : "scMK0Y2Z3Ed7e7j-Rjhp2$$4iEoVYfYbT9eM7cswJOdw"
  }, {
    "id" : "hBDevCjt6Gm1-8Mc5mJm2",
    "source" : "RBBqUr4dpZNVnm5tWNW8D$$NdUDYQv2OrgDNHD2VJ0cB",
    "sourcePort" : "huMv9i577INmS1q__fkI3$$KSngA5cXQhw-iN-A_l6E0",
    "target" : "uBR1aQ01XBKZLCyNC-N0I$$yXxFyuwiwRcKUmfdRF6xY",
    "targetPort" : "t72zOle2b1rnyrFF1aLUD$$OttCQZg8uMzKuYawo5Jbe"
  }, {
    "id" : "YSTIPW3hPZ-RPKTi7zEwk",
    "source" : "b5iLKRgznTuJnTToFoHFs$$10R6GPkgJMTqSZvlQno0S",
    "sourcePort" : "Mldt1wmYMOpv6nAc8aBl3$$8ZTts6QCo4sXxcPR69xCq",
    "target" : "0XsmhQ5Tbs3kFP8nEhjFc$$Zpo0i3bzAR2ZaT2BxsSFu",
    "targetPort" : "LtreNnG93P2tD1LmCJtUg$$onBfpshEgIE1aPAaM_lYW"
  }, {
    "id" : "YkkxfruVL5tLP1DD8d2qh",
    "source" : "0XsmhQ5Tbs3kFP8nEhjFc$$Zpo0i3bzAR2ZaT2BxsSFu",
    "sourcePort" : "y0SaPNgkT3YVa_4sLO6B3$$ayOdWPH96KRhBMRU4gb34",
    "target" : "tw8EKT2W2iowee5qrJ_JH$$NAdCUULfzOI44Hkq_jAYL",
    "targetPort" : "BcE7nKlVBYdQlyl0maTaH$$gB-qQZ-PobSe4gjBbXh5Q"
  }, {
    "id" : "L7C1xRKJui1Zt3MuVuNEj",
    "source" : "x6-e9o7PmYi74M-VxDFOS$$1twtczQfqc0UZllW4Pive",
    "sourcePort" : "EmpZkrGIn8sEN1ZGk0blC$$J8YVSku8UXBfXdOvGyUvt",
    "target" : "U97KPDJIcP5qKUhQ9njTG$$J84QZgcLBgqq5JdAvGh-S",
    "targetPort" : "v7gq3f8pe7axlSSJ118v6$$SbQuxBiE6cfTEa1fzIT-l"
  }, {
    "id" : "Ks22ALfRB9GwXjTUqz7hX",
    "source" : "J6fjz0hQY29uv10LLQ6hU$$CK_Nl_BZsSbAUx2nAW4Ia",
    "sourcePort" : "7MSjWnqrvS41z-ANpwKXN$$ZSIzXduIWL9VMya6IsZpB",
    "target" : "MLZ2fgQ3msTF0GOvhUqD-$$86vbeJZRRg8AtLTmckeHR",
    "targetPort" : "hM6P1_SabsHNK3ilBw48X$$SV-7ht0iCEht6GCJAf7SP"
  }, {
    "id" : "IbWul2SeTNvy7kjpibX5N",
    "source" : "MLZ2fgQ3msTF0GOvhUqD-$$86vbeJZRRg8AtLTmckeHR",
    "sourcePort" : "7FRJJQKjJ2ecBLF_Rrqmd$$uTZDNjbmEbiRgoVJwiljO",
    "target" : "b5iLKRgznTuJnTToFoHFs$$10R6GPkgJMTqSZvlQno0S",
    "targetPort" : "3tAwEeSMIk_yCnqS2V89c$$HPmwhn_vGngoazfhOrcZs"
  }, {
    "id" : "bNsJBWQLGABx6ENOSw1lh",
    "source" : "y-m6OkI8CIOyv0kGlSaK9$$Ss9JYcUtFkU6YmYdrBy9z",
    "sourcePort" : "R7SZMrxnlUmfuv5CfTykl$$BnxjBoiTjiwZ8t0vWDvmO",
    "target" : "QHWoT3ZPQuf1QukUU12ho$$z-PZk030XyvJVf9kka-AN",
    "targetPort" : "l1sRXScC1juVDd3HjiW0b$$narbM55gewR0rkZH4E_vv"
  }, {
    "id" : "niSlE-TX6RrYg86F-FSwi",
    "source" : "vKNfiHSvwz-ASGB0rmEVI$$LZPS24kdDgRr1_JaOpRjw",
    "sourcePort" : "aAdBpJZJITnhqLUVsr3Yy$$eD_sDGuwD33lYZE354ZkV",
    "target" : "RBBqUr4dpZNVnm5tWNW8D$$NdUDYQv2OrgDNHD2VJ0cB",
    "targetPort" : "fFKjdmCxFbuszWWfZTXvU$$CoidSZAlGQa1bqsxjKP5-"
  }, {
    "id" : "L3LVv008lH-bU9eLeiwa1",
    "source" : "Ij_U4Axq1dHCVC-2pIWvC$$BAYL6HtkUYT8c0-H07jNw",
    "sourcePort" : "3ldfxfgulZD-C6uw7Kuqw$$LPrcQ2xenhLINX51frUz_",
    "target" : "Hgf5DAS1R-FD_dKZeOdmd$$CrPq1G1VlHpSx5GydDGbO",
    "targetPort" : "alpnzSF7dUPbXG7Xst0jB$$LyysmNJFH-FWMfI750r9U"
  }, {
    "id" : "QFQjuiXeVIegcsMmkOKMx",
    "source" : "Hgf5DAS1R-FD_dKZeOdmd$$CrPq1G1VlHpSx5GydDGbO",
    "sourcePort" : "oeiXY8In7xKIhLXAOa7SR$$0MAFaMNZvGZ_HeuaGL9gk",
    "target" : "btQqSKid5gvTBDzik0I7b$$t_OXMCoK5vibXnH2FDC8D",
    "targetPort" : "AmER0j9sSGoN_SoLYyetV$$38aKxZbK076TlhrLvOOYm"
  }, {
    "id" : "4Jzxq71VPVZGwxb_XhPgN",
    "source" : "uBR1aQ01XBKZLCyNC-N0I$$yXxFyuwiwRcKUmfdRF6xY",
    "sourcePort" : "TVjsclY53stxPQuDMdqa7$$0X36X6IfQHdXNRmvsKFpK",
    "target" : "Hgf5DAS1R-FD_dKZeOdmd$$CrPq1G1VlHpSx5GydDGbO",
    "targetPort" : "Qd9HB2dEg82pCZuOga3rP$$o3slPbUooq96dzyaxltC7"
  }, {
    "id" : "vgJcz42qn0RQCQPP_4ENx",
    "source" : "iLCBWNwlU4rZ-zrD39NlO$$pYmIZTlRyNwK02U3cBgEU",
    "sourcePort" : "axHoGnKhBJMWoNsL09Tt6$$w9jbV3OuwMXovL9OT62yH",
    "target" : "gztr7dxOl6-dGaE-1at_i$$C8lVxU9VogSJWJJBYwdnn",
    "targetPort" : "Ij5oZsTkmcpLnUUhZY70w$$y87DSOFfSsOR0-KJGFbE-"
  }, {
    "id" : "kJr_OS-I3zza5nCMwjilw",
    "source" : "gztr7dxOl6-dGaE-1at_i$$C8lVxU9VogSJWJJBYwdnn",
    "sourcePort" : "X__5bzL0HmZxu0HGq7eF3$$So6w5lsV8BsSq1lwQaI03",
    "target" : "SP3GIx64LeRuO7vfX5COW$$zswxLvkA2uhHN3zr6zrKE",
    "targetPort" : "Mwjm-Lp3o9_XY1sYIUfPJ$$0e0wh5rpsnlMa4ejbXbMd"
  }, {
    "id" : "m6udqrOdTzIWuhTHUidXd",
    "source" : "U97KPDJIcP5qKUhQ9njTG$$J84QZgcLBgqq5JdAvGh-S",
    "sourcePort" : "nAecmu2Y8B45zWpozYM2e$$2yZ_psvZ5ivzKmWT-tWUw",
    "target" : "sLG-1tclvkxeUSOB_ZyrA$$5e57JAQoONa1dWQ6TtbhJ",
    "targetPort" : "iNJgROMqIBiZb1KSqNYDf$$zLubz5RFwq7iQmui9pWkk"
  }, {
    "id" : "qy3-I9jKrOAE4CVlDsvT0",
    "source" : "sLG-1tclvkxeUSOB_ZyrA$$5e57JAQoONa1dWQ6TtbhJ",
    "sourcePort" : "YY_5_Kq6tWxVKYlpDqE_e$$L4JjlXOd8DBkfCG6hAk7J",
    "target" : "IVOE4eqMDCX0iAK57gJbn$$zeEdlgdzuuniA3bfd6F6A",
    "targetPort" : "TjZsZpAiHoIkvV47qPbOB$$mfX_L5g0sgDmKG-Ygy6z9"
  }, {
    "id" : "e21DVa6LF23IbQXnBeJ3Q",
    "source" : "VMN3zOsqy8XcvcScQl2KW$$tIbzBsKedXk0HnTmrZHWm",
    "sourcePort" : "Kr52eTVRp2Wj-qyMV9EZH$$pHCc2SwO5DP44eppo0hWb",
    "target" : "vKNfiHSvwz-ASGB0rmEVI$$LZPS24kdDgRr1_JaOpRjw",
    "targetPort" : "w5AdwZt6673vq5yWaH6AN$$UeK7vbFBKiyE0aUgkIJgN"
  }, {
    "id" : "T6uMe77EB0glPkWgHbO2x$$j4K3vVLQITpnZ2Rke0U4U",
    "source" : "MZXjVgsTlFrCOsgbrh6tI$$bgaHPzaXnVAHqudoIFwIt",
    "sourcePort" : "2sfusYhyW5DmWpOSq0hFv$$fdWI-MNlkWZn49hP7PXjj",
    "target" : "bQ1XrnLqKirtJc3djEyNV$$mRnIsUCOUdF2KfuRJI0XC",
    "targetPort" : "-fO7GCzOJHWgRek5RVhaW$$vfvw9OtRsr5YXpOgkI0nz"
  }, {
    "id" : "SoUVRdQVP3h8eXUV1ldoo$$LeHSBD9W-xaa2C8dsES-E",
    "source" : "SP3GIx64LeRuO7vfX5COW$$zswxLvkA2uhHN3zr6zrKE",
    "sourcePort" : "v6k1wg79g1kZlq64irXUB$$-njhhKtYpyz3mTFOaKAHD",
    "target" : "MZXjVgsTlFrCOsgbrh6tI$$bgaHPzaXnVAHqudoIFwIt",
    "targetPort" : "9U8wFH8yV9WsJdyO-qR0S$$3t1RiYHBrk2DRww-2XjTR"
  }, {
    "id" : "a4QVeUZJKSOA6La7LQaqO",
    "source" : "WPwvwUYSyzyGJwGrxog7E$$dila5inO1kk-iAWbfki_0",
    "sourcePort" : "D05MewfEUW-_zQJPLI796$$51yytXgldVB6xUX-l0A0r",
    "target" : "jZeb21Pxoz4Lv3giVtLPX$$zkvp8Lr4YXAN4xK-fN_To",
    "targetPort" : "3PtkE6nQ3BJeMPNI_LNlv$$nfWPRuH7DnBFfOURAbeNx"
  }, {
    "id" : "EXMagIDD003QNZLz-fUZF",
    "source" : "JB32GmgXxUskZ_V-wAb81$$faMNpiQq5PZsq8jK-xAbo",
    "sourcePort" : "nuLIqeUXh4pByRcgEuuO_$$kjPt90BDZpUDYuuTPg7wE",
    "target" : "w1ePGHe-uHbRy_LYBrwd6$$5s455UdOQK7Vg-Nu7ikG5",
    "targetPort" : "qZgrKbEc5IiKlvzeOqQXn$$dQrxno8sqqyko9eLe13FX"
  }, {
    "id" : "M0lbVl_sQZS9NzXYxIAFM",
    "source" : "-HWhIbTJNxycZcwI9Sq3w$$SJAnEcpJS0Q5NicBUqB7i",
    "sourcePort" : "uQN5Yif5jsAbvJLCqTiDd$$bgDUlmyNmNfxFtcBzgrhY",
    "target" : "Ga5tBdfNKuRKxtlMvtMkO$$dvthpkrQFi1kYd2YNtkET",
    "targetPort" : "awF0Yf57r8e6_EKB_p2xy$$UACVQwkS7qDkKXnsE0E0V"
  }, {
    "id" : "rJ-9a-PjL9yW2R3NGsXno",
    "source" : "tw8EKT2W2iowee5qrJ_JH$$NAdCUULfzOI44Hkq_jAYL",
    "sourcePort" : "d5DR6BWsfXSRiM2NF07RD$$PvSPv3sfyXEAeHXxyl6Tc",
    "target" : "O_ToGlviXaR1cWzi7yApg$$WOHgnFUDO0ao0YqcmToJd",
    "targetPort" : "E04uKZEjgrvasB6AULnNS$$U4TK3tkLAsbbGzfD37CFY"
  }, {
    "id" : "13awTwVAvXVv6BxluXvWL",
    "source" : "O_ToGlviXaR1cWzi7yApg$$WOHgnFUDO0ao0YqcmToJd",
    "sourcePort" : "tnAiG_eIFfzBuCehhJVWb$$3mV4034A2tWaprUruZz_K",
    "target" : "x6-e9o7PmYi74M-VxDFOS$$1twtczQfqc0UZllW4Pive",
    "targetPort" : "DamBQ0RtEhBQNhYA2sEW7$$gLySIy4zuro9EWaXJDUOU"
  }, {
    "id" : "ZheKQBlc6OHKKW6S_Lopt",
    "source" : "YNgpozNBwPuffrvW5YU9j$$raZHITPDu9IJ8awefPWUu",
    "sourcePort" : "6V3etwaq0Cv1FnkcU95dP$$StmFs5w0QXwO3f6Ghb7sl",
    "target" : "J6fjz0hQY29uv10LLQ6hU$$CK_Nl_BZsSbAUx2nAW4Ia",
    "targetPort" : "F9LCxXb0Zu1m06NR2LRk0$$NaoHMtfnd1cOpFHdRWhu6"
  }, {
    "id" : "JaIvkpeDBKadRMx1o6ab6",
    "source" : "EnfQ7uWDAR9PQgbLcfbip$$eizUFBt06Keq1uCYDN22P",
    "sourcePort" : "F86o336jAFqMEQvk3jmNk$$bXHqFX9yd37X61-UUZBIJ",
    "target" : "19Es0MHx1pfrBprQj5hG7$$S64p_J0adIrQbxOJnJ8fO",
    "targetPort" : "oRV4BTz9DbhX04lp0tF_T$$H5dUoTz98O9ktsWvnlPON"
  }, {
    "id" : "mlIDZEFl1xAJ0OFzKri6m",
    "source" : "QHWoT3ZPQuf1QukUU12ho$$z-PZk030XyvJVf9kka-AN",
    "sourcePort" : "jBik_iDUudAn_JBfLDK0h$$r4QTQrOFyOQfsVOOf_OJC",
    "target" : "i2C9DDNannmhRMhfhU5AE$$oKDJLFTWId6o45uYOIzFi",
    "targetPort" : "xuRCY-BM_mV9-nt24bGwR$$DjkwCUciWW25gZg5hGcR1"
  }, {
    "id" : "ToJH5PkhXJFWsL0PGBgt3",
    "source" : "i2C9DDNannmhRMhfhU5AE$$oKDJLFTWId6o45uYOIzFi",
    "sourcePort" : "CetsR6R0l2xe5VRMWzIYJ$$68lwGJA9tet9nOB3RRJHq",
    "target" : "iLCBWNwlU4rZ-zrD39NlO$$pYmIZTlRyNwK02U3cBgEU",
    "targetPort" : "J6pukRNozYFXrh6hu11L7$$zaaOECgzJt-rray0ckm47"
  }, {
    "id" : "g2NrkVowJUB_vs7eV1IVF",
    "source" : "bQ1XrnLqKirtJc3djEyNV$$mRnIsUCOUdF2KfuRJI0XC",
    "sourcePort" : "6kc7H7P7v6cH48fcVKqKi$$FRAUe5WSyGZ3-7bgZc5ld",
    "target" : "VMN3zOsqy8XcvcScQl2KW$$tIbzBsKedXk0HnTmrZHWm",
    "targetPort" : "3Z-gE7d3EJP4IlhuiWKEr$$Zu-94wvFGyI0fA1_5k25x"
  }, {
    "id" : "GnCjZI9fbQInan4nKr263$$Qyi2w1xbh0cveul43-eiU",
    "source" : "btQqSKid5gvTBDzik0I7b$$t_OXMCoK5vibXnH2FDC8D",
    "sourcePort" : "AfBsMj_YHcRQrj5_qm07y$$TXjpzCUNFnU4EYSy4Onlw",
    "target" : "nJN6hRRk2RrSF8R-1W3i1$$EeMkGvgQAf4igaTg_5TtH",
    "targetPort" : "5qU7PDYnUsXbGUJmhTyhJ$$vV4g5y0Fn-NF5pjUZ4-8D"
  }, {
    "id" : "MeaFY7pnKjgWj-SJ1_oro",
    "source" : "nJN6hRRk2RrSF8R-1W3i1$$EeMkGvgQAf4igaTg_5TtH",
    "sourcePort" : "5cuMgJ6q55jLNwtOtD_Wm$$JH1FgU7sc07_D_DPcz3H7",
    "target" : "5I9w6UESsmDlhfmUL07UY$$mWCyl0Zqbe_HbwHuPg6F2",
    "targetPort" : "u9KR5hzmm6pWnQWNQ0vw0$$mJSG7SX8fv4v1apaWi7Ly"
  }, {
    "id" : "sOTAYXqmtuOYxQ71VdGvt",
    "source" : "nJN6hRRk2RrSF8R-1W3i1$$EeMkGvgQAf4igaTg_5TtH",
    "sourcePort" : "5cuMgJ6q55jLNwtOtD_Wm$$JH1FgU7sc07_D_DPcz3H7",
    "target" : "1w2Y-FhwG2D2tTokmhCdC$$rxXAwHre2rqCfbqgrQA33",
    "targetPort" : "NAnmHEhSKy-Kmd02t1ix2$$r7De1FHtI2JLdA5Qkp8I1"
  }, {
    "id" : "kMc02zCq8_XFiNuJCo4Wz$$2VloJC6-KzDJICwuwPnDH",
    "source" : "8TQl4BrnklSyziDM1pDq2$$PJh89RDre0QIipfFMy8HU",
    "sourcePort" : "5Jsn5eN4EgAVCoL1Er62w$$w4D3a1k7yp86S2DFj4jwJ",
    "target" : "YNgpozNBwPuffrvW5YU9j$$raZHITPDu9IJ8awefPWUu",
    "targetPort" : "p4ooTouM-aAbjrxGhHKWS$$4zMJAtvPm4L-wKlLzlUWG"
  }, {
    "id" : "i1ydAxzAtlhNxSO_HFDhN",
    "source" : "qCyo_ZpxGMT3wM4FUCkB8$$6S2ukC40_BY5laPMcQtRv",
    "sourcePort" : "wdW6ALIS3lvrkMvsH4ILh$$vMH750-tJ2F1tJXizUWIC",
    "target" : "8TQl4BrnklSyziDM1pDq2$$PJh89RDre0QIipfFMy8HU",
    "targetPort" : "vk3mHrosPWyw6g8Y4uDKR$$2bNECW62AENwSllT1JBwL"
  }, {
    "id" : "yfwXoXw6CxMrxIHigIVI0$$EAm-VPQpMZ--dQWThBaiB",
    "source" : "w2MZMOsj39JbiRhPXWShH$$8PJNM3348yL9FIrwlHOzn",
    "sourcePort" : "wtOaygbQx_I-1K3OJz20d$$JO7l1k_pO3TPQeOQa-gRF",
    "target" : "qCyo_ZpxGMT3wM4FUCkB8$$6S2ukC40_BY5laPMcQtRv",
    "targetPort" : "Yz_3iARfXg-AZTNaFbI-l$$-sEP0R9kFpBPw7N4BbHFk"
  }, {
    "id" : "0KDmd16IFWjiP7PIGzT3I",
    "source" : "jZeb21Pxoz4Lv3giVtLPX$$zkvp8Lr4YXAN4xK-fN_To",
    "sourcePort" : "2bU46E_0HdQ98jwTLfCHo$$0uuvtER1E4SGyYM8UKeqz",
    "target" : "w2MZMOsj39JbiRhPXWShH$$8PJNM3348yL9FIrwlHOzn",
    "targetPort" : "Nbt8gOmE6jSSW7MqNOMcF$$LfdD-GNyE8RjtedVBDT7b"
  }, {
    "id" : "aq8tZYRt8sYRSx1jQDWt4",
    "source" : "19Es0MHx1pfrBprQj5hG7$$S64p_J0adIrQbxOJnJ8fO",
    "sourcePort" : "efNyA9RzhHjqVAh8HNTDA$$ggq_eo_2HZzg8ZJhZ5njv",
    "target" : "-HWhIbTJNxycZcwI9Sq3w$$SJAnEcpJS0Q5NicBUqB7i",
    "targetPort" : "l6hjs83kiCN1VK2MHw6b4$$Xev6hnuVBNicgBBQF2hRe"
  }, {
    "id" : "sGm-i_IjRD1v7u9T7WfRc",
    "source" : "Ga5tBdfNKuRKxtlMvtMkO$$dvthpkrQFi1kYd2YNtkET",
    "sourcePort" : "4GEbpr4gEFd-MvrzVvSu5$$GnuKRlJI-N_fm-Dmx9hVX",
    "target" : "JB32GmgXxUskZ_V-wAb81$$faMNpiQq5PZsq8jK-xAbo",
    "targetPort" : "oFTVc2YT2-2LsJX7BbRI9$$2LIvIYIHIb0jWoG-NxQbd"
  }, {
    "id" : "Kmv_HxMUemKvrIftuPI_-",
    "source" : "JB32GmgXxUskZ_V-wAb81$$faMNpiQq5PZsq8jK-xAbo",
    "sourcePort" : "yjmy41YhFjPBABhKekCmW$$3ZgvLIymbCGJHArPsmdmy",
    "target" : "y-m6OkI8CIOyv0kGlSaK9$$Ss9JYcUtFkU6YmYdrBy9z",
    "targetPort" : "qlVSwiDXeHFWhSokqkn3a$$QnWNsRGJyWfBLUdzsIzOI"
  }, {
    "id" : "pWB90p6OwpUNo0YCxenBL",
    "source" : "IVOE4eqMDCX0iAK57gJbn$$zeEdlgdzuuniA3bfd6F6A",
    "sourcePort" : "_sN14kMZmsrJM_dku2hkr$$2Xewu3mRmztd2gX7oeHWG",
    "target" : "7X1ScPt9O6DCx6AkZIMBC$$nT-3k1VRc5kl47UyaLcuW",
    "targetPort" : "nE996SsbLqum0bNawuTQv$$lOkGnjnJTJvtfuVo5JBtH"
  }, {
    "id" : "iC0Hc8OlyHyvNjmWNcQRb",
    "source" : "7X1ScPt9O6DCx6AkZIMBC$$nT-3k1VRc5kl47UyaLcuW",
    "sourcePort" : "sa0b-KEYC6KL959C-hlFS$$eSUl-evVSpu2aRCR0wYj7",
    "target" : "EnfQ7uWDAR9PQgbLcfbip$$eizUFBt06Keq1uCYDN22P",
    "targetPort" : "9yN2ElCio63iDJy3_8pak$$Ipx_CnwBZjJ_jy9-CzsRv"
  } ],
  "processes" : {
    "U97KPDJIcP5qKUhQ9njTG$$J84QZgcLBgqq5JdAvGh-S" : {
      "id" : "U97KPDJIcP5qKUhQ9njTG$$J84QZgcLBgqq5JdAvGh-S",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_order_by",
        "slug" : "optional_order_by",
        "x" : 2620,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "v7gq3f8pe7axlSSJ118v6$$SbQuxBiE6cfTEa1fzIT-l",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "nAecmu2Y8B45zWpozYM2e$$2yZ_psvZ5ivzKmWT-tWUw",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"######################################\")\r\nprintln(\"#####Step name: optional_order_by#####\")\r\nprintln(\"######################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n\r\nval out0 = if (Config.orderby_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\") {\r\n  import scala.collection.mutable.ListBuffer\r\n  var outputList = new ListBuffer[org.apache.spark.sql.Column]()\r\n  val order_by_rules = Config.orderby_rules.split(',').map(x => x.trim())\r\n\r\n  order_by_rules.foreach { rule =>\r\n    if (rule.toLowerCase().contains(\" asc\")) {\r\n      if (rule.toLowerCase().contains(\" asc nulls\")) {\r\n        if (rule.toLowerCase().contains(\"nulls first\")) {\r\n          outputList += asc_nulls_first(rule.split(\" \")(0).trim())\r\n        } else {\r\n          outputList += asc_nulls_last(rule.split(\" \")(0).trim())\r\n        }\r\n      } else {\r\n        outputList += asc(rule.split(\" \")(0).trim())\r\n\r\n      }\r\n    } else {\r\n      if (rule.toLowerCase().contains(\" desc nulls\")) {\r\n        if (rule.toLowerCase().contains(\"nulls first\")) {\r\n          outputList += desc_nulls_first(rule.split(\" \")(0).trim())\r\n        } else {\r\n          outputList += desc_nulls_last(rule.split(\" \")(0).trim())\r\n        }\r\n      } else {\r\n        outputList += desc(rule.split(\" \")(0).trim())\r\n      }\r\n\r\n    }\r\n  }\r\n\r\n  in0.orderBy(outputList: _*)\r\n} else {\r\n  in0\r\n}\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "i2C9DDNannmhRMhfhU5AE$$oKDJLFTWId6o45uYOIzFi" : {
      "id" : "i2C9DDNannmhRMhfhU5AE$$oKDJLFTWId6o45uYOIzFi",
      "component" : "Script",
      "metadata" : {
        "label" : "sk_service_placeholder",
        "slug" : "sk_service_placeholder",
        "x" : 4820,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "xuRCY-BM_mV9-nt24bGwR$$DjkwCUciWW25gZg5hGcR1",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "CetsR6R0l2xe5VRMWzIYJ$$68lwGJA9tet9nOB3RRJHq",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import org.apache.commons.codec.binary.Hex\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs._\nimport org.apache.hadoop.io.IOUtils\nimport org.apache.http.client.methods.HttpPost\nimport org.apache.http.entity.StringEntity\nimport org.apache.http.impl.client.HttpClientBuilder\nimport org.apache.http.util.EntityUtils\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.storage.StorageLevel\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.jackson.Serialization.write\nimport pureconfig._\nimport pureconfig.generic.auto._\nimport java.io.{IOException, _}\nimport java.security.MessageDigest\nimport java.time._\nimport java.time.format._\nimport java.util\nimport java.sql.DriverManager\nimport java.util.Properties\nimport javax.crypto.Cipher\nimport javax.crypto.spec.{IvParameterSpec, SecretKeySpec}\nimport scala.collection.mutable.ListBuffer\nimport scala.language.implicitConversions\nimport scala.util.Try\nimport spark.implicits._\nimport com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\nimplicit val formats = DefaultFormats\n\nprintln(\"#####Step name: create sk and create placeholders#####\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nspark.conf.set(\"main_table_api_type\", \"API\")\nspark.conf.set(\"main_table_max_sk\", \"0\")\nspark.conf.set(\"main_table_new_sk_count\", \"0\")\nspark.conf.set(\"main_table_table_id\", \"-1\")\nspark.conf.set(\"main_table_max_sk_update_sql\", \"\")\nval isPKSet = spark.conf.getAll.contains(\"primary_key\")\nval prim_key_columns = if (isPKSet) {\n  spark.conf\n    .get(\"primary_key\")\n    .split(\",\")\n    .map(x => x.trim())\n} else {\n  Config.primary_key\n    .split(\",\")\n    .map(x => x.trim())\n}\nval isSKSet = spark.conf.getAll.contains(\"sk_service_col\")\nval sk_service_col = if (isSKSet) {\n  spark.conf\n    .get(\"sk_service_col\")\n    .toLowerCase\n    .trim\n} else {\n  Config.sk_service_col.toLowerCase.trim\n}\n\nConfig.sk_table_name_override = if (Config.sk_table_name_override != \"None\"){\n  println(\"main table name override for sk service: \" + Config.sk_table_name_override)\n  Config.sk_table_name_override\n} else {\n  Config.target_table\n}\n\nval main_table_name = Config.sk_table_name_override\nval sk_service_base_url = Config.sk_service_base_url.toLowerCase match {\n    case \"none\"  => dbutils.secrets.get(scope = Config.api_scope, key = Config.sk_service_base_url_key)\n    case _ => Config.sk_service_base_url\n  }\n\nval out0 =\n  if (\n    Config.enable_sk_service != \"false\" && spark.conf.get(\n      \"new_data_flag\"\n    ) == \"true\"\n  ) {\n\n    val api_header_key =\n      dbutils.secrets.get(scope = Config.api_scope, key = Config.api_key)\n\n    val current_ts_val = to_timestamp(\n      from_utc_timestamp(current_timestamp(), \"America/Chicago\").cast(\"string\"),\n      \"yyyy-MM-dd HH:mm:ss\"\n    )\n\n    val skip_placeholder_tables =\n      Config.list_tables_skip_delta_synapse_write_common_dimension_placeholder\n        .split(\",\")\n        .map(x => x.trim())\n\n    val run_id = spark.conf.get(\"run_id\")\n\n    var run_id_for_data = run_id\n    if (Config.custom_run_id_suffix != \"None\") {\n      run_id_for_data = run_id.substring(0, 8) + Config.custom_run_id_suffix\n    }\n\n    println(\"run id for data: \" + run_id_for_data)\n\n    val load_ready_insert_path =\n      Config.load_ready_insert_path\n    val baseFilePath =\n      Config.single_load_ready_path\n\n    val hadoopConfig = new Configuration()\n    val hdfs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n\n    case class PkSkGen(pks_sks: String, sk_value: String, p_flag: String)\n\n    case class SKDef(\n        tableName: String,\n        skCol: String,\n        nkCol: List[String]\n    )\n\n    case class SKService(\n        sks: List[SKDef]\n    )\n\n    case class PkSKDef(\n        fmt: String,\n        src: String,\n        pkCols: List[String],\n        skCols: String,\n        synapseTable: Option[String] = None,\n        encryptCols: Option[List[String]] = None,\n        decryptCols: Option[List[String]] = None,\n        dropPartitionCols: Option[List[String]] = None,\n        orderByDedup: Option[String]=None,\n        defaultToDecimalType: Option[String]=None\n    )\n\n    type PkSKDefs = Map[String, PkSKDef]\n\n    val pkSKDefConfig =\n      ConfigSource.string(Config.pk_sk_info).loadOrThrow[PkSKDefs]\n\n    def jsonStrToMap(jsonStr: String): Map[String, Any] = {\n      parse(jsonStr).extract[Map[String, Any]]\n    }\n\n    def get_sk_values_response(\n        pk_to_find: List[String],\n        post_url: String\n    ): String = {\n      val req =\n        Map(\n          \"client_id\" -> Config.pipeline_name,\n          \"request_id\" -> (Config.pipeline_name + \"_\" + java.time.LocalDateTime.now.toString),\n          \"pks\" -> pk_to_find,\n          \"detailed\" -> \"true\"\n        )\n      val reqAsJson = write(req)\n\n      val post = new HttpPost(\n        post_url\n      )\n\n      post.setHeader(\"Content-type\", \"application/json\")\n      post.setHeader(\"Ocp-Apim-Subscription-Key\", api_header_key)\n      post.setEntity(new StringEntity(reqAsJson, \"UTF-8\"))\n\n      val response = (HttpClientBuilder.create().build()).execute(post)\n\n      val status_code = response.getStatusLine().getStatusCode()\n\n      if (status_code == 206) {\n        println(\n          \"Partial results got returned. Please connect with admin and check SK service memory. It should be less than 70 percent.\"\n        )\n      }\n\n      if (status_code == 429) {\n        Thread.sleep(60000)\n        get_sk_values_response(pk_to_find, post_url)\n      }\n\n      val entity = response.getEntity\n\n      var response_str = EntityUtils.toString(entity, \"UTF-8\")\n      response_str\n    }\n\n    def get_sk_values(response: String, post_url: String): List[PkSkGen] = {\n\n      var a = \"\"\n      var b = \"\"\n      var c = \"\"\n      var response_str = response\n      var pk_sk_values = jsonStrToMap(response_str)\n        .get(\"pks_sks\")\n        .get\n        .asInstanceOf[Map[String, Any]]\n        .map {\n          case (k, v) => {\n            a = k\n            b = v\n              .asInstanceOf[Map[String, Any]]\n              .get(\"value\")\n              .get\n              .asInstanceOf[scala.math.BigInt]\n              .toString\n            c = v\n              .asInstanceOf[Map[String, Any]]\n              .get(\"generated\")\n              .get\n              .asInstanceOf[Boolean]\n              .toString\n            PkSkGen(a, b, c)\n          }\n        }\n\n      pk_sk_values.toList\n    }\n\n    def writeTextFile(\n        filePath: String,\n        filename: String,\n        s: String\n    ): Unit = {\n\n      val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n      val file = new Path(filePath + \"/\" + filename)\n      val dataOutputStream: FSDataOutputStream = fs.create(file)\n      val bw: BufferedWriter = new BufferedWriter(\n        new OutputStreamWriter(dataOutputStream, \"UTF-8\")\n      )\n      bw.write(s)\n      bw.close()\n\n      val crcPath = new Path(filePath + \"/.\" + filename + \".crc\")\n      if (Try(fs.exists(crcPath)).isSuccess) {\n        fs.delete(crcPath, true)\n      }\n    }\n\n    def copyMerge(\n        srcFS: FileSystem,\n        srcDir: String,\n        dstFS: FileSystem,\n        dstFile: Path,\n        deleteSource: Boolean,\n        conf: Configuration,\n        lazy_write: Boolean\n    ): Boolean = {\n      if (dstFS.exists(dstFile))\n        throw new IOException(s\"Target $dstFile already exists\")\n\n      val new_path_var = if (lazy_write) {\n        srcDir + \"_lazy/\"\n      } else {\n        srcDir\n      }\n      val new_path = new Path(new_path_var)\n      // Source path is expected to be a directory:\n      if (srcFS.getFileStatus(new_path).isDirectory()) {\n\n        val outputFile = dstFS.create(dstFile)\n        Try {\n          srcFS\n            .listStatus(new_path)\n            .sortBy(_.getPath.getName)\n            .filter(_.getPath.getName.endsWith(\".parquet\"))\n            .collect {\n              case status if status.isFile() =>\n                val inputFile = srcFS.open(status.getPath())\n                Try(IOUtils.copyBytes(inputFile, outputFile, conf, false))\n                inputFile.close()\n            }\n        }\n        outputFile.close()\n\n        if (deleteSource) {\n          if (lazy_write) {\n            srcFS.delete(new Path(srcDir), true)\n          }\n          srcFS.delete(new_path, true)\n        } else true\n      } else false\n    }\n\n    var encrypt_key = \"Secret not defined\"\n    var encrypt_iv = \"Secret not defined\"\n    var decrypt_key = \"Secret not defined\"\n    var decrypt_iv = \"Secret not defined\"\n\n    try {\n\n      // Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\n      import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n      // loading db secrets\n      encrypt_key = dbutils.secrets.get(\n        scope = Config.encrypt_scope,\n        key = Config.encrypt_EncKey\n      )\n\n      encrypt_iv = dbutils.secrets.get(\n        scope = Config.encrypt_scope,\n        key = Config.encrypt_InitVec\n      )\n\n      decrypt_key = dbutils.secrets.get(\n        scope = Config.decrypt_scope,\n        key = Config.decrypt_EncKey\n      )\n\n      decrypt_iv = dbutils.secrets.get(\n        scope = Config.decrypt_scope,\n        key = Config.decrypt_InitVec\n      )\n      println(\"Found secrets successfully for encrypt decrypt UDFs.\")\n    } catch {\n      case e: Exception => {\n        println(\n          \"Please define databricks secrets for encrypt_key, decrypt_key, envrypt_iv and decrypt_iv on your cluster to use encrypt decrypt functions as udf\"\n        )\n      }\n    }\n\n    def encrypt(key: String, ivString: String, plainValue: String): String = {\n      if (plainValue != null) {\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n        cipher.init(Cipher.ENCRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\n        var encrypted_str =\n          Hex.encodeHexString(cipher.doFinal(plainValue.getBytes(\"UTF-8\")))\n        encrypted_str = encrypted_str\n        return encrypted_str\n      } else {\n        null\n      }\n    }\n\n    def decrypt(\n        key: String,\n        ivString: String,\n        encryptedValue: String\n    ): String = {\n      if (encryptedValue != null) {\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n        cipher.init(Cipher.DECRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\n        new String(cipher.doFinal(Hex.decodeHex(encryptedValue.toCharArray())))\n      } else {\n        null\n      }\n    }\n\n    def keyToSpec(key: String): SecretKeySpec = {\n      var keyBytes: Array[Byte] = (key).getBytes(\"UTF-8\")\n      // val sha: MessageDigest = MessageDigest.getInstance(\"MD5\")\n      keyBytes = Hex.decodeHex(key)\n      keyBytes = util.Arrays.copyOf(keyBytes, 16)\n      new SecretKeySpec(keyBytes, \"AES\")\n    }\n\n    def getIVSpec(IVString: String) = {\n      new IvParameterSpec(IVString.getBytes() ++ Array.fill[Byte](16-IVString.length)(0x00.toByte))\n\n    }\n\n    val encryptUDF = udf(encrypt _)\n    val decryptUDF = udf(decrypt _)\n    spark.udf.register(\"aes_encrypt_udf\", encryptUDF)\n    spark.udf.register(\"aes_decrypt_udf\", decryptUDF)\n\n    in0.persist(StorageLevel.DISK_ONLY).count()\n    var res = in0\n    var final_output = res\n    var final_output_main_tbl = res\n\n    var sksConfig =\n      ConfigSource.string(Config.optional_sk_config).loadOrThrow[SKService]\n\n    var maxCounter = Config.max_retries_tbl_sk_gen //Defaults to 1\n    var mainTableBreakFlag = false\n    var tblRetryCounter = 0\n    val mainTableSkListBuffer = ListBuffer[SKDef]() \n    mainTableSkListBuffer ++= sksConfig.sks\n\n    if (sk_service_col != \"none\") {\n      for (i <- 0 until maxCounter){\n        mainTableSkListBuffer += SKDef(\n              tableName = main_table_name,\n              skCol = sk_service_col,\n              nkCol = prim_key_columns.toList\n            )\n      }\n      \n    }\n\n    sksConfig = SKService(mainTableSkListBuffer.toList)\n\n\n    println(\"sk and placeholder generate process started\")\n    println(\n      \"start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    )\n\n    def identifyLastOccurrences(arr: Array[String]): Map[String, Int] = {\n      var lastOccurrenceIndices: Map[String, Int] = Map.empty\n\n      arr.indices.reverse.foreach { i =>\n        lastOccurrenceIndices.get(arr(i)) match {\n          case None    => lastOccurrenceIndices += (arr(i) -> i)\n          case Some(_) => // Do nothing, already encountered\n        }\n      }\n\n      lastOccurrenceIndices\n    }\n\n    def convertDataFrameToMap(df: DataFrame): Map[String, Map[String, Any]] = {\n      val columns = df.columns\n      val keyColumn = columns.head\n      val valueColumns = columns.tail\n\n      df.rdd\n        .map { row =>\n          val key = row.getAs[String](keyColumn)\n          val values =\n            valueColumns.map(column => column -> row.getAs[Any](column)).toMap\n          key.toUpperCase -> values\n        }\n        .collectAsMap()\n        .toMap\n    }\n\n    // import scala.collection.parallel._\n    // val skServiceParallel: ParSeq[SKDef] = sksConfig.sks.toSeq.par\n    // val forkJoinPool =\n    //   new java.util.concurrent.ForkJoinPool(1) // Config.sk_service_parallel_pool\n    // skServiceParallel.tasksupport = new ForkJoinTaskSupport(forkJoinPool)\n\n    // code to join and collect dataframe\n\n    val create_sk_table_list = sksConfig.sks.map(x => x.tableName).toArray\n\n    val lastOccurrencesMap = identifyLastOccurrences(create_sk_table_list)\n\n\n    val metadata_db_name =\n    dbutils.secrets.get(\n      scope = Config.metadata_scope,\n      key = Config.metadata_dbname_key\n    )\n\n    val metadata_url =\n    dbutils.secrets.get(\n      scope = Config.metadata_scope,\n      key = Config.metadata_url_key\n    )\n\n    val metadata_user =\n    dbutils.secrets.get(\n      scope = Config.metadata_scope,\n      key = Config.metadata_user_key\n    )\n\n    val metadata_password =\n    dbutils.secrets.get(\n      scope = Config.metadata_scope,\n      key = Config.metadata_password_key\n    )\n\n    val fs_access_key =\n      dbutils.secrets.get(\n        scope = Config.synapse_scope,\n        key = Config.synapse_fs_access_key\n      )\n\n    val synapse_fs_access_url =\n      dbutils.secrets.get(\n        scope = Config.synapse_scope,\n        key = Config.synapse_fs_access_url_key\n      )\n\n    val synapse_temp_dir =\n      dbutils.secrets.get(\n        scope = Config.synapse_scope,\n        key = Config.synapse_temp_dir_key\n      )\n\n    spark.conf.set(\n      synapse_fs_access_url,\n      fs_access_key\n    )\n\n    val tables_for_sk =\n      \"('\" + create_sk_table_list\n        .map(x => x.toUpperCase)\n        .distinct\n        .mkString(\"', '\") + \"')\"\n\n    val sk_metadata =\n      if (Config.sk_service_logic_from_metadata_table == \"true\") {\n\n        val metadata_query =\n          s\"select A.TableName, B.TableType, B.ApiEnabled_IND, B.SkEnabled_IND, A.TableDetail_Id from ${Config.sql_server_lookup_table_detail} A inner join ${Config.sql_server_sk_metadata_table} B on A.TableDetail_Id = B.TableDetail_Id where upper(A.TableName) in ${tables_for_sk}\"\n\n        println(\"metadata_query: \" + metadata_query)\n\n        val sk_metadata_df = spark.read\n          .format(\"jdbc\")\n          .option(\n            \"url\",\n            \"jdbc:sqlserver://\" + metadata_url + \";database=\" + metadata_db_name\n          )\n          .option(\"user\", metadata_user)\n          .option(\"password\", metadata_password)\n          .option(\n            \"query\",\n            metadata_query\n          )\n          .option(\"useAzureMSI\", \"true\")\n          .option(\"tempDir\", synapse_temp_dir)\n          .load()\n        val temp_map = convertDataFrameToMap(sk_metadata_df)\n        val diff_na = create_sk_table_list\n          .map(x => x.toUpperCase)\n          .toSet\n          .diff(temp_map.keys.toSet)\n        if (diff_na.size > 0) {\n          throw new Exception(\n            \"Missing entries found in SK service metadata table. Missing tables: \" + diff_na\n          )\n        }\n        temp_map\n      } else {\n        Map.empty[String, Map[String, Any]]\n      }\n\n    sksConfig.sks.zipWithIndex.map { ele =>\n      val x = ele._1\n      if( x.tableName != main_table_name || !mainTableBreakFlag) {\n      \n\n      println(\n        \"sk generate process started: \" + x.tableName + \" for sk_col: \" + x.skCol\n      )\n      println(\n        \"start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n      )\n\n      var enabled = true\n      var api_type = \"API\"\n      var sk_table_id = \"-1\"\n      var new_data = true\n\n      if (Config.sk_service_logic_from_metadata_table == \"true\") {\n        // overwrite above 2 variables\n        val temp_sk_metadata = sk_metadata.get(x.tableName.toUpperCase)\n        enabled =\n          if (temp_sk_metadata.get(\"SkEnabled_IND\").toString == \"Y\") true\n          else false\n        api_type = if (temp_sk_metadata.get(\"ApiEnabled_IND\").toString == \"Y\") {\n          \"API\"\n        } else {\n          \"NON-API\"\n        }\n        sk_table_id = temp_sk_metadata.get(\"TableDetail_Id\").toString\n      }\n\n      val lit_value =  if (Config.enable_negative_one_self_join_sk && x.tableName == main_table_name) {\n        \"-9191\"\n      } else if (x.tableName == main_table_name) {\n        \"-1\"\n      } else {\n        \"-9090\"\n      }\n\n      println(\"lit_value: \" + lit_value)\n\n      val placeholder_table_config = if (x.tableName != main_table_name) {\n        pkSKDefConfig.get(x.tableName).get\n      } else {\n        PkSKDef(\n          \"\",\n          \"\",\n          prim_key_columns.toList,\n          sk_service_col\n        )\n      }\n      var placeholder_table = if (x.tableName != main_table_name) {\n        if (placeholder_table_config.fmt == \"csv\") {\n          spark.read\n            .format(\"csv\")\n            .option(\"header\", true)\n            .option(\"sep\", \",\")\n            .load(placeholder_table_config.src)\n        } else if (placeholder_table_config.fmt == \"parquet\") {\n          spark.read\n            .format(\"parquet\")\n            .load(placeholder_table_config.src)\n        } else {\n          spark.read.table(placeholder_table_config.src)\n        }\n      } else {\n        spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))\n      }\n      val helper_col_pf = x.tableName + \"_pf_\" + x.skCol\n      val helper_col_sk = x.tableName + \"_sk_\" + x.skCol\n      val helper_col_nk = x.tableName + \"_nk_\" + x.skCol\n\n      var new_max_sk = 0.toLong\n      var new_sks_to_generate = 0.toLong\n      if(x.tableName == main_table_name && tblRetryCounter == 0) {\n        final_output_main_tbl = final_output \n      }\n      if (enabled) {\n        val df_to_eval = if (Config.sk_placeholder_skip_final_override == \"true\") {\n          if (x.tableName == main_table_name && tblRetryCounter != 0)\n            final_output_main_tbl\n          else final_output\n        } else res\n\n        val temp_df = df_to_eval.where(\n              x.skCol + \" is null or \" + x.skCol + \" == '\" + lit_value + \"'\"\n            )\n            .select(\n              concat_ws(\n                \"~|~\",\n                x.nkCol.map { x => expr(x) }: _*\n              ).alias(\"concat_pk\")\n            )\n        //println(\"dummy lit_value: \" + lit_value)\n        //val temp_df_count = temp_df.count()\n        //if (temp_df_count == 0) {\n        if(Config.debug_flag) {\n          println(s\"#### For Placeholder Generation of ${x.tableName}\")\n          temp_df.show(truncate=false)\n        }\n        if (temp_df.take(1).isEmpty) {\n          new_data = false\n        }\n        //println(\"dummy temp_df count: \" + temp_df_count)\n\n        val sk_output: Seq[(String, String, String)] =\n          if (api_type == \"API\" && new_data) {\n\n            println(\"Genrating sk via old api sk service\")\n\n            var pk_to_find =\n              temp_df.collect().map(x => x.getString(0)).toList.distinct\n\n            val pk_to_find_length = pk_to_find.length\n            println(\"number of new sk to generate: \" + pk_to_find_length)\n\n            if (pk_to_find_length > 0) {\n              val post_url =\n                sk_service_base_url + x.tableName + \"/get_sks\"\n              val pk_batches =\n                pk_to_find.toSet.grouped(Config.sk_service_batch_size).toList\n\n              import scala.collection.mutable.ListBuffer\n\n              var temp_list: ListBuffer[PkSkGen] = ListBuffer()\n\n              var while_count = 0\n              pk_batches.foreach { x =>\n                val sk_input = x.toList\n                var sk_response_output = (get_sk_values(\n                  get_sk_values_response(sk_input, post_url),\n                  post_url\n                ))\n\n                while (\n                  sk_response_output.length != sk_input.length && while_count < 5\n                ) {\n                  println(\n                    \"sk_response_output length: \" + sk_response_output.length\n                  )\n                  println(\"sk_input length: \" + sk_input.length)\n                  Thread.sleep(61000)\n                  while_count = while_count + 1\n                  println(\"while_count: \" + while_count)\n                  sk_response_output = (get_sk_values(\n                    get_sk_values_response(sk_input, post_url),\n                    post_url\n                  ))\n                  if (while_count == 3) {\n                    throw new Exception(\n                      \"Please check SK service. Input response size and output response size did not matched in the response after 3 retries with 60 sec gap. Please connect with admin and see memory usaage for redis cache and clear it in case it is more that 70 percent.\"\n                    )\n                  }\n\n                }\n                temp_list = temp_list ++ sk_response_output\n\n              }\n              val sk_seq = temp_list\n                .map(row_val =>\n                  (row_val.pks_sks, row_val.sk_value, row_val.p_flag)\n                )\n                .toSeq\n              new_sks_to_generate = sk_seq.length.toLong\n              sk_seq\n            } else {\n              println(\"no new sk to fetch from api service: \" + x.tableName)\n              Seq((\"\", \"\", \"\"))\n            }\n          } else if (api_type == \"NON-API\" && new_data) {\n\n            println(\"Generating sk via new non-api sk service.\")\n            var lock = true\n            var retry_count = 1\n            val rand = new scala.util.Random\n            while (lock) {\n              try {\n                val tryDelay = (1 + rand.nextInt(10)) * 1000\n                Thread.sleep(tryDelay)\n                dbutils.fs.put(\n                  Config.sk_lock_file_path + x.tableName + \".txt\",\n                  x.tableName\n                )\n                lock = false\n              } catch {\n                case e: Exception => {\n                  println(\n                    x.tableName + \" is locked. Retry: \" + retry_count.toString\n                  )\n\n                  val retryDelay = (10 + rand.nextInt(30)) * 1000\n                  Thread.sleep(retryDelay)\n                  retry_count = retry_count + 1\n                  if (retry_count > Config.sk_lock_retry_count.toInt) {\n                    throw new Exception(\n                      \"Retry count limit reached. Please check sk_lock_path on storage to see for any locks on table and remove it before re-running the process.\"\n                    )\n                  }\n                }\n              }\n            }\n            println(\"Applying lock on: \" + x.tableName)\n            try {\n              val pk_cols = placeholder_table_config.pkCols\n\n              var available_sks = Array[(String, String, String)]()\n              val sks_to_generate = if (x.tableName != main_table_name) {\n                val available_keys_df = temp_df\n                  .alias(\"in0\")\n                  .join(\n                    placeholder_table\n                      .select(\n                        concat_ws(\n                          \"~|~\",\n                          pk_cols.map { x => expr(x) }: _*\n                        ).alias(\"concat_pk\"),\n                        col(placeholder_table_config.skCols).alias(\"sk\")\n                      )\n                      .alias(\"in1\"),\n                    col(\"in0.concat_pk\") === col(\"in1.concat_pk\"),\n                    \"left\"\n                  )\n                  .select(\"in0.concat_pk\", \"in1.sk\")\n                  .dropDuplicates(List(\"concat_pk\"))\n\n                available_sks = available_keys_df\n                  .where(\"sk is not null\")\n                  .collect()\n                  .map(x => (x.getString(0), x.getLong(1).toString, \"false\"))\n\n                available_keys_df\n                  .where(\"sk is null\")\n                  .collect()\n                  .map(x => x.getString(0).toString)\n                  .toList\n              } else {\n                temp_df\n                  .collect()\n                  .map(x => x.getString(0).toString)\n                  .toList\n              }\n\n              val max_sk_query =\n                s\"select MaxSk from ${Config.sql_server_max_sk_table} where TableDetail_ID = ${sk_table_id}\"\n\n              var curr_max_sk =\n                if (\n                  !(x.tableName == main_table_name && Config.temp_output_flag == \"true\" && Config.max_sk_counter_reset_for_main_table == \"true\")\n                ) {\n                  try {\n                    spark.read\n                      .format(\"jdbc\")\n                      .option(\n                        \"url\",\n                        \"jdbc:sqlserver://\" + metadata_url + \";database=\" + metadata_db_name\n                      )\n                      .option(\"user\", metadata_user)\n                      .option(\"password\", metadata_password)\n                      .option(\n                        \"query\",\n                        max_sk_query\n                      )\n                      .option(\"useAzureMSI\", \"true\")\n                      .option(\"tempDir\", synapse_temp_dir)\n                      .load()\n                      .collect()(0)(0)\n                      .toString\n                      .toLong\n                  } catch {\n                    case e: Exception => {\n                      println(\n                        \"Exception while reading existing max SK from table. Please make sure that max SK entry is present for:\" + x.tableName\n                      )\n                      dbutils.fs.rm(\n                        Config.sk_lock_file_path + x.tableName + \".txt\"\n                      )\n                      throw e\n                    }\n                  }\n                } else {\n                  0.toLong\n                }\n\n              val new_sks_length = sks_to_generate.length\n\n              println(\"new sks to generate: \" + new_sks_length)\n\n              new_max_sk = curr_max_sk + new_sks_length\n              new_sks_to_generate = new_sks_length\n              println(\"new_max_sk: \" + new_max_sk)\n              if (x.tableName == main_table_name) {\n                spark.conf.set(\"main_table_api_type\", \"NON-API\")\n                spark.conf.set(\"main_table_max_sk\", new_max_sk.toString)\n                spark.conf.set(\n                  \"main_table_new_sk_count\",\n                  new_sks_to_generate.toString\n                )\n                spark.conf.set(\"main_table_table_id\", sk_table_id.toString)\n              }\n\n              val new_sks = sks_to_generate.zipWithIndex.map(pk_ele =>\n                (pk_ele._1, (pk_ele._2 + curr_max_sk + 1).toString, \"true\")\n              )\n              available_sks ++ new_sks\n            } catch {\n              case e: Exception => {\n                println(\n                  \"Exception while trying to generate SK. Removing Lock for:\" + x.tableName\n                )\n\n                dbutils.fs.rm(\n                  Config.sk_lock_file_path + x.tableName + \".txt\"\n                )\n                throw e\n                Seq((\"\", \"\", \"\"))\n              }\n            }\n\n            // set new_max\n\n          } else {\n            if (new_data) {\n              throw new Exception(\n                \"Undefined api service call type defined for \" + x.tableName + \" :\" + api_type\n              )\n            }\n\n            Seq((\"\", \"\", \"\"))\n          }\n        if (new_data && new_sks_to_generate > 0) {\n          try {\n            if(Config.debug_flag) {\n              println(\"################### sk_output DF ############################\")\n              sk_output.toDF(\"concat_pk\", \"service_sk\", \"placeholder_flag\").where(\"service_sk IS NULL\").show(truncate=false)\n            }\n            final_output = final_output\n              .withColumn(\n                x.tableName + \"_concat_pk\",\n                concat_ws(\n                  \"~|~\",\n                  x.nkCol.map { nk_col => expr(nk_col) }: _*\n                )\n              )\n              .as(\"in0\")\n              .join(\n                sk_output\n                  .toDF(\"concat_pk\", \"service_sk\", \"placeholder_flag\")\n                  .as(\"in1\"),\n                expr(\"in0.\" + x.tableName + \"_concat_pk == in1.concat_pk\"),\n                \"left\"\n              )\n              .withColumn(\n                x.skCol,\n                when(\n                  (col(x.skCol) === lit(lit_value)) || col(x.skCol).isNull,\n                  col(\"service_sk\")\n                ).otherwise(col(x.skCol)).cast(\"long\")\n              )\n              .withColumn(\n                helper_col_pf,\n                col(\"in1.placeholder_flag\")\n              )\n              .withColumn(helper_col_sk, col(\"in1.service_sk\"))\n              .withColumn(helper_col_nk, col(\"in1.concat_pk\"))\n            if(Config.debug_flag && Config.final_output_debug_flag) {\n              println(\"################### final_output DF ############################\")\n              final_output.select(\"in0.\" + x.tableName + \"_concat_pk\", \"in1.concat_pk\", \"service_sk\", x.skCol).where(s\"${x.skCol} IS NULL\").show(truncate=false)\n            }\n            final_output = final_output\n              .selectExpr(\n                \"in0.*\",\n                x.skCol,\n                helper_col_pf,\n                helper_col_sk,\n                helper_col_nk\n              )\n          } catch {\n            case e: Exception => {\n\n              if (api_type == \"NON-API\") {\n                println(\"Exception while trying to join SK. Removing Lock.\")\n\n                dbutils.fs.rm(\n                  Config.sk_lock_file_path + x.tableName + \".txt\"\n                )\n              }\n              throw e\n            }\n          }\n        } else {\n          println(\n            \"No new SK to generate.\"\n          )\n        }\n      } else {\n        println(\n          \"sk and placeholder generate process skipped due to disable in metadata table: \" + x.tableName\n        )\n      }\n\n      if(x.tableName == main_table_name && maxCounter > 1) {\n        val mainTableNullSKCnt = final_output.where(s\"${x.skCol} IS NULL\").count() \n        if(mainTableNullSKCnt == 0) {\n          mainTableBreakFlag = true\n          println(s\"No Null SKs found in final_output. Exiting loop\")\n          \n        } else {\n          tblRetryCounter = tblRetryCounter + 1 \n          println(s\"${mainTableNullSKCnt} Null SKs found in final_output. Regenerating now. Attempt #${tblRetryCounter}\")\n          // TODO: Put final_output in debug path -> logf\n          if (api_type == \"NON-API\") {\n            dbutils.fs.rm(\n              Config.sk_lock_file_path + x.tableName + \".txt\"\n            )\n          }\n          val retry_log_file_full_path = s\"${Config.retry_log_file_folder}/${x.tableName}/sk_gen_retry_${Config.target_table}_${run_id}_retry_num_${tblRetryCounter}\"\n          println(s\"Writing Started for Intermediate DF at location: ${retry_log_file_full_path} \")\n          final_output.write.mode(\"overwrite\").parquet(retry_log_file_full_path)\n          println(s\"Writing Ended for Intermediate DF at location: ${retry_log_file_full_path} \")\n          if(tblRetryCounter >= maxCounter) {\n            throw new Exception(\"NULL SKs getting Generated even after retrying. Please contact Prophecy for help on this issue.\")\n          }\n        }\n      }\n\n      println(\"SK generation process ended\")\n      println(\n        \"end time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n      )\n\n      val update_sql =\n        s\"update ${Config.sql_server_max_sk_table} set MaxSk = ${new_max_sk}, UpdatedDate = cast(sysdatetimeoffset() at time zone 'Central Standard Time' as datetime), UpdatedUid = '${Config.pipeline_name}'  where TableDetail_ID = ${sk_table_id}\"\n\n      var path =\n        Config.load_ready_insert_path + \"/\" + x.tableName + \"/ph_\" + Config.target_table + \"/insert_files/\" + run_id\n\n      var lazy_placeholder = false\n      var lazy_write = false\n      val lazy_table_count_for_placeholder =\n        sksConfig.sks.filter(row => row.tableName == x.tableName).length\n      if (lazy_table_count_for_placeholder > 1) {\n        println(\"creating temporary placeholder file.\")\n        lazy_placeholder = true\n        lazy_write = if (ele._2 == lastOccurrencesMap.get(x.tableName).get) {\n          true\n        } else {\n          false\n        }\n      }\n\n      if (\n        enabled && (x.tableName != main_table_name) && (lazy_write || new_sks_to_generate > 0) && (Config.skip_placeholder_tables_delta_load_ready == \"false\")\n      ) {\n        println(\n          \"placeholder generate process started: \" + x.tableName + \" for sk_col: \" + x.skCol\n        )\n        println(\n          \"start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n        )\n\n        \n        try {\n          var placeholder_df = if (lazy_write && (new_sks_to_generate == 0)){\n            final_output\n          } else {\n            if (placeholder_table_config.orderByDedup != None){\n              import scala.collection.mutable.ListBuffer\n              println(\"Deduplicating placeholder records on:\" + placeholder_table_config.orderByDedup.get)\n              var outputList = new ListBuffer[org.apache.spark.sql.Column]()\n              val order_by_rules = placeholder_table_config.orderByDedup.get.split(',').map(x => x.trim())\n\n              order_by_rules.foreach { rule =>\n                if (rule.toLowerCase().contains(\" asc\")) {\n                  if (rule.toLowerCase().contains(\" asc nulls\")) {\n                    if (rule.toLowerCase().contains(\"nulls first\")) {\n                      outputList += asc_nulls_first(rule.split(\" \")(0).trim())\n                    } else {\n                      outputList += asc_nulls_last(rule.split(\" \")(0).trim())\n                    }\n                  } else {\n                    outputList += asc(rule.split(\" \")(0).trim())\n\n                  }\n                } else {\n                  if (rule.toLowerCase().contains(\" desc nulls\")) {\n                    if (rule.toLowerCase().contains(\"nulls first\")) {\n                      outputList += desc_nulls_first(rule.split(\" \")(0).trim())\n                    } else {\n                      outputList += desc_nulls_last(rule.split(\" \")(0).trim())\n                    }\n                  } else {\n                    outputList += desc(rule.split(\" \")(0).trim())\n                  }\n\n                }\n              }\n\n              val window = Window\n                .partitionBy(col(helper_col_nk), col(helper_col_sk))\n                .orderBy(\n                  outputList: _*\n                )\n              final_output\n                .withColumn(\"dedup_row_num\", row_number().over(window))\n                .where(\"dedup_row_num == 1\")\n                .drop(\"dedup_row_num\")\n                .withColumn(\"pk_arr\", split(col(helper_col_nk), \"~\\\\|~\"))\n                .withColumn(\"sk\", col(helper_col_sk))\n            } else {\n              println(\"Deduplicating placeholder records on PK.\")\n              final_output\n                .where(helper_col_pf + \" == 'true'\")\n                .withColumn(\"pk_arr\", split(col(helper_col_nk), \"~\\\\|~\"))\n                .withColumn(\"sk\", col(helper_col_sk))\n                .dropDuplicates(List(\"pk_arr\", \"sk\"))\n            }\n          }\n\n          var placeholder_df_count = if (lazy_write && (new_sks_to_generate == 0)){\n            0\n          } else{\n            placeholder_df.count\n          }\n   \n          println(\"placeholder_df_count: \" + placeholder_df_count.toString)\n\n          var final_write_df = if (placeholder_df_count > 0) {\n\n            val pk_col_list = placeholder_table_config.pkCols.zipWithIndex.map {\n              case (pk_x, i) => col(\"pk_arr\").getItem(i).as(pk_x)\n            }.toList\n\n            var final_ph_output = placeholder_df\n              .select(\n                (placeholder_df.columns\n                  .filter(x =>\n                    !(placeholder_table_config.pkCols.contains(\n                      x\n                    )) && (x != placeholder_table_config.skCols)\n                  )\n                  .map(x => col(x)) ++ pk_col_list ++ List(\n                  col(\"sk\").as(placeholder_table_config.skCols)\n                )): _*\n              )\n              .withColumn(\n                \"insert_ts\",\n                current_ts_val\n              )\n              .withColumn(\n                \"update_ts\",\n                current_ts_val\n              )\n              .withColumn(\n                \"insert_uid\",\n                substring(concat(lit(\"ph_\"), lit(Config.target_table)), 0, 20)\n              )\n              .withColumn(\"update_uid\", lit(null).cast(\"string\"))\n              .withColumn(\n                \"run_id\",\n                lit(run_id_for_data).cast(\"long\")\n              )\n              .withColumn(\"rec_stat_cd\", lit(\"3\").cast(\"short\"))\n              .withColumn(\"src_env_sk\", col(\"src_env_sk\").cast(\"int\"))\n\n            if (Config.run_id_from_filename == \"true\") {\n              final_ph_output = final_ph_output.withColumn(\n                \"run_id\",\n                col(\"file_name_timestamp\").cast(\"long\")\n              )\n            }\n\n            val audit_columns =\n              Config.audit_cols.split(\",\").map(x => x.trim()).toList\n\n            val final_cols = final_ph_output.columns.toList\n\n            var outputList = new ListBuffer[org.apache.spark.sql.Column]()\n\n            placeholder_table = placeholder_table.filter(lit(false))\n\n            placeholder_table.dtypes\n              .map { case (ph_x, ph_y) => (ph_x, ph_y.toString.toLowerCase()) }\n              .foreach { case (col_name, y) =>\n                if (audit_columns.contains(col_name)) {\n                  outputList += expr(col_name).as(col_name)\n                } else if (final_cols.contains(col_name)) {\n                  val expression = y match {\n                    case value if value.startsWith(\"string\") =>\n                      \"coalesce(\" + col_name + \", '-')\"\n                    case value if value.startsWith(\"date\") =>\n                      \"cast(\" + col_name + \" as date)\"\n                    case value if value.contains(\"time\") =>\n                      \"cast(\" + col_name + \" as timestamp)\"\n                    case value \n                      if value.startsWith(\"decimal\") &&\n                        placeholder_table_config.defaultToDecimalType != None && placeholder_table_config.defaultToDecimalType.get == \"true\" =>\n                        \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as \" + y.replace(\"type\", \"\") + \")\"\n                    case value\n                        if value.startsWith(\"decimal\") || value.startsWith(\n                          \"double\"\n                        ) || value.startsWith(\"float\") =>\n                      \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\n                    case _ =>\n                      \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as \" + y\n                        .replace(\"type\", \"\") + \")\"\n                  }\n                  outputList += expr(expression).as(col_name)\n                } else if (col_name.endsWith(\"_sk\")) {\n                  outputList += expr(\"cast(-1 as long)\").as(col_name)\n                } else {\n                  val expression = y match {\n                    case value if value.startsWith(\"string\") =>\n                      \"'-'\"\n                    case value if value.startsWith(\"date\") =>\n                      \"cast('1900-01-01' as date)\"\n                    case value if value.startsWith(\"timestamp\") =>\n                      \"cast('1900-01-01 00:00:00' as timestamp)\"\n                    case value \n                        if value.startsWith(\"decimal\") &&\n                         placeholder_table_config.defaultToDecimalType != None && placeholder_table_config.defaultToDecimalType.get == \"true\" =>\n                         \"CAST('0' as \"  + y.replace(\"type\", \"\") + \")\"\n                    case value\n                        if value.startsWith(\"decimal\") || value.startsWith(\n                          \"double\"\n                        ) || value.startsWith(\"float\") =>\n                      \"CAST('0' as double)\"\n                    case _ =>\n                      \"CAST('0' as \" + y\n                        .replace(\"type\", \"\") + \")\"\n                  }\n                  outputList += expr(expression).as(col_name)\n                }\n              }\n\n            final_ph_output\n              .select(outputList: _*)\n              .select(placeholder_table.columns.map { x =>\n                if (x.endsWith(\"_sk\") && x != \"src_env_sk\") {\n                  when(col(x) === \"-9090\" || col(x) === \"-9191\", lit(-1).cast(\"long\"))\n                    .otherwise(col(x))\n                    .as(x)\n                } else { col(x) }\n              }: _*)\n          } else {\n            spark.read.table(placeholder_table_config.src).where(\"1==2\")\n          }\n\n          if (placeholder_table_config.encryptCols != None) {\n            val encrypt_col_list = placeholder_table_config.encryptCols.get\n            println(\"Encrypting columns: \", encrypt_col_list)\n            val final_write_df_cols = final_write_df.columns\n            final_write_df =\n              final_write_df.select(final_write_df_cols.map { x =>\n                if (encrypt_col_list.contains(x)) {\n                  expr(\n                    s\"aes_encrypt_udf('$encrypt_key','$encrypt_iv',\" + x + \")\"\n                  ).as(x)\n                } else {\n                  col(x)\n                }\n              }: _*)\n          }\n\n          if ((!skip_placeholder_tables.contains(x.tableName))) {\n\n            if (placeholder_df_count > 0) {\n              try {\n                final_write_df = final_write_df.persist(StorageLevel.DISK_ONLY)\n                final_write_df.write\n                  .format(\"delta\")\n                  .mode(\"append\")\n                  .saveAsTable(placeholder_table_config.src)\n                println(\"delta table write completed: \" + x.tableName)\n              } catch {\n                case e: Exception => {\n                  println(\n                    s\"\"\"Process failed while writing data to delta table: ${x.tableName}. Removing lock. Please fix the issue and re-run the process.\"\"\"\n                  )\n                  if (api_type == \"NON-API\") {\n                    dbutils.fs.rm(\n                      Config.sk_lock_file_path + x.tableName + \".txt\"\n                    )\n                  }\n                  throw e\n                }\n              }\n            } else {\n              println(\"final_write_df is empty. Skipping delta write.\")\n            }\n          }\n\n          if (\n            Config.generate_load_ready_files == \"true\" && (lazy_write || placeholder_df_count > 0)\n          ) {\n\n            try {\n              if (placeholder_table_config.decryptCols != None) {\n                val decrypt_col_list = placeholder_table_config.decryptCols.get\n                println(\"Decrypting columns: \", decrypt_col_list)\n                val final_write_df_cols = final_write_df.columns\n                final_write_df =\n                  final_write_df.select(final_write_df_cols.map { x =>\n                    if (decrypt_col_list.contains(x)) {\n                      expr(\n                        s\"aes_decrypt_udf('$decrypt_key','$decrypt_iv',\" + x + \")\"\n                      )\n                        .as(x)\n                    } else {\n                      col(x)\n                    }\n                  }: _*)\n              }\n\n              val write_mode = if (lazy_placeholder) {\n                \"append\"\n              } else {\n                \"overwrite\"\n              }\n\n              if (placeholder_table_config.dropPartitionCols != None) {\n                println(\n                  \"dropping partition cols: \" + placeholder_table_config.dropPartitionCols\n                )\n                final_write_df\n                  .drop(placeholder_table_config.dropPartitionCols.get: _*)\n                  .repartition(1)\n                  .write\n                  .format(\"parquet\")\n                  .mode(write_mode)\n                  .save(path)\n              } else {\n                final_write_df\n                  .repartition(1)\n                  .write\n                  .format(\"parquet\")\n                  .mode(write_mode)\n                  .save(path)\n              }\n              println(\"temp placeholder file created\")\n            } catch {\n              case e: Exception => {\n                println(\n                  s\"\"\"Generation of temp load ready files failed. Please create single load ready files from delta table, create trigger file and update event log.\"\"\"\n                )\n\n                if (api_type == \"NON-API\") {\n                  println(\n                    s\"\"\"Please update max sk manually and remove lock to proceed.\n          To update metadata table run this query in metadata db: ${update_sql}\n          Also insert and update load ready files needs to be generated manually for corresponding run.\n          To remove lock for ${x.tableName}. Run below command in databricks: \n          dbutils.fs.rm(${Config.sk_lock_file_path}${x.tableName}.txt\")\n          \"\"\"\n                  )\n                }\n\n                // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\n                throw e\n              }\n            }\n\n            if ((!lazy_placeholder) || (lazy_write)) {\n\n              var final_ph_write_df = final_write_df\n              if (lazy_write) {\n                try {\n                  final_ph_write_df = spark.read\n                    .parquet(path)\n                    .dropDuplicates(placeholder_table_config.pkCols)\n                  final_ph_write_df\n                    .repartition(1)\n                    .write\n                    .format(\"parquet\")\n                    .mode(\"overwrite\")\n                    .save(path + \"_lazy/\")\n                  println(\"lazy combined temp placeholder file created.\")\n                } catch {\n                  case e: Exception => {\n                    println(\n                      s\"\"\"Generation of lazy load ready files failed. Please create single load ready file from ${path}_lazy/ path, create trigger file and update event log.\"\"\"\n                    )\n\n                    if (api_type == \"NON-API\") {\n                      println(\n                        s\"\"\"Please update max sk manually and remove lock to proceed.\n          To update metadata table run this query in metadata db: ${update_sql}\n          Also insert and update load ready files needs to be generated manually for corresponding run.\n          To remove lock for ${x.tableName}. Run below command in databricks: \n          dbutils.fs.rm(${Config.sk_lock_file_path}${x.tableName}.txt\")\n          \"\"\"\n                      )\n                    }\n\n                    // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\n                    throw e\n                  }\n                }\n              }\n              val row_count = final_ph_write_df.count()\n              val tableName = x.tableName\n\n              if (row_count == 0) {\n                println(\n                  \"Skipping generation of load ready files as no rows to write.\"\n                )\n              } else {\n                    \n                val finalInsertPath =  if (Config.ht2_flag){\n                  new Path(\n                    baseFilePath + \"/\" + tableName + \"/\" + run_id.substring(\n                      0,\n                      4\n                    ) + \"/placeholder.\" + Config.pipeline_name.toLowerCase.replace(\"ht2_\", \"\") + \".\" + tableName + \".insert.000.\" + run_id + \".parquet\"\n                  )\n                  } else {\n                    new Path(\n                    baseFilePath + \"/\" + tableName + \"/\" + run_id.substring(\n                      0,\n                      4\n                    ) + \"/placeholder.\" + Config.pipeline_name + \".\" + tableName + \".insert.000.\" + run_id + \".parquet\"\n                  )\n                  }\n\n                val tempInsertPath = path\n\n                try {\n                  copyMerge(\n                    hdfs,\n                    tempInsertPath,\n                    hdfs,\n                    finalInsertPath,\n                    true,\n                    hadoopConfig,\n                    lazy_write\n                  )\n                  println(\"single load ready insert file created.\")\n                } catch {\n                  case e: Exception => {\n                    val new_tempInsertPath =\n                      if (lazy_write) tempInsertPath\n                      else tempInsertPath + \"_lazy\"\n                    println(\n                      s\"\"\"Generation of single load ready file failed. Please create single load ready file from ${new_tempInsertPath} path, create trigger file and update event log.\"\"\"\n                    )\n\n                    if (api_type == \"NON-API\") {\n                      println(\n                        s\"\"\"Please update max sk manually and remove lock to proceed.\n          To update metadata table run this query in metadata db: ${update_sql}\n          Also insert and update load ready files needs to be generated manually for corresponding run.\n          To remove lock for ${x.tableName}. Run below command in databricks: \n          dbutils.fs.rm(${Config.sk_lock_file_path}${x.tableName}.txt\")\n          \"\"\"\n                      )\n                    }\n\n                    // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\n                    throw e\n                  }\n                }\n\n                // trigger file generation\n\n                val dataset_name =\n                  Config.trigger_file_content_data_mart_prefix + \".\" + tableName\n                val transfer_type = \"incr\"\n                val trigger_path =\n                  Config.single_load_ready_path + \"/\" + tableName + \"/\" + run_id\n                    .substring(0, 4)\n\n                // <TABLE_NAME>.<RUN_ID>.trigger\n                val fileName = if (Config.ht2_flag){\n                  \"placeholder.\" + Config.pipeline_name.toLowerCase.replace(\"ht2_\", \"\") + \".\" + tableName + \".\" + run_id + \".trigger\"\n                }\n                else{\n                  \"placeholder.\" + Config.pipeline_name + \".\" + tableName + \".\" + run_id + \".trigger\"\n                }\n                val file_date = java.time.LocalDate.now.toString\n                val ins_parquet_file_name = if (Config.ht2_flag){\n                \"placeholder.\" + Config.pipeline_name.toLowerCase.replace(\"ht2_\", \"\") + \".\" + tableName + \".insert.000.\" + run_id + \".parquet\"\n                }\n                else {\n                  \"placeholder.\" + Config.pipeline_name + \".\" + tableName + \".insert.000.\" + run_id + \".parquet\"\n                }\n                val current_time = Instant\n                  .now()\n                  .atZone(ZoneId.of(\"America/Chicago\"))\n\n                val current_time_trigger_file = current_time\n                  .format(DateTimeFormatter.ofPattern(\"yyyyMMdd HH:mm:ss\"))\n                  .toString\n\n                val current_time_event_log = current_time\n                  .format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n                  .toString\n\n                val trggr_data =\n                  dataset_name + \"|\" + transfer_type + \"|\" + ins_parquet_file_name + \"||\" + row_count.toString + \"|0|\" + current_time_trigger_file\n                try {\n                  writeTextFile(trigger_path, fileName, trggr_data)\n                  println(\"trigger file created\")\n                } catch {\n                  case e: Exception => {\n                    println(\n                      s\"\"\"Generation of trigger file failed. Please create trigger file from ${finalInsertPath} path and update event log.\"\"\"\n                    )\n\n                    if (api_type == \"NON-API\") {\n                      println(\n                        s\"\"\"Please update max sk manually and remove lock to proceed.\n          To update metadata table run this query in metadata db: ${update_sql}\n          Also insert and update load ready files needs to be generated manually for corresponding run.\n          To remove lock for ${x.tableName}. Run below command in databricks: \n          dbutils.fs.rm(${Config.sk_lock_file_path}${x.tableName}.txt\")\n          \"\"\"\n                      )\n                    }\n\n                    // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\n                    throw e\n                  }\n                }\n\n                if (!skip_placeholder_tables.contains(x.tableName)) {\n                  // write data to event log\n                  try {\n                    val event_log_df = spark\n                      .createDataFrame(\n                        Seq(\n                          (\n                            Config.data_mart,\n                            Config.pipeline_name,\n                            tableName,\n                            run_id\n                              .substring(0, 4) + \"/\" + ins_parquet_file_name,\n                            current_time_event_log\n                          )\n                        )\n                      )\n                      .toDF(\n                        \"data_mart\",\n                        \"pipeline_name\",\n                        \"event_table\",\n                        \"event_file\",\n                        \"created_ts\"\n                      )\n\n                    event_log_df.write\n                      .format(\"delta\")\n                      .mode(\"append\")\n                      .partitionBy(\"event_table\")\n                      .saveAsTable(Config.event_log_table_name)\n                    println(\"event log entry created\")\n                  } catch {\n                    case e: Exception => {\n                      println(\n                        s\"\"\"Updating event log failed. Please create event log entry from ${finalInsertPath} path.\"\"\"\n                      )\n\n                      if (api_type == \"NON-API\") {\n                        println(\n                          s\"\"\"Please update max sk manually and remove lock to proceed.\n          To update metadata table run this query in metadata db: ${update_sql}\n          Also insert and update load ready files needs to be generated manually for corresponding run.\n          To remove lock for ${x.tableName}. Run below command in databricks: \n          dbutils.fs.rm(${Config.sk_lock_file_path}${x.tableName}.txt\")\n          \"\"\"\n                        )\n                      }\n\n                      // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\n                      throw e\n                    }\n                  }\n                }\n              }\n\n            } else {\n              println(\n                \"Lazy write for placeholder file as more SK for table: \" + x.tableName + \" needs to be generated in other iterations\"\n              )\n            }\n\n          }\n\n          println(\n            \"placeholder generate process ended: \" + x.tableName + \" for sk_col: \" + x.skCol\n          )\n          println(\n            \"end time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n          )\n\n        } catch {\n          case e: Exception => {\n            println(\n              s\"\"\"Process failed while creating placeholder files. Please create single placeholder file from delta, create trigger file and update event log.\"\"\"\n            )\n\n            if (api_type == \"NON-API\") {\n              println(\n                s\"\"\"Please update max sk manually and remove lock to proceed.\n          To update metadata table run this query in metadata db: ${update_sql}\n          Also insert and update load ready files needs to be generated manually for corresponding run.\n          To remove lock for ${x.tableName}. Run below command in databricks: \n          dbutils.fs.rm(${Config.sk_lock_file_path}${x.tableName}.txt\")\n          \"\"\"\n              )\n            }\n\n            // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\n            throw e\n          }\n        }\n\n      }\n\n      if (x.tableName == main_table_name) {\n        spark.conf.set(\"main_table_max_sk_update_sql\", update_sql)\n      }\n      if (\n        api_type == \"NON-API\" && new_sks_to_generate > 0 && (x.tableName != main_table_name)\n      ) {\n        // update max_sk\n\n        val jdbcUrl =\n          s\"jdbc:sqlserver://${metadata_url}:1433;database=${metadata_db_name}\"\n\n        // Create a Properties() object to hold the parameters.\n        val connectionProperties = new Properties()\n        connectionProperties.put(\"user\", s\"${metadata_user}\")\n        connectionProperties.put(\"password\", s\"${metadata_password}\")\n\n        val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n        connectionProperties.setProperty(\"Driver\", driverClass)\n\n        val con =\n          DriverManager.getConnection(jdbcUrl, connectionProperties)\n        val stmt = con.createStatement()\n\n        println(\"dummy update_sql: \" + update_sql)\n        try {\n          stmt.execute(update_sql)\n          println(\n            \"update max sk successful: \" + x.tableName + \" with value: \" + new_max_sk\n          )\n        } catch {\n          case e: Exception => {\n            println(\n              s\"\"\"Updating max sk in metadata table failed. Please update max sk manually and remove lock to proceed.\n          To update metadata table run this query in metadata db: ${update_sql}\n          Also insert and update load ready files needs to be generated manually for corresponding run.\n          To remove lock for ${x.tableName}. Run below command in databricks: \n          dbutils.fs.rm(${Config.sk_lock_file_path}${x.tableName}.txt\")\n          \"\"\"\n            )\n\n            // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\n            throw e\n          }\n        }\n      }\n    if (api_type == \"NON-API\" && x.tableName != main_table_name) {\n      println(\"Successful. Removing Lock.\")\n      dbutils.fs.rm(\n        Config.sk_lock_file_path + x.tableName + \".txt\"\n      )\n    }\n    \n    }\n    }\n    println(\"SK and placeholder generate process ended\")\n    println(\n      \"end time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    )\n    final_output\n  } else {\n    println(\n      \"sk and placeholder process skipped due to no new data or enable_sk_service flag being false.\"\n    )\n    in0\n  }\n\nif(Config.debug_flag) {\n  var printDf = out0\n  if(Config.debug_filter.toLowerCase() != \"none\"){\n    printDf = printDf.where(Config.debug_filter)\n  }  \n  if(Config.debug_col_list.toLowerCase() != \"none\"){\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\n  } else {\n    printDf.show(truncate=true)\n  }\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "nJN6hRRk2RrSF8R-1W3i1$$EeMkGvgQAf4igaTg_5TtH" : {
      "id" : "nJN6hRRk2RrSF8R-1W3i1$$EeMkGvgQAf4igaTg_5TtH",
      "component" : "Script",
      "metadata" : {
        "label" : "update_max_sk",
        "slug" : "update_max_sk",
        "x" : 7220,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "5qU7PDYnUsXbGUJmhTyhJ$$vV4g5y0Fn-NF5pjUZ4-8D",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "5cuMgJ6q55jLNwtOtD_Wm$$JH1FgU7sc07_D_DPcz3H7",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : true
      },
      "properties" : {
        "script" : "println(\"#################################################\")\r\nprintln(\"#####Step name: update max_sk for main table#####\")\r\nprintln(\"#################################################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\nif (\r\n  Config.enable_sk_service != \"false\" && spark.conf.get(\r\n    \"new_data_flag\"\r\n  ) == \"true\"\r\n) {\r\n  import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n  val new_max_sk = spark.conf.get(\"main_table_max_sk\").toLong\r\n  val new_sk_count = spark.conf.get(\"main_table_new_sk_count\").toLong\r\n  val sk_table_id = spark.conf.get(\"main_table_table_id\").toInt\r\n  if (spark.conf.get(\"main_table_api_type\") == \"NON-API\" && new_sk_count > 0) {\r\n\r\n    import java.io.{IOException, _}\r\n    import java.time._\r\n    import java.time.format._\r\n    import java.util\r\n    import java.sql.DriverManager\r\n    import java.util.Properties\r\n    import scala.language.implicitConversions\r\n    import scala.util.Try\r\n\r\n    val metadata_db_name =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_dbname_key\r\n    )\r\n\r\n    val metadata_url =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_url_key\r\n    )\r\n\r\n    val metadata_user =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_user_key\r\n    )\r\n\r\n    val metadata_password =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_password_key\r\n    )\r\n\r\n    // update max_sk\r\n    // Create the JDBC URL without passing in the user and password parameters.\r\n    val jdbcUrl =\r\n      s\"jdbc:sqlserver://${metadata_url}:1433;database=${metadata_db_name}\"\r\n\r\n    // Create a Properties() object to hold the parameters.\r\n    val connectionProperties = new Properties()\r\n    connectionProperties.put(\"user\", s\"${metadata_user}\")\r\n    connectionProperties.put(\"password\", s\"${metadata_password}\")\r\n\r\n    val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n    connectionProperties.setProperty(\"Driver\", driverClass)\r\n\r\n    val con =\r\n      DriverManager.getConnection(jdbcUrl, connectionProperties)\r\n    val stmt = con.createStatement()\r\n\r\n    val update_sql = spark.conf.get(\"main_table_max_sk_update_sql\")\r\n    println(\"dummy update_sql: \" + update_sql)\r\n    try {\r\n      stmt.execute(update_sql)\r\n      println(\r\n        \"update max sk successful: \" + Config.sk_table_name_override + \" with value: \" + new_max_sk\r\n      )\r\n    } catch {\r\n      case e: Exception => {\r\n        println(\r\n          s\"\"\"Updating max sk in metadata table failed. Please update max sk manually and remove lock to proceed.\r\n          To update metadata table run this query in metadata db: ${update_sql}\r\n          To remove lock for ${Config.target_table}. Run below command in databricks: \r\n          dbutils.fs.rm(${Config.sk_lock_file_path}${Config.sk_table_name_override}.txt\")\r\n          \"\"\"\r\n        )\r\n\r\n        // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\r\n        throw e\r\n      }\r\n    }\r\n    \r\n\r\n  }\r\n  if (spark.conf.get(\"main_table_api_type\") == \"NON-API\"){\r\n    println(\"Removing lock from main table: \" + Config.target_table)\r\n    dbutils.fs.rm(\r\n      Config.sk_lock_file_path + Config.sk_table_name_override + \".txt\"\r\n    )\r\n  }\r\n\r\n}\r\nval out0 = in0\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "IVOE4eqMDCX0iAK57gJbn$$zeEdlgdzuuniA3bfd6F6A" : {
      "id" : "IVOE4eqMDCX0iAK57gJbn$$zeEdlgdzuuniA3bfd6F6A",
      "component" : "Script",
      "metadata" : {
        "label" : "apply_reformat_rules",
        "slug" : "apply_reformat_rules",
        "x" : 3020,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "TjZsZpAiHoIkvV47qPbOB$$mfX_L5g0sgDmKG-Ygy6z9",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "_sN14kMZmsrJM_dku2hkr$$2Xewu3mRmztd2gX7oeHWG",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import _root_.io.prophecy.abinitio.ScalaFunctions._\nimport _root_.io.prophecy.libs._\n\nprintln(\"#########################################\")\nprintln(\"#####Step name: apply_reformat_rules#####\")\nprintln(\"#########################################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    )\n// parse config json of key-value pair where key is target column name and value is expression\n\nval out0 =\n  if (\n    Config.reformat_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\n  ) {\n    import org.json4s._\n    import org.json4s.jackson.JsonMethods._\n    import scala.collection.mutable.ListBuffer\n    import java.text.SimpleDateFormat\n    import org.apache.spark.storage.StorageLevel\n\n    registerAllUDFs(spark: SparkSession)\n    \n    var encrypt_key = \"Secret not defined\"\n    var encrypt_iv = \"Secret not defined\"\n    var decrypt_key = \"Secret not defined\"\n    var decrypt_iv = \"Secret not defined\"\n\n    import java.security.MessageDigest\n    import java.util\n    import javax.crypto.Cipher\n    import javax.crypto.spec.{IvParameterSpec, SecretKeySpec}\n    import org.apache.commons.codec.binary.Hex\n    import org.apache.spark.sql.functions._\n\n    try {\n\n      // Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\n      import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n      // loading db secrets\n      encrypt_key = dbutils.secrets.get(\n        scope = Config.encrypt_scope,\n        key = Config.encrypt_EncKey\n      )\n\n      encrypt_iv = dbutils.secrets.get(\n        scope = Config.encrypt_scope,\n        key = Config.encrypt_InitVec\n      )\n\n      decrypt_key = dbutils.secrets.get(\n        scope = Config.decrypt_scope,\n        key = Config.decrypt_EncKey\n      )\n\n      decrypt_iv = dbutils.secrets.get(\n        scope = Config.decrypt_scope,\n        key = Config.decrypt_InitVec\n      )\n    } catch {\n      case e: Exception => {\n        println(\n          \"Please define databricks secrets for encrypt_key, decrypt_key, envrypt_iv and decrypt_iv on your cluster to use encrypt decrypt as udf\"\n        )\n      }\n    }\n\n    def encrypt(key: String, ivString: String, plainValue: String): String = {\n      if (plainValue != null) {\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n        cipher.init(Cipher.ENCRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\n        var encrypted_str =\n          Hex.encodeHexString(cipher.doFinal(plainValue.getBytes(\"UTF-8\")))\n        encrypted_str = encrypted_str\n        return encrypted_str\n      } else {\n        null\n      }\n    }\n\n    def decrypt(\n        key: String,\n        ivString: String,\n        encryptedValue: String\n    ): String = {\n      if (encryptedValue != null) {\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n        cipher.init(Cipher.DECRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\n        new String(cipher.doFinal(Hex.decodeHex(encryptedValue.toCharArray())))\n      } else {\n        null\n      }\n    }\n\n    def keyToSpec(key: String): SecretKeySpec = {\n      var keyBytes: Array[Byte] = (key).getBytes(\"UTF-8\")\n      // val sha: MessageDigest = MessageDigest.getInstance(\"MD5\")\n      keyBytes = Hex.decodeHex(key)\n      keyBytes = util.Arrays.copyOf(keyBytes, 16)\n      new SecretKeySpec(keyBytes, \"AES\")\n    }\n\n    def getIVSpec(IVString: String) = {\n      new IvParameterSpec(IVString.getBytes() ++ Array.fill[Byte](16-IVString.length)(0x00.toByte))\n    }\n\n    val encryptUDF = udf(encrypt _)\n    val decryptUDF = udf(decrypt _)\n    spark.udf.register(\"aes_encrypt_udf\", encryptUDF)\n    spark.udf.register(\"aes_decrypt_udf\", decryptUDF)\n\n    println(\"Registered encrypt and decrypt UDFs in reforamt\")\n\n    var ff3_encrypt_key = \"Secret not defined\"\n    var ff3_encrypt_tweak = \"Secret not defined\"\n\n    try {\n\n      // Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\n      import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n      // loading db secrets\n      ff3_encrypt_key = dbutils.secrets.get(\n        scope = Config.ff3_encrypt_scope,\n        key = Config.ff3_encrypt_key\n      )\n\n      ff3_encrypt_tweak = dbutils.secrets.get(\n        scope = Config.ff3_encrypt_scope,\n        key = Config.ff3_encrypt_tweak\n      )\n    } catch {\n      case e: Exception => {\n        println(\n          \"Please define databricks secrets for ff3_encrypt_key and ff3_encrypt_tweak on your cluster to use ff3_encrypt_idwdata as udf\"\n        )\n      }\n    }\n\n    def jsonStrToMap(jsonStr: String): Map[String, String] = {\n      implicit val formats: DefaultFormats.type = org.json4s.DefaultFormats\n      parse(jsonStr).extract[Map[String, String]]\n    }\n\n    val col_values = jsonStrToMap(Config.reformat_rules.replace(\"\\n\", \" \"))\n\n    var outputList = new ListBuffer[org.apache.spark.sql.Column]()\n\n    col_values.foreach { case (key, value) =>\n      outputList += expr(\n        value\n          .replace(\"SSSZ\", \"SSS\")\n          .replace(\n            \"STRING), 1, 20), 'T', -1), ' '), 'yyyy-MM-dd HH:mm:ss.'), \",\n            \"STRING), 1, 20), 'T', -1), ' '), 'yyyy-MM-dd HH:mm:ss'), \"\n          )\n          .replace(\n            \"aes_encrypt_udf(\",\n            \"aes_encrypt_udf('\" + encrypt_key + \"', '\" + encrypt_iv + \"', \"\n          )\n          .replace(\n            \"aes_decrypt_udf(\",\n            \"aes_decrypt_udf('\" + decrypt_key + \"', '\" + decrypt_iv + \"', \"\n          )\n          .replace(\n            \"ff3_encrypt_idwdata(\",\n            \"ff3_encrypt_idwdata_new('\" + ff3_encrypt_key + \"', '\" + ff3_encrypt_tweak + \"', \"\n          )\n      ).as(key)\n    }\n\n    in0.select(\n      (outputList ++ List(\n        col(\"file_name_timestamp\"),\n        col(\"source_file_base_path\"),\n        col(\"source_file_full_path\"),\n        col(\"dxf_src_sys_id\")\n      )): _*\n    )\n  } else {\n    in0 // if no reformat rules are provided\n  }\n\nif(Config.debug_flag) {\n  var printDf = out0\n  if(Config.debug_filter.toLowerCase() != \"none\"){\n    printDf = printDf.where(Config.debug_filter)\n  }  \n  if(Config.debug_col_list.toLowerCase() != \"none\"){\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\n  } else {\n    printDf.show(truncate=true)\n  }\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "MZXjVgsTlFrCOsgbrh6tI$$bgaHPzaXnVAHqudoIFwIt" : {
      "id" : "MZXjVgsTlFrCOsgbrh6tI$$bgaHPzaXnVAHqudoIFwIt",
      "component" : "Script",
      "metadata" : {
        "label" : "encryption",
        "slug" : "encryption",
        "x" : 5620,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "9U8wFH8yV9WsJdyO-qR0S$$3t1RiYHBrk2DRww-2XjTR",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "2sfusYhyW5DmWpOSq0hFv$$fdWI-MNlkWZn49hP7PXjj",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import java.security.MessageDigest\r\nimport java.util\r\nimport javax.crypto.Cipher\r\nimport javax.crypto.spec.{IvParameterSpec, SecretKeySpec}\r\nimport org.apache.commons.codec.binary.Hex\r\nimport org.apache.spark.sql.functions._\r\n\r\nprintln(\"###############################\")\r\nprintln(\"#####Step name: encryption#####\")\r\nprintln(\"###############################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\nvar out0 = in0\r\nif (\r\n  (Config.encrypt_cols != \"None\" && spark.conf.get(\r\n    \"new_data_flag\"\r\n  ) == \"true\") || (Config.encrypt_cols != \"None\" && Config.generate_only_load_ready_files == \"true\")\r\n) {\r\n\r\n  try {\r\n\r\n//Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\r\n    import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n\r\n    // loading db secrets\r\n    val encrypt_key =\r\n      dbutils.secrets.get(\r\n        scope = Config.encrypt_scope,\r\n        key = Config.encrypt_EncKey\r\n      )\r\n\r\n    val encrypt_iv =\r\n      dbutils.secrets.get(\r\n        scope = Config.encrypt_scope,\r\n        key = Config.encrypt_InitVec\r\n      )\r\n\r\n    val pii_cols =\r\n      Config.encrypt_cols.split(\",\").map(x => x.trim().toLowerCase())\r\n    val non_pii_cols =\r\n      (in0.columns.map(x => x.toLowerCase()).toList diff pii_cols).toList\r\n\r\n    def encrypt(key: String, ivString: String, plainValue: String): String = {\r\n      if (plainValue != null) {\r\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\r\n        cipher.init(Cipher.ENCRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\r\n        var encrypted_str =\r\n          Hex.encodeHexString(cipher.doFinal(plainValue.getBytes(\"UTF-8\")))\r\n        encrypted_str = encrypted_str\r\n        return encrypted_str\r\n      } else {\r\n        null\r\n      }\r\n    }\r\n\r\n    def keyToSpec(key: String): SecretKeySpec = {\r\n      var keyBytes: Array[Byte] = (key).getBytes(\"UTF-8\")\r\n      // val sha: MessageDigest = MessageDigest.getInstance(\"MD5\")\r\n      keyBytes = Hex.decodeHex(key)\r\n      keyBytes = util.Arrays.copyOf(keyBytes, 16)\r\n      new SecretKeySpec(keyBytes, \"AES\")\r\n    }\r\n\r\n    def getIVSpec(IVString: String) = {\r\n      new IvParameterSpec(IVString.getBytes() ++ Array.fill[Byte](16-IVString.length)(0x00.toByte))\r\n    }\r\n\r\n    val encryptUDF = udf(encrypt _)\r\n\r\n    spark.udf.register(\"encrypt\", encryptUDF)\r\n\r\n    val str = if (Config.encryption_to_uppercase) {\r\n      pii_cols\r\n        .map(x =>\r\n          s\"UPPER(encrypt('$encrypt_key','$encrypt_iv',\" + x + \")) as \" + x\r\n        )\r\n        .mkString(\",\")\r\n    } else {\r\n      pii_cols\r\n        .map(x => s\"encrypt('$encrypt_key','$encrypt_iv',\" + x + \") as \" + x)\r\n        .mkString(\",\")\r\n    }\r\n\r\n    val all_cols = str + \",\" + non_pii_cols.mkString(\",\")\r\n    in0.createOrReplaceTempView(\r\n      s\"temp_tbl_enc_${Config.pipeline_name.replace(\"-\", \"\")}\"\r\n    )\r\n\r\n    out0 = spark\r\n      .sql(\r\n        s\"select $all_cols from temp_tbl_enc_${Config.pipeline_name.replace(\"-\", \"\")}\"\r\n      )\r\n      .select(in0.columns.map(x => col(x)): _*)\r\n  } catch {\r\n    case e: Exception => {\r\n      println(\r\n        s\"\"\"Process failed while encrypting data\"\"\"\r\n      )\r\n\r\n      if (spark.conf.get(\"main_table_api_type\") == \"NON-API\") {\r\n        import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n        println(s\" Removing lock from: ${Config.sk_table_name_override}\")\r\n        dbutils.fs.rm(Config.sk_lock_file_path + Config.sk_table_name_override + \".txt\")\r\n      }\r\n      throw e\r\n    }\r\n  }\r\n} else {\r\n  out0 = in0\r\n}\r\n\r\nif(Config.debug_flag) {\r\n  var printDf = out0\r\n  if(Config.debug_filter.toLowerCase() != \"none\"){\r\n    printDf = printDf.where(Config.debug_filter)\r\n  }  \r\n  if(Config.debug_col_list.toLowerCase() != \"none\"){\r\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\r\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\r\n  } else {\r\n    printDf.show(truncate=true)\r\n  }\r\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "JB32GmgXxUskZ_V-wAb81$$faMNpiQq5PZsq8jK-xAbo" : {
      "id" : "JB32GmgXxUskZ_V-wAb81$$faMNpiQq5PZsq8jK-xAbo",
      "component" : "Script",
      "metadata" : {
        "label" : "apply_len_rules",
        "slug" : "apply_len_rules",
        "x" : 4220,
        "y" : 220,
        "phase" : 0,
        "cache" : true,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "oFTVc2YT2-2LsJX7BbRI9$$2LIvIYIHIb0jWoG-NxQbd",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "yjmy41YhFjPBABhKekCmW$$3ZgvLIymbCGJHArPsmdmy",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        }, {
          "id" : "nuLIqeUXh4pByRcgEuuO_$$kjPt90BDZpUDYuuTPg7wE",
          "slug" : "out1",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "// This will check the maximum bytes allowed in the column. It can be customised to provide any rule based on which record\n// needs to be rejected.\n\nprintln(\"####################################\")\nprintln(\"#####Step name: apply_len_rules#####\")\nprintln(\"####################################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    )\n\nvar out1 = in0\nval out0 =\n  if (\n    Config.length_rules_from_metadata_table == \"true\" && spark.conf.get(\n      \"new_data_flag\"\n    ) == \"true\" && spark.catalog.tableExists(Config.length_rules_metadata_table)\n  ) {\n    def check_null(obj: Any): String = {\n      val str: String = obj match {\n        case null  => \"\"\n        case other => other.toString\n      }\n      str.toLowerCase\n    }\n    val df_collect = spark.read\n      .table(Config.length_rules_metadata_table)\n      .where(\"lower(table_name) = '\" + Config.target_table + \"'\")\n      .select(\n        \"COLUMN_NAME\",\n        \"DATA_TYPE\",\n        \"CHARACTER_MAXIMUM_LENGTH\",\n        \"NUMERIC_PRECISION\",\n        \"NUMERIC_SCALE\"\n      )\n      .collect()\n\n    if (df_collect.length > 0) {\n      val length_map = df_collect\n        .map(x =>\n          x(0).toString.toLowerCase -> Seq(\n            check_null(x(1)),\n            check_null(x(2)),\n            check_null(x(3)),\n            check_null(x(4))\n          )\n        )\n        .toMap\n\n      val audit_columns = Config.audit_cols.split(\",\").map(x => x.trim()).toList\n      val length_map_keys =\n        length_map.keySet.filter(x => !audit_columns.contains(x)).toList\n\n      val base_df_columns = in0.columns.map(x => x.toLowerCase())\n\n      var case_when = \"case \"\n      length_map_keys.filter(x=> base_df_columns.contains(x.toLowerCase())).foreach { x =>\n        val column_meta = length_map(x)\n        column_meta(0) match {\n          case value\n              if List(\"int\", \"smallint\", \"tinyint\", \"bigint\").contains(value) =>\n            case_when =\n              case_when + \" when length( cast(\" + x + \" as \" + value.replace(\n                \"bigint\",\n                \"long\"\n              ) + \")) > \" + column_meta(2) + \" then '\" + x + \"' \"\n          case value if List(\"varchar\", \"nvarchar\", \"char\").contains(value) =>\n            case_when =\n              case_when + \" when length( cast(\" + x + \" as \" + \"string\" + \")) > \" + column_meta(\n                1\n              ) + \" then '\" + x + \"' \"\n          case _ => case_when = case_when\n        }\n      }\n      case_when = case_when + \" else 'pass' end\"\n\n      if (!case_when.contains(\"then\")) {\n        case_when = \"'pass'\"\n      }\n      val length_rules_df = in0.withColumn(\"reject_record\", expr(case_when))\n      out1 = length_rules_df\n      length_rules_df\n        .select(length_rules_df.columns.map { x =>\n          if (length_map_keys.contains(x)) {\n            val column_meta = length_map(x)\n            column_meta(0) match {\n              case value\n                  if List(\"int\", \"smallint\", \"tinyint\", \"bigint\")\n                    .contains(value) =>\n                when(\n                  length(\n                    col(x).cast(value.replace(\"bigint\", \"long\"))\n                  ) <= column_meta(2).toInt,\n                  col(x).cast(value.replace(\"bigint\", \"long\"))\n                ).otherwise(null).as(x)\n              case value if List(\"decimal\", \"numeric\").contains(value) =>\n                col(x)\n                  .cast(\n                    \"decimal(\" + column_meta(2) + \",\" + column_meta(3) + \")\"\n                  )\n                  .as(x)\n              case value\n                  if List(\"varchar\", \"nvarchar\", \"char\").contains(value) =>\n                when(\n                  length(col(x).cast(\"string\")) <= column_meta(1).toInt,\n                  col(x).cast(\"string\")\n                ).otherwise(null).as(x)\n              case value\n                  if List(\"datetime\", \"datetime2\", \"time\").contains(value) =>\n                col(x).cast(\"string\").as(x)\n              case value if List(\"date\").contains(value) =>\n                col(x).cast(\"string\").as(x)\n              case _ => col(x).as(x)\n            }\n          } else col(x)\n        }: _*)\n\n    } else if (\n      Config.length_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\n    ) {\n      in0.withColumn(\"reject_record\", expr(Config.length_rules))\n    } else {\n      in0.withColumn(\"reject_record\", lit(\"pass\"))\n    }\n  } else if (\n    Config.length_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\n  ) {\n    in0.withColumn(\"reject_record\", expr(Config.length_rules))\n  } else {\n    in0.withColumn(\"reject_record\", lit(\"pass\"))\n  }\n\nimport org.apache.spark.storage.StorageLevel\nif (!out1.columns.contains(\"reject_record\")) {\n  out1 = out0\n}\nspark.conf.set(\"reject_record_count\", \"0\")\n\nif (\n  (Config.length_rules != \"None\" || Config.length_rules_from_metadata_table == \"true\") && spark.conf\n    .get(\"new_data_flag\") == \"true\"\n) {\n  val df = in0.persist(StorageLevel.DISK_ONLY)\n\n  out1 = out1.filter(col(\"reject_record\") =!= lit(\"pass\"))\n\n  val reject_record_count = out1.count()\n\n  spark.conf.set(\"reject_record_count\", reject_record_count.toString)\n\n}\n\nif(Config.debug_flag) {\n  var printDf = out0\n  if(Config.debug_filter.toLowerCase() != \"none\"){\n    printDf = printDf.where(Config.debug_filter)\n  }  \n  if(Config.debug_col_list.toLowerCase() != \"none\"){\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\n  } else {\n    printDf.show(truncate=true)\n  }\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): (DataFrame, DataFrame) = {",
        "scriptMethodFooter" : "    (out0, out1)\n}"
      }
    },
    "jZeb21Pxoz4Lv3giVtLPX$$zkvp8Lr4YXAN4xK-fN_To" : {
      "id" : "jZeb21Pxoz4Lv3giVtLPX$$zkvp8Lr4YXAN4xK-fN_To",
      "component" : "Script",
      "metadata" : {
        "label" : "generate_lookups",
        "slug" : "generate_lookups",
        "x" : 220,
        "y" : 220,
        "phase" : -1,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "3PtkE6nQ3BJeMPNI_LNlv$$nfWPRuH7DnBFfOURAbeNx",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "2bU46E_0HdQ98jwTLfCHo$$0uuvtER1E4SGyYM8UKeqz",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#####################################\")\nprintln(\"#####Step name: generate_lookups#####\")\nprintln(\"#####################################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    ) \n\n// case class to define lookup definition\ncase class LookupDef(\n    fmt: String,\n    src: String,\n    keyCols: List[String],\n    valCols: List[String]\n)\n\ntype Lookups = Map[String, LookupDef]\n\n// create lookups if lookup is not empty\nif (Config.lookups != \"None\" || (Config.survivorship_lookup != \"None\" && Config.survivorship_flag)) {\n  import pureconfig._\n  import pureconfig.generic.auto._\n\n  val lookupConfig = if (Config.lookups != \"None\") {\n    ConfigSource.string(Config.lookups).loadOrThrow[Lookups]\n  } else {\n    Map.empty[String, LookupDef]\n  }\n\n  val survivorshipLookupConfig = if (Config.survivorship_lookup != \"None\") {\n    ConfigSource.string(Config.survivorship_lookup).loadOrThrow[Lookups]\n  } else {\n    Map.empty[String, LookupDef]\n  }\n\n  val final_lookup_config = lookupConfig.++(survivorshipLookupConfig)\n\n  // iterate through list of lookups based on different source types and create lookup\n  final_lookup_config.foreach({\n    case (lookupName, lookupDef) => {\n      val in0 = if (lookupDef.fmt == \"csv\") {\n        spark.read\n          .format(\"csv\")\n          .option(\"header\", true)\n          .option(\"sep\", \",\")\n          .load(lookupDef.src)\n      } else if (lookupDef.fmt == \"parquet\") {\n        spark.read\n          .format(\"parquet\")\n          .load(lookupDef.src)\n      } else if (lookupDef.fmt == \"query\") {\n        spark.sql(lookupDef.src)\n      } else {\n        spark.read.table(lookupDef.src)\n      }\n\n      var in1 = in0\n        .select(in0.columns.map(x => col(x).as(x.toLowerCase)): _*)\n        .select((lookupDef.keyCols ++ lookupDef.valCols).map(x => col(x)): _*)\n      \n      createLookup(\n        lookupName,\n        in1,\n        spark,\n        lookupDef.keyCols,\n        lookupDef.valCols: _*\n      )\n    }\n  })\n\n}\n\nval out0 = spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "btQqSKid5gvTBDzik0I7b$$t_OXMCoK5vibXnH2FDC8D" : {
      "id" : "btQqSKid5gvTBDzik0I7b$$t_OXMCoK5vibXnH2FDC8D",
      "component" : "Script",
      "metadata" : {
        "label" : "create_trigger_file",
        "slug" : "create_trigger_file",
        "x" : 7020,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "AmER0j9sSGoN_SoLYyetV$$38aKxZbK076TlhrLvOOYm",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "AfBsMj_YHcRQrj5_qm07y$$TXjpzCUNFnU4EYSy4Onlw",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import java.text.SimpleDateFormat\r\nimport org.apache.hadoop.fs.{FSDataOutputStream, FileSystem, FileUtil, Path}\r\nimport scala.util.Try\r\nimport scala.io.Source\r\nimport scala.sys.process.Process\r\nimport org.apache.hadoop.io.IOUtils\r\nimport java.io._\r\nimport java.time._\r\nimport java.time.format._\r\n\r\nprintln(\"########################################\")\r\nprintln(\"#####Step name: create_trigger_file#####\")\r\nprintln(\"########################################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\ndef writeTextFile(filePath: String, filename: String, s: String): Unit = {\r\n\r\n  val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\r\n  val file = new Path(filePath + \"/\" + filename)\r\n  val dataOutputStream: FSDataOutputStream = fs.create(file)\r\n  val bw: BufferedWriter = new BufferedWriter(\r\n    new OutputStreamWriter(dataOutputStream, \"UTF-8\")\r\n  )\r\n  bw.write(s)\r\n  bw.close()\r\n\r\n  val crcPath = new Path(filePath + \"/.\" + filename + \".crc\")\r\n  if (Try(fs.exists(crcPath)).isSuccess) {\r\n    fs.delete(crcPath, true)\r\n  }\r\n}\r\n\r\nval out0 =\r\n  if (Config.skip_main_table_load_ready_files == \"false\") {\r\n    if (\r\n      Config.generate_load_ready_files == \"true\" || (Config.generate_only_load_ready_files == \"true\")\r\n    ) {\r\n      try {\r\n        println(\"Generating trigger file\")\r\n\r\n//<Dataset Name>|<Transfer type - incr/hist>|<Insert/Merge Parquet file Name>|<Update Parquet file Name>|<Insert file Count>|<Update file Count>|<Create TimeStamp in \"YYYYMMDD HH24:MI:SS\">\r\n//ids_common.d_cag|incr|d_cag.merge.000.20221127015930.parquet||5502|159878|20221127 02:00:02\r\n        val run_id = spark.conf.get(\"run_id\")\r\n        val dataset_name =\r\n          Config.trigger_file_content_data_mart_prefix + \".\" + Config.target_table\r\n        val transfer_type = \"incr\"\r\n        val insert_count = spark.conf.get(\"insert_load_ready_count\")\r\n        val update_count = spark.conf.get(\"update_load_ready_count\")\r\n        val path =\r\n          Config.single_load_ready_path + \"/\" + Config.target_table + \"/\" + run_id\r\n            .substring(0, 4)\r\n\r\n//<TABLE_NAME>.<RUN_ID>.trigger\r\n        val fileName = Config.target_table + \".\" + run_id + \".trigger\"\r\n        val ins_parquet_file_name =\r\n          Config.target_table + \".insert.000.\" + run_id + \".parquet\"\r\n        val upd_parquet_file_name =\r\n          Config.target_table + \".update.000.\" + run_id + \".parquet\"\r\n        val current_time = Instant\r\n          .now()\r\n          .atZone(ZoneId.of(\"America/Chicago\"))\r\n\r\n        val current_time_trigger_file = current_time\r\n          .format(DateTimeFormatter.ofPattern(\"yyyyMMdd HH:mm:ss\"))\r\n          .toString\r\n\r\n        val current_time_event_log = current_time\r\n          .format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\r\n          .toString\r\n\r\n        val trggr_data =\r\n          dataset_name + \"|\" + transfer_type + \"|\" + ins_parquet_file_name + \"|\" + upd_parquet_file_name + \"|\" + insert_count + \"|\" + update_count + \"|\" + current_time_trigger_file\r\n\r\n        writeTextFile(path, fileName, trggr_data)\r\n\r\n// write data to event log\r\n        if (Config.skip_delta_synapse_write_common_dimension == \"false\") {\r\n          val event_log_df = spark\r\n            .createDataFrame(\r\n              Seq(\r\n                (\r\n                  Config.data_mart,\r\n                  Config.pipeline_name,\r\n                  Config.target_table,\r\n                  run_id.substring(0, 4) + \"/\" + ins_parquet_file_name,\r\n                  current_time_event_log\r\n                ),\r\n                (\r\n                  Config.data_mart,\r\n                  Config.pipeline_name,\r\n                  Config.target_table,\r\n                  run_id.substring(0, 4) + \"/\" + upd_parquet_file_name,\r\n                  current_time_event_log\r\n                )\r\n              )\r\n            )\r\n            .toDF(\r\n              \"data_mart\",\r\n              \"pipeline_name\",\r\n              \"event_table\",\r\n              \"event_file\",\r\n              \"created_ts\"\r\n            )\r\n\r\n          event_log_df.write\r\n            .format(\"delta\")\r\n            .mode(\"append\")\r\n            .partitionBy(\"event_table\")\r\n            .saveAsTable(Config.event_log_table_name)\r\n        }\r\n\r\n        spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))\r\n      } catch {\r\n        case e: Exception => {\r\n          if (spark.conf.get(\"main_table_api_type\") == \"NON-API\") {\r\n            val update_sql = spark.conf.get(\"main_table_max_sk_update_sql\")\r\n            println(\r\n              s\"\"\"Process failed while generating trigger file and updating event log. Please create trigger file if it does not exist and create entry in event log.\r\n            Please update max sk manually and remove lock to proceed.\r\n            Update metadata table run this query in metadata db: ${update_sql}\r\n            To remove lock for ${Config.target_table}. Run below command in databricks: \r\n            dbutils.fs.rm(${Config.sk_lock_file_path}${Config.sk_table_name_override}.txt\")\r\n            \"\"\"\r\n            )\r\n          }\r\n          // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\r\n          throw e\r\n        }\r\n      }\r\n    } else {\r\n      in0\r\n    }\r\n  } else {\r\n    in0\r\n  }\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "7X1ScPt9O6DCx6AkZIMBC$$nT-3k1VRc5kl47UyaLcuW" : {
      "id" : "7X1ScPt9O6DCx6AkZIMBC$$nT-3k1VRc5kl47UyaLcuW",
      "component" : "Script",
      "metadata" : {
        "label" : "create_ht2_encrypted_file",
        "slug" : "create_ht2_encrypted_file",
        "x" : 3220,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "nE996SsbLqum0bNawuTQv$$lOkGnjnJTJvtfuVo5JBtH",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "sa0b-KEYC6KL959C-hlFS$$eSUl-evVSpu2aRCR0wYj7",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ {
              "name" : "hold_catgy_desc",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_nm",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_catgy_priorty_id",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_id",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "mnl_intrvn_hold_flg",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_typ_sk",
              "type" : "long",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "type_code",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "patient_hold_flg",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_actv_start_dt",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_actv_end_dt",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "ref_info_desc",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_desc",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "src_env_sk",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "tat_hold_exclusion_flg",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_sk",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "file_name_timestamp",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_base_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_full_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_sys_id",
              "type" : "short",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            } ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : true
      },
      "properties" : {
        "script" : "import org.apache.commons.codec.binary.Hex\r\nimport org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs._\r\nimport org.apache.hadoop.io.IOUtils\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.storage.StorageLevel\r\nimport org.json4s._\r\nimport org.json4s.jackson.JsonMethods._\r\nimport org.json4s.jackson.Serialization.write\r\nimport pureconfig._\r\nimport pureconfig.generic.auto._\r\nimport play.api.libs.json._\r\n\r\nimport java.io.{IOException, _}\r\nimport java.security.MessageDigest\r\nimport java.time._\r\nimport java.time.format._\r\nimport java.util\r\nimport java.sql.DriverManager\r\nimport java.util.Properties\r\nimport javax.crypto.Cipher\r\nimport javax.crypto.spec.{IvParameterSpec, SecretKeySpec}\r\nimport scala.collection.mutable.ListBuffer\r\nimport scala.language.implicitConversions\r\nimport scala.util.Try\r\nimport spark.implicits._\r\n\r\nimport com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n\r\nimplicit val formats = DefaultFormats\r\n\r\nval out0 =\r\n  if (\r\n    Config.create_ht2_encrypt_file == \"true\" && spark.conf.get(\r\n      \"new_data_flag\"\r\n    ) == \"true\"\r\n  ) {\r\n    println(\"#####Step name: create ht2_idw encrypted file#####\")\r\n    println(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n    val isPKSet = spark.conf.getAll.contains(\"primary_key\")\r\n    val prim_key_columns = if (isPKSet) {\r\n      spark.conf\r\n        .get(\"primary_key\")\r\n        .split(\",\")\r\n        .map(x => x.trim())\r\n    } else {\r\n      Config.primary_key\r\n        .split(\",\")\r\n        .map(x => x.trim())\r\n    }\r\n    val isSKSet = spark.conf.getAll.contains(\"sk_service_col\")\r\n    val sk_service_col = if (isSKSet) {\r\n      spark.conf\r\n        .get(\"sk_service_col\")\r\n        .toLowerCase\r\n        .trim\r\n    } else {\r\n      Config.sk_service_col.toLowerCase.trim\r\n    }\r\n    val run_id = spark.conf.get(\"run_id\")\r\n    val hadoopConfig = new Configuration()\r\n    val hdfs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\r\n    val load_ready_insert_path =\r\n      Config.ht2_load_ready_insert_path\r\n    val baseFilePath =\r\n      Config.ht2_single_load_ready_path\r\n\r\n    case class PkSkGen(pks_sks: String, sk_value: String, p_flag: String)\r\n\r\n    case class SKDef(\r\n        tableName: String,\r\n        skCol: String,\r\n        nkCol: List[String]\r\n    )\r\n\r\n    case class SKService(\r\n        sks: List[SKDef]\r\n    )\r\n\r\n    case class PkSKDef(\r\n        fmt: String,\r\n        src: String,\r\n        pkCols: List[String],\r\n        skCols: String,\r\n        synapseTable: Option[String] = None,\r\n        encryptCols: Option[List[String]] = None,\r\n        decryptCols: Option[List[String]] = None,\r\n        dropPartitionCols: Option[List[String]] = None,\r\n        orderByDedup: Option[String] = None\r\n    )\r\n\r\n    type PkSKDefs = Map[String, PkSKDef]\r\n\r\n    val pkSKDefConfig =\r\n      ConfigSource.string(Config.pk_sk_info).loadOrThrow[PkSKDefs]\r\n\r\n    def jsonStrToMap(jsonStr: String): Map[String, String] = {\r\n      implicit val formats: DefaultFormats.type = org.json4s.DefaultFormats\r\n      parse(jsonStr).extract[Map[String, String]]\r\n    }\r\n\r\n    def copyMerge(\r\n        srcFS: FileSystem,\r\n        srcDir: Path,\r\n        dstFS: FileSystem,\r\n        dstFile: Path,\r\n        deleteSource: Boolean,\r\n        conf: Configuration\r\n    ): Boolean = {\r\n      if (dstFS.exists(dstFile))\r\n        throw new IOException(s\"Target $dstFile already exists\")\r\n\r\n      // Source path is expected to be a directory:\r\n      if (srcFS.getFileStatus(srcDir).isDirectory()) {\r\n\r\n        val outputFile = dstFS.create(dstFile)\r\n        Try {\r\n          srcFS\r\n            .listStatus(srcDir)\r\n            .sortBy(_.getPath.getName)\r\n            .filter(_.getPath.getName.endsWith(\".parquet\"))\r\n            .collect {\r\n              case status if status.isFile() =>\r\n                val inputFile = srcFS.open(status.getPath())\r\n                Try(IOUtils.copyBytes(inputFile, outputFile, conf, false))\r\n                inputFile.close()\r\n            }\r\n        }\r\n        outputFile.close()\r\n\r\n        if (deleteSource) srcFS.delete(srcDir, true) else true\r\n      } else false\r\n    }\r\n\r\n    val json = Json.parse(Config.final_table_schema)\r\n\r\n    val col_values_from_schema = json.as[JsObject].keys.toSeq.toList\r\n    val audit_columns =\r\n      Config.audit_cols.split(\",\").map(x => x.trim()).toList\r\n\r\n    val col_list_wout_audit_column =\r\n      if (Config.ht2_extra_columns_reformat == \"true\") {\r\n        in0.columns.toList\r\n      } else {\r\n        (col_values_from_schema diff audit_columns)\r\n      }\r\n    val col_map = jsonStrToMap(Config.final_table_schema)\r\n\r\n    val col_values = col_map.keySet.toList\r\n    var outputList = new ListBuffer[org.apache.spark.sql.Column]()\r\n    for (col_name <- in0.columns) {\r\n      if (prim_key_columns.contains(col_name)) {\r\n        if (col_values.contains(col_name)) {\r\n          val expression = col_map.get(col_name).get match {\r\n            case \"string\" =>\r\n              \"case when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else '-' end\"\r\n            case \"date\" =>\r\n              \"coalesce(to_date(trim(\" + col_name + \"), 'yyyyMMdd'), to_date(trim(\" + col_name + \"), 'yyyy-MM-dd'), CAST(null as date))\"\r\n            case \"timestamp\" =>\r\n              \"coalesce(to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmssSSS'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmss'), to_timestamp(rpad(\" + col_name + \",23,'0'), 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(\" + col_name + \", 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss'), CAST(null as timestamp))\"\r\n            case value if value.startsWith(\"decimal\") =>\r\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\r\n            case value if List(\"float\", \"double\").contains(value) =>\r\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\r\n            case value if value.contains(\"struct\") || value.contains(\"array\") =>\r\n              col_name\r\n            case _ =>\r\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as \" + col_map\r\n                .get(col_name)\r\n                .get + \")\"\r\n          }\r\n          outputList += expr(expression).as(col_name)\r\n        } else {\r\n          val expression =\r\n            \"case when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else '-' end\"\r\n          outputList += expr(expression).as(col_name)\r\n        }\r\n      } else if (col_values.contains(col_name)) {\r\n        val expression = col_map.get(col_name).get match {\r\n          case \"string\" =>\r\n            \"case when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else CAST(null as string) end\"\r\n          case \"date\" =>\r\n            \"coalesce(to_date(trim(\" + col_name + \"), 'yyyyMMdd'), to_date(trim(\" + col_name + \"), 'yyyy-MM-dd'), CAST(null as date))\"\r\n          case \"timestamp\" =>\r\n            \"coalesce(to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmss'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss'), CAST(null as timestamp))\"\r\n          case value if value.startsWith(\"decimal\") =>\r\n            \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\r\n          case value if List(\"float\", \"double\").contains(value) =>\r\n            \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\r\n          case value if value.contains(\"struct\") || value.contains(\"array\") =>\r\n            col_name\r\n          case _ =>\r\n            \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else null end as \" + col_map\r\n              .get(col_name)\r\n              .get + \")\"\r\n        }\r\n        outputList += expr(expression).as(col_name)\r\n      } else {\r\n        val expression =\r\n          \"coalesce(format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########'), \" + col_name + \")\"\r\n        outputList += expr(col_name).as(col_name)\r\n      }\r\n    }\r\n\r\n    val defaulted_df = in0.select(outputList: _*)\r\n\r\n    var sksConfig =\r\n      ConfigSource.string(Config.optional_sk_config).loadOrThrow[SKService]\r\n\r\n    sksConfig = SKService(\r\n      List(\r\n        SKDef(\r\n          tableName = Config.target_table,\r\n          skCol = Config.target_table,\r\n          nkCol = col_list_wout_audit_column\r\n        )\r\n      ) ++ sksConfig.sks\r\n    )\r\n\r\n    var ff3_encrypt_key = \"Secret not defined\"\r\n    var ff3_encrypt_tweak = \"Secret not defined\"\r\n\r\n    try {\r\n\r\n      // Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\r\n      import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n\r\n      // loading db secrets\r\n      ff3_encrypt_key = dbutils.secrets.get(\r\n        scope = Config.ff3_encrypt_scope,\r\n        key = Config.ff3_encrypt_key\r\n      )\r\n\r\n      ff3_encrypt_tweak = dbutils.secrets.get(\r\n        scope = Config.ff3_encrypt_scope,\r\n        key = Config.ff3_encrypt_tweak\r\n      )\r\n    } catch {\r\n      case e: Exception => {\r\n        println(\r\n          \"Please define databricks secrets for ff3_encrypt_key and ff3_encrypt_tweak on your cluster to use ff3_encrypt_idwdata as udf\"\r\n        )\r\n      }\r\n    }\r\n    val encypt_col_df = spark.read\r\n      .table(Config.ht2_encrypt_columns_meta_table)\r\n      .where(\"lower(Table_Name) == '\" + Config.target_table.toLowerCase + \"'\")\r\n      .select(\"fields\")\r\n    val encrypt_col_list = if (encypt_col_df.count > 0) {\r\n      encypt_col_df\r\n        .collect()(0)(0)\r\n        .toString\r\n        .split(\",\")\r\n        .map(x => x.trim())\r\n        .toList\r\n    } else {\r\n      List[String]()\r\n    }\r\n\r\n    val null_col_df = spark.read\r\n      .table(Config.ht2_nullify_columns_meta_table)\r\n      .where(\"lower(Table_Name) == '\" + Config.target_table.toLowerCase + \"'\")\r\n      .select(\"fields\")\r\n    val null_cols_list = if (null_col_df.count() > 0) {\r\n      null_col_df\r\n        .collect()(0)(0)\r\n        .toString\r\n        .split(\",\")\r\n        .map(x => x.trim())\r\n        .toList\r\n    } else {\r\n      List[String]()\r\n    }\r\n\r\n    println(\"encrpt_list: \" + encrypt_col_list)\r\n    println(\"null_cols_list: \" + null_cols_list)\r\n\r\n    var finalOutputList = new ListBuffer[org.apache.spark.sql.Column]()\r\n    sksConfig.sks.zipWithIndex.map { ele =>\r\n      val x = ele._1\r\n      val placeholder_table_config = if (x.tableName != Config.target_table) {\r\n        pkSKDefConfig.get(x.tableName).get\r\n      } else {\r\n\r\n        PkSKDef(\r\n          \"\",\r\n          \"\",\r\n          col_list_wout_audit_column,\r\n          Config.target_table\r\n        )\r\n      }\r\n      finalOutputList += struct(\r\n        placeholder_table_config.pkCols.zip(x.nkCol).map { y =>\r\n          if (null_cols_list.contains(y._1)) {\r\n            lit(null).cast(\"string\").as(y._1)\r\n          } else if (encrypt_col_list.contains(y._1)) {\r\n            expr(\r\n              \"ff3_encrypt_idwdata_new('\" + ff3_encrypt_key + \"', '\" + ff3_encrypt_tweak + \"', \" + y._2 + \")\"\r\n            ).as(y._1)\r\n          } else {\r\n            expr(y._2).as(y._1)\r\n          }\r\n        }: _*\r\n      ).as(x.skCol)\r\n\r\n    }\r\n    val ht2_df = defaulted_df.select(finalOutputList: _*)\r\n\r\n    println(\r\n      \"ht2_idw file write start time: \" + Instant\r\n        .now()\r\n        .atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n    val path =\r\n      load_ready_insert_path + \"/\" + Config.target_table + \"/insert_files/\" + run_id + \"/\"\r\n    ht2_df\r\n      .repartition(1)\r\n      .write\r\n      .format(\"parquet\")\r\n      .mode(\"overwrite\")\r\n      .save(path)\r\n\r\n    println(\r\n      \"ht2_idw file single load ready write start time: \" + Instant\r\n        .now()\r\n        .atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n    val finalInsertPath = new Path(\r\n      baseFilePath + \"/\" + Config.target_table + \"/encrypt.ht2_idw.\" + Config.target_table + \".\" + run_id + \".parquet\"\r\n    )\r\n    copyMerge(hdfs, new Path(path), hdfs, finalInsertPath, true, hadoopConfig)\r\n    println(\r\n      \"ht2_idw file write end time: \" + Instant\r\n        .now()\r\n        .atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n\r\n    in0\r\n  } else {\r\n    in0\r\n  }\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "uBR1aQ01XBKZLCyNC-N0I$$yXxFyuwiwRcKUmfdRF6xY" : {
      "id" : "uBR1aQ01XBKZLCyNC-N0I$$yXxFyuwiwRcKUmfdRF6xY",
      "component" : "Script",
      "metadata" : {
        "label" : "create_update_load_ready_file",
        "slug" : "create_update_load_ready_file",
        "x" : 6620,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "t72zOle2b1rnyrFF1aLUD$$OttCQZg8uMzKuYawo5Jbe",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "TVjsclY53stxPQuDMdqa7$$0X36X6IfQHdXNRmvsKFpK",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"##################################################\")\nprintln(\"#####Step name: create_update_load_ready_file#####\")\nprintln(\"##################################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nif (Config.skip_main_table_load_ready_files == \"false\") {\n  if (\n    Config.generate_load_ready_files == \"true\" || (Config.generate_only_load_ready_files == \"true\")\n  ) {\n    println(\"Writing update load ready temp file\")\n    val path =\n      Config.load_ready_update_path + \"/\" + Config.target_table + \"/update_files/\" + spark.conf\n        .get(\"run_id\") + \"/\"\n    in0\n      .repartition(1)\n      .drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\")\n      .write\n      .format(\"parquet\")\n      .mode(\"append\")\n      .save(path)\n  }\n}\nval out0 = in0\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "19Es0MHx1pfrBprQj5hG7$$S64p_J0adIrQbxOJnJ8fO" : {
      "id" : "19Es0MHx1pfrBprQj5hG7$$S64p_J0adIrQbxOJnJ8fO",
      "component" : "Script",
      "metadata" : {
        "label" : "survivorship_rule",
        "slug" : "survivorship_rule",
        "x" : 3620,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "oRV4BTz9DbhX04lp0tF_T$$H5dUoTz98O9ktsWvnlPON",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "efNyA9RzhHjqVAh8HNTDA$$ggq_eo_2HZzg8ZJhZ5njv",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"######################################\")\r\nprintln(\"#####Step name: survivorship_rule#####\")\r\nprintln(\"######################################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\nval isPKSet = spark.conf.getAll.contains(\"primary_key\")\r\nval prim_key_columns = if (isPKSet) {\r\n  spark.conf\r\n    .get(\"primary_key\")\r\n    .split(\",\")\r\n    .map(_.trim.toLowerCase)\r\n} else {\r\n  Config.primary_key\r\n    .split(\",\")\r\n    .map(_.trim.toLowerCase)\r\n}\r\n\r\nvar out0 =\r\n  if (\r\n    Config.survivorship_flag && !prim_key_columns\r\n      .contains(\"src_env_sk\") &&\r\n    spark.catalog.tableExists(\r\n      s\"${Config.target_table_db}.${Config.target_table}\"\r\n    ) &&\r\n    spark.conf.get(\"new_data_flag\") == \"true\"\r\n  ) {\r\n\r\n    val tgt_tbl_nm =\r\n      if (\r\n        Config.survivorship_override_tables\r\n          .split(\",\")\r\n          .map(_.trim)\r\n          .contains(Config.target_table)\r\n      ) Config.target_table\r\n      else \"-\"\r\n    val srcEnvRnk = lookup(\r\n      \"lookup_src_envrt_id\",\r\n      col(Config.survivorship_lookup_column).cast(StringType),\r\n      lit(tgt_tbl_nm)\r\n    ).getField(\"src_env_rnk\")\r\n    val in1SrcEnvRnk = lookup(\r\n      \"lookup_src_envrt_id\",\r\n      col(\"in1_src_env_sk\").cast(StringType),\r\n      lit(tgt_tbl_nm)\r\n    ).getField(\"src_env_rnk\")\r\n\r\n    in0\r\n      .withColumn(\"src_env_rnk\", coalesce(srcEnvRnk, lit(9999)))\r\n      .withColumn(\"in1_src_env_rnk\", coalesce(in1SrcEnvRnk, lit(9999)))\r\n      .where(\"cast (src_env_rnk as long)  <= cast(in1_src_env_rnk as long)\")\r\n \r\n  } else {\r\n    in0\r\n  }\r\n\r\nif(Config.debug_flag) {\r\n  var printDf = out0\r\n  if(Config.debug_filter.toLowerCase() != \"none\"){\r\n    printDf = printDf.where(Config.debug_filter)\r\n  }  \r\n  if(Config.debug_col_list.toLowerCase() != \"none\"){\r\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\r\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\r\n  } else {\r\n    printDf.show(truncate=true)\r\n  }\r\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "EnfQ7uWDAR9PQgbLcfbip$$eizUFBt06Keq1uCYDN22P" : {
      "id" : "EnfQ7uWDAR9PQgbLcfbip$$eizUFBt06Keq1uCYDN22P",
      "component" : "Script",
      "metadata" : {
        "label" : "self_join_sk",
        "slug" : "self_join_sk",
        "x" : 3420,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "9yN2ElCio63iDJy3_8pak$$Ipx_CnwBZjJ_jy9-CzsRv",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ {
              "name" : "hold_catgy_desc",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_nm",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_catgy_priorty_id",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_id",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "mnl_intrvn_hold_flg",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_typ_sk",
              "type" : "long",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "type_code",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "patient_hold_flg",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_actv_start_dt",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_actv_end_dt",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "ref_info_desc",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_desc",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "src_env_sk",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "tat_hold_exclusion_flg",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_sk",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "file_name_timestamp",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_base_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_full_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_sys_id",
              "type" : "short",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            } ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "F86o336jAFqMEQvk3jmNk$$bXHqFX9yd37X61-UUZBIJ",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#################################\")\nprintln(\"#####Step name: self_join_sk#####\")\nprintln(\"#################################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    )\n\nval target_tbl = Config.target_table_db + \".\" + Config.target_table\nval isPKSet = spark.conf.getAll.contains(\"primary_key\")\nprintln(\"isPKSet: \"+isPKSet)\n\nval prim_key_columns = if (isPKSet) {\n  spark.conf\n\t.get(\"primary_key\")\n\t.split(\",\")\n\t.map(x => x.trim().toLowerCase())\n} else {\n  Config.primary_key\n\t.split(\",\")\n\t.map(x => x.trim().toLowerCase())\n}\nval isSKSet = spark.conf.getAll.contains(\"sk_service_col\")\nvar sk_service_col = if (isSKSet) {\n    spark.conf\n    .get(\"sk_service_col\").toLowerCase.trim\n  } else {\n    Config.sk_service_col.toLowerCase.trim\n  }\n\n\nval survivorshipCnd = (Config.survivorship_flag && !prim_key_columns\n  .contains(\"src_env_sk\"))\nval out0 =\n  if (\n    (survivorshipCnd || sk_service_col != \"none\") && spark.catalog\n      .tableExists(\n        target_tbl\n      ) && spark.conf.get(\"new_data_flag\") == \"true\"\n  ) {\n\n    import pureconfig._\n    import pureconfig.generic.auto._\n    import scala.language.implicitConversions\n\n    // case class to define source\n    case class JoinDef(\n        fmt: String,\n        src: String,\n        joinCond: String,\n        valCols: List[String],\n        joinType: Option[String] = Some(\"left_outer\"),\n        filterCond: Option[String] = Some(\"true\"),\n        joinHint: Option[String] = Some(\"broadcast\")\n    )\n\n    val skColElement =\n      if (sk_service_col != \"none\")\n        List(\n          \"in1.\" + sk_service_col + \" as in1_\" + sk_service_col\n        )\n      else List.empty[String]\n      \n    val survivorshipElement =\n      if (survivorshipCnd) List(\"in1.src_env_sk as in1_src_env_sk\")\n      else List.empty[String]\n\n    val valColsLst = skColElement ::: survivorshipElement\n\n    val skColFinalElement =\n      if (sk_service_col != \"none\") List(col(sk_service_col))\n      else List.empty[org.apache.spark.sql.Column]\n    val survivorshipFinalElement =\n      if (survivorshipCnd) List(col(\"src_env_sk\"))\n      else List.empty[org.apache.spark.sql.Column]\n    val finalColsLst = skColFinalElement ::: survivorshipFinalElement\n\n    val joinDef =\n      if (\n        (prim_key_columns\n          .toSet & Config.encrypt_cols\n          .split(\",\")\n          .map(x => x.trim().toLowerCase())\n          .toSet).size > 0\n      ) {\n        val encrypt_columns =\n          Config.encrypt_cols.split(\",\").map(x => x.trim().toLowerCase())\n\n        var encrypt_key = \"Secret not defined\"\n        var encrypt_iv = \"Secret not defined\"\n        var decrypt_key = \"Secret not defined\"\n        var decrypt_iv = \"Secret not defined\"\n\n        import java.security.MessageDigest\n        import java.util\n        import javax.crypto.Cipher\n        import javax.crypto.spec.{IvParameterSpec, SecretKeySpec}\n        import org.apache.commons.codec.binary.Hex\n        import org.apache.spark.sql.functions._\n\n        try {\n\n          // Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\n          import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n          // loading db secrets\n          encrypt_key = dbutils.secrets.get(\n            scope = Config.encrypt_scope,\n            key = Config.encrypt_EncKey\n          )\n\n          encrypt_iv = dbutils.secrets.get(\n            scope = Config.encrypt_scope,\n            key = Config.encrypt_InitVec\n          )\n\n          decrypt_key = dbutils.secrets.get(\n            scope = Config.decrypt_scope,\n            key = Config.decrypt_EncKey\n          )\n\n          decrypt_iv = dbutils.secrets.get(\n            scope = Config.decrypt_scope,\n            key = Config.decrypt_InitVec\n          )\n          println(\"Found secrets successfully for encrypt decrypt UDFs.\")\n        } catch {\n          case e: Exception => {\n            println(\n              \"Please define databricks secrets for encrypt_key, decrypt_key, envrypt_iv and decrypt_iv on your cluster to use encrypt decrypt functions as udf\"\n            )\n          }\n        }\n\n        def encrypt(\n            key: String,\n            ivString: String,\n            plainValue: String\n        ): String = {\n          if (plainValue != null) {\n            val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n            cipher.init(\n              Cipher.ENCRYPT_MODE,\n              keyToSpec(key),\n              getIVSpec(ivString)\n            )\n            var encrypted_str =\n              Hex.encodeHexString(cipher.doFinal(plainValue.getBytes(\"UTF-8\")))\n            encrypted_str = encrypted_str\n            return encrypted_str\n          } else {\n            null\n          }\n        }\n\n        def decrypt(\n            key: String,\n            ivString: String,\n            encryptedValue: String\n        ): String = {\n          if (encryptedValue != null) {\n            val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n            cipher.init(\n              Cipher.DECRYPT_MODE,\n              keyToSpec(key),\n              getIVSpec(ivString)\n            )\n            new String(\n              cipher.doFinal(Hex.decodeHex(encryptedValue.toCharArray()))\n            )\n          } else {\n            null\n          }\n        }\n\n        def keyToSpec(key: String): SecretKeySpec = {\n          var keyBytes: Array[Byte] = (key).getBytes(\"UTF-8\")\n          // val sha: MessageDigest = MessageDigest.getInstance(\"MD5\")\n          keyBytes = Hex.decodeHex(key)\n          keyBytes = util.Arrays.copyOf(keyBytes, 16)\n          new SecretKeySpec(keyBytes, \"AES\")\n        }\n\n        def getIVSpec(IVString: String) = {\n          new IvParameterSpec(IVString.getBytes() ++ Array.fill[Byte](16-IVString.length)(0x00.toByte))\n        }\n\n        val encryptUDF = udf(encrypt _)\n        val decryptUDF = udf(decrypt _)\n        spark.udf.register(\"aes_encrypt_udf\", encryptUDF)\n        spark.udf.register(\"aes_decrypt_udf\", decryptUDF)\n\n        println(\"Registered encrypt and decrypt UDFs in self_join_sk\")\n\n        JoinDef(\n          fmt = \"table\",\n          src = target_tbl,\n          joinCond = prim_key_columns\n            .map { x =>\n              if (encrypt_columns.contains(x)) {\n                \"in1.\" + x + \" <=> \" + \"aes_encrypt_udf('\" + encrypt_key + \"', '\" + encrypt_iv + \"', \" + \"trim(in0.\" + x + \"))\"\n              } else {\n                \"in1.\" + x + \" <=> \" + \"trim(in0.\" + x + \")\"\n              }\n            }\n            .mkString(\" and \"),\n          valCols = valColsLst\n        )\n      } else {\n        JoinDef(\n          fmt = \"table\",\n          src = target_tbl,\n          joinCond = prim_key_columns\n            .map(x => \"trim(in0.\" + x + \") <=> \" + \"in1.\" + x)\n            .mkString(\" and \"),\n          valCols = valColsLst\n        )\n      }\n\n    var res = in0\n    val df =\n      if (joinDef.fmt == \"csv\") {\n        spark.read\n          .format(\"csv\")\n          .option(\"header\", true)\n          .option(\"sep\", \",\")\n          .load(joinDef.src)\n      } else if (joinDef.fmt == \"parquet\") {\n        spark.read.format(\"parquet\").load(joinDef.src)\n      } else {\n        spark.read.table(joinDef.src)\n      }\n    val final_cols =\n      List(\"in0.*\") ++ joinDef.valCols\n    val join_condition = expr(joinDef.joinCond)\n    res = res\n      .as(\"in0\")\n      .join(\n        df.where(joinDef.filterCond.get)\n          .select(\n            (prim_key_columns\n              .map(x => col(x))\n              .toList ++ finalColsLst\n            ): _*\n          )\n          .as(\"in1\"),\n        join_condition,\n        joinDef.joinType.get\n      )\n      .selectExpr(final_cols: _*)\n    if (sk_service_col != \"none\" && Config.sk_service_self_join != \"false\") {\n      res.withColumn(\n        sk_service_col,\n        coalesce( col(\"in1_\" + sk_service_col), col(sk_service_col))\n      )\n    } else {\n      res\n    }\n  } else {\n    in0\n  }\n\nif(Config.debug_flag) {\n  var printDf = out0\n  if(Config.debug_filter.toLowerCase() != \"none\"){\n    printDf = printDf.where(Config.debug_filter)\n  }  \n  if(Config.debug_col_list.toLowerCase() != \"none\"){\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\n  } else {\n    printDf.show(truncate=true)\n  }\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "QHWoT3ZPQuf1QukUU12ho$$z-PZk030XyvJVf9kka-AN" : {
      "id" : "QHWoT3ZPQuf1QukUU12ho$$z-PZk030XyvJVf9kka-AN",
      "component" : "Script",
      "metadata" : {
        "label" : "apply_default_rules",
        "slug" : "apply_default_rules",
        "x" : 4620,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "l1sRXScC1juVDd3HjiW0b$$narbM55gewR0rkZH4E_vv",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "jBik_iDUudAn_JBfLDK0h$$r4QTQrOFyOQfsVOOf_OJC",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "// parse config json of key-value pair where key is target column name and value is data type\nprintln(\"#####Step name: apply_default_rules#####\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval out0 =\n  if (\n    Config.final_table_schema != \"None\" && spark.conf.get(\n      \"new_data_flag\"\n    ) == \"true\"\n  ) {\n    import org.json4s._\n    import org.json4s.jackson.JsonMethods._\n    import scala.collection.mutable.ListBuffer\n    import spark.sqlContext.implicits._\n\n    def jsonStrToMap(jsonStr: String): Map[String, String] = {\n      implicit val formats: DefaultFormats.type = org.json4s.DefaultFormats\n      parse(jsonStr).extract[Map[String, String]]\n    }\n\n    val audit_columns = Config.audit_cols.split(\",\").map(x => x.trim()).toList\n\n    val col_map = jsonStrToMap(Config.final_table_schema)\n\n    val col_values = col_map.keySet.toList\n\n    var outputList = new ListBuffer[org.apache.spark.sql.Column]()\n\n    val isPKSet = spark.conf.getAll.contains(\"primary_key\")\n    val prim_key_columns = if (isPKSet) {\n      spark.conf\n        .get(\"primary_key\")\n        .split(\",\")\n        .map(_.trim.toLowerCase)\n    } else {\n      Config.primary_key\n        .split(\",\")\n        .map(_.trim.toLowerCase)\n    }\n\n    val isSKSet = spark.conf.getAll.contains(\"sk_service_col\")\n    val sk_service_col = if (isSKSet) {\n      spark.conf\n        .get(\"sk_service_col\")\n        .toLowerCase\n        .trim\n    } else {\n      Config.sk_service_col.toLowerCase.trim\n    }\n\n    for (col_name <- in0.columns) {\n      if (prim_key_columns.contains(col_name)) {\n        if (col_values.contains(col_name)) {\n          val expression = col_map.get(col_name).get match {\n            case \"string\" =>\n              \"case when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else '-' end\"\n            case \"date\" =>\n              \"coalesce(to_date(trim(\" + col_name + \"), 'yyyyMMdd'), to_date(trim(\" + col_name + \"), 'yyyy-MM-dd'), CAST(null as date))\"\n            case \"timestamp\" =>\n              \"coalesce(to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmssSSS'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmss'), to_timestamp(rpad(\"+ col_name +\",23,'0'), 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(\" + col_name + \", 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss'), CAST(null as timestamp))\"\n            case value if value.startsWith(\"decimal\") => \n              if (Config.default_to_decimal_type == \"true\") {\n                \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as \" + value + \")\"\n              } else {\n                \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\n              }\n            case value if List(\"float\", \"double\").contains(value) =>\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\n            case value if value.contains(\"struct\") || value.contains(\"array\") =>\n              col_name\n            case _ =>\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as \" + col_map\n                .get(col_name)\n                .get + \")\"\n          }\n          outputList += expr(expression).as(col_name)\n        } else {\n          val expression =\n            \"case when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else '-' end\"\n          outputList += expr(expression).as(col_name)\n        }\n      } else if (\n        audit_columns.contains(\n          col_name\n        ) || (Config.apply_defaults == \"false\" && col_values.contains(col_name))\n      ) {\n        val expression = col_map.get(col_name).get match {\n          case \"string\" =>\n            \"case when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else CAST(null as string) end\"\n          case \"date\" =>\n            \"coalesce(to_date(trim(\" + col_name + \"), 'yyyyMMdd'), to_date(trim(\" + col_name + \"), 'yyyy-MM-dd'), CAST(null as date))\"\n          case \"timestamp\" =>\n            \"coalesce(to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmss'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss'), CAST(null as timestamp))\"\n          case value if value.startsWith(\"decimal\") =>\n            if (Config.default_to_decimal_type == \"true\") {\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as \" + value + \")\"\n            } else {\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\n            }\n          case value if List(\"float\", \"double\").contains(value) =>\n            \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST(null as string) end as double)\"\n          case value if value.contains(\"struct\") || value.contains(\"array\") =>\n            col_name\n          case _ =>\n            \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else null end as \" + col_map\n              .get(col_name)\n              .get + \")\"\n        }\n        outputList += expr(expression).as(col_name)\n      } else if (col_values.contains(col_name) && col_name.endsWith(\"_dt_sk\")) {\n        val expression =\n          \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else '19000101' end as \" + col_map\n            .get(col_name)\n            .get + \")\"\n        outputList += expr(expression).as(col_name)\n      } else if (col_values.contains(col_name) && col_name == sk_service_col && Config.enable_negative_one_self_join_sk) { \n        println(\"applying default with -9191 condition...\")\n        val expression =\n          \"cast(coalesce(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else '-9191' end, '-9191') as \" + col_map\n            .get(col_name)\n            .get + \")\"\n        outputList += expr(expression).as(col_name)\n      } else if (col_values.contains(col_name) && col_name == sk_service_col) {\n        val expression =\n          \"cast(coalesce(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else '-1' end, '-1') as \" + col_map\n            .get(col_name)\n            .get + \")\"\n        outputList += expr(expression).as(col_name)\n      } else if (col_values.contains(col_name) && col_name.endsWith(\"_sk\")) {\n        val expression =\n          \"cast(coalesce(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else '-9090' end, '-9090') as \" + col_map\n            .get(col_name)\n            .get + \")\"\n        outputList += expr(expression).as(col_name)\n      } else if (col_values.contains(col_name)) {\n        val expression = col_map.get(col_name).get match {\n          case \"string\" =>\n            \"case when lower(\" + col_name + \") == 'null' then '-' when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else '-' end\"\n          case \"date\" =>\n            \"coalesce(to_date(trim(\" + col_name + \"), 'yyyyMMdd'), to_date(trim(\" + col_name + \"), 'yyyy-MM-dd'), CAST('1900-01-01' as date))\"\n          case \"timestamp\" =>\n            \"coalesce(to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmssSSS'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMddHHmmss'), to_timestamp(rpad(\"+ col_name +\",23,'0'), 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(\" + col_name + \", 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyy-MM-dd HH:mm:ss'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss.SSS'), to_timestamp(trim(\" + col_name + \"), 'yyyyMMdd HH:mm:ss'), CAST('1900-01-01 00:00:00' as timestamp))\"\n          case value if value.startsWith(\"decimal\") =>\n            if (Config.default_to_decimal_type == \"true\") {\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST('0' as string) end as \" + value + \")\"\n            } else {\n              \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST('0' as string) end as double)\"\n            }\n          case value if List(\"float\", \"double\").contains(value) =>\n            \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else CAST('0' as string) end as double)\"\n          case value if value.contains(\"struct\") || value.contains(\"array\") =>\n            col_name\n          case _ =>\n            \"cast(case when \" + col_name + \" is not null then format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########') else '0' end as \" + col_map\n              .get(col_name)\n              .get + \")\"\n        }\n        outputList += expr(expression).as(col_name)\n      } else {\n        val expression =\n          \"coalesce(format_number(cast(\" + col_name + \" as decimal(38,10)), '0.##########'), \" + col_name + \")\"\n        outputList += expr(col_name).as(col_name)\n      }\n    }\n\n    in0.select(outputList: _*)\n  } else {\n    in0 // if no target schema is provided\n  }\n\nif(Config.debug_flag) {\n  var printDf = out0\n  if(Config.debug_filter.toLowerCase() != \"none\"){\n    printDf = printDf.where(Config.debug_filter)\n  }  \n  if(Config.debug_col_list.toLowerCase() != \"none\"){\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\n  } else {\n    printDf.show(truncate=true)\n  }\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "Ij_U4Axq1dHCVC-2pIWvC$$BAYL6HtkUYT8c0-H07jNw" : {
      "id" : "Ij_U4Axq1dHCVC-2pIWvC$$BAYL6HtkUYT8c0-H07jNw",
      "component" : "Script",
      "metadata" : {
        "label" : "create_insert_load_ready_file",
        "slug" : "create_insert_load_ready_file",
        "x" : 6620,
        "y" : 20,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "scMK0Y2Z3Ed7e7j-Rjhp2$$4iEoVYfYbT9eM7cswJOdw",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "3ldfxfgulZD-C6uw7Kuqw$$LPrcQ2xenhLINX51frUz_",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"##################################################\")\nprintln(\"#####Step name: create_insert_load_ready_file#####\")\nprintln(\"##################################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nif (Config.skip_main_table_load_ready_files == \"false\") {\n  if (\n    Config.generate_load_ready_files == \"true\" || (Config.generate_only_load_ready_files == \"true\")\n  ) {\n    println(\"Writing insert load ready temp file\")\n    val path =\n      Config.load_ready_insert_path + \"/\" + Config.target_table + \"/insert_files/\" + spark.conf\n        .get(\"run_id\") + \"/\"\n    in0\n      .repartition(1)\n      .drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\")\n      .write\n      .format(\"parquet\")\n      .mode(\"append\")\n      .save(path)\n  }\n}\nval out0 = in0\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "YNgpozNBwPuffrvW5YU9j$$raZHITPDu9IJ8awefPWUu" : {
      "id" : "YNgpozNBwPuffrvW5YU9j$$raZHITPDu9IJ8awefPWUu",
      "component" : "Script",
      "metadata" : {
        "label" : "cleanse_hex",
        "slug" : "cleanse_hex",
        "x" : 1020,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "p4ooTouM-aAbjrxGhHKWS$$4zMJAtvPm4L-wKlLzlUWG",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ {
              "name" : "dxf_src_dataset_id",
              "type" : "long",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_rec_cnt",
              "type" : "integer",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_sys_id",
              "type" : "short",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_file_name",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_id",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "name",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "type_code",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "description",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "start_date_active",
              "type" : "timestamp",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "end_date_active",
              "type" : "timestamp",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "order_holds_update_ts",
              "type" : "timestamp",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "newline",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_full_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "file_name_timestamp",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_base_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            } ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "6V3etwaq0Cv1FnkcU95dP$$StmFs5w0QXwO3f6Ghb7sl",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"################################\")\r\nprintln(\"#####Step name: cleanse_hex#####\")\r\nprintln(\"################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    ) \r\n\r\nval lower_case_col_df = in0.select(in0.columns.map(x => col(x).as(x.toLowerCase)): _*)\r\nval out0 =\r\n  if (\r\n    spark.conf.get(\r\n      \"new_data_flag\"\r\n    ) == \"true\" && Config.hex_cleanse_pattern != \"None\"\r\n  ) {\r\n\r\n    import scala.util.matching.Regex\r\n\r\n    val stringColumns = lower_case_col_df.schema.fields\r\n      .filter(_.dataType == org.apache.spark.sql.types.StringType)\r\n      .map(_.name)\r\n\r\n    val pattern = Config.hex_cleanse_pattern\r\n\r\n    import scala.collection.mutable.ListBuffer\r\n    var outputList = new ListBuffer[org.apache.spark.sql.Column]()\r\n\r\n    println(\"Cleaning hex values from string columns\")\r\n    lower_case_col_df.columns.foreach { x =>\r\n      if (stringColumns.contains(x)) {\r\n        outputList += regexp_replace(col(x), pattern, Config.hex_replace_value).as(x)\r\n      } else {\r\n        outputList += col(x)\r\n      }\r\n    }\r\n    lower_case_col_df.select(outputList: _*)\r\n  } else {\r\n    lower_case_col_df\r\n  }\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "bQ1XrnLqKirtJc3djEyNV$$mRnIsUCOUdF2KfuRJI0XC" : {
      "id" : "bQ1XrnLqKirtJc3djEyNV$$mRnIsUCOUdF2KfuRJI0XC",
      "component" : "Script",
      "metadata" : {
        "label" : "write_target",
        "slug" : "write_target",
        "x" : 5820,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "-fO7GCzOJHWgRek5RVhaW$$vfvw9OtRsr5YXpOgkI0nz",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "6kc7H7P7v6cH48fcVKqKi$$FRAUe5WSyGZ3-7bgZc5ld",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n// this component writes output to delta table\nprintln(\"#################################\")\nprintln(\"#####Step name: write_target#####\")\nprintln(\"#################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval target_tbl = Config.target_table_db + \".\" + Config.target_table\n\nval current_ts_val = to_timestamp(\n  from_utc_timestamp(current_timestamp(), \"America/Chicago\").cast(\"string\"),\n  \"yyyy-MM-dd HH:mm:ss\"\n)\n\nval run_id = spark.conf.get(\"run_id\")\nvar run_id_for_data = run_id\nif (Config.custom_run_id_suffix != \"None\") {\n  run_id_for_data = run_id.substring(0, 8) + Config.custom_run_id_suffix\n}\n\nval uid_col = Config.get_insert_uid_from_source_df match {\n          case true if in0.columns.contains(\"insert_uid\") => col(\"insert_uid\")\n          case _ => Config.uid.toLowerCase match {\n            case \"none\"  => {\n              import java.nio.charset.StandardCharsets\n              // Hex string\n              val hexString = \"E281A3\"\n              // Convert hex string to byte array\n              val byteArray = hexString.sliding(2, 2).toArray.map(Integer.parseInt(_, 16).toByte)\n              // Decode byte array to UTF-8 string\n              val invisibleSep = new String(byteArray, StandardCharsets.UTF_8)\n              val secret = dbutils.secrets.get(scope = Config.api_scope, key = Config.update_uid_key)\n              val plaintextSecret = secret.replace(\"\", invisibleSep)\n              lit(plaintextSecret)\n            }\n            case _ => lit(Config.uid)\n          }\n        }\n\n\n// list of primary key columns\nval isPKSet = spark.conf.getAll.contains(\"primary_key\")\nvar prim_key_columns = if (isPKSet) {\n  spark.conf\n    .get(\"primary_key\")\n    .split(\",\")\n    .map(_.trim.toLowerCase)\n} else {\n  Config.primary_key\n    .split(\",\")\n    .map(_.trim.toLowerCase)\n}\n\nif (Config.merge_logic_including_sk) {\n  val isSKSet = spark.conf.getAll.contains(\"sk_service_col\")\n  val sk_service_col = if (isSKSet) {\n    spark.conf\n      .get(\"sk_service_col\")\n      .toLowerCase\n      .trim\n  } else {\n    Config.sk_service_col.toLowerCase.trim\n  }\n  if (sk_service_col != \"-\" && sk_service_col != \"none\") {\n    prim_key_columns =\n      List.concat(List(sk_service_col), prim_key_columns.toList).toArray\n  }\n\n}\n\nimport scala.util.Try\n// Fetch partition columns from target table\nval partition_cols: List[String] = Try {\n  spark\n    .sql(s\"SHOW PARTITIONS $target_tbl\")\n    .columns\n    .map(_.toLowerCase.trim)\n    .toList\n}.getOrElse(Nil)\n\n// Parse explicit partition columns from config\nval partition_column_explicit: List[String] =\n  Config.partition_column_explicit.toLowerCase match {\n    case \"none\"  => Nil\n    case columns => columns.split(\",\").map(_.toLowerCase.trim).toList\n  }\n\n// Combine explicit partition columns with primary key columns and partition columns, then remove duplicates\nprim_key_columns = (partition_column_explicit ++ (prim_key_columns.intersect(\n  partition_cols.map(_.trim.toLowerCase)\n).toList) ++ prim_key_columns).distinct.toArray\n\nif (spark.conf.get(\"new_data_flag\") == \"true\") {\n  if (spark.catalog.tableExists(target_tbl)) {\n\n    import _root_.io.delta.tables._\n\n    val hash_cols = Config.hash_cols.split(\",\").map(x => x.trim())\n\n    // list of columns to check for updated values in case of delta merge\n\n    val audit_columns =\n      Config.audit_cols.split(\",\").map(x => x.trim()).toList\n\n    var dup_check_column =\n      ((in0.columns.toList diff audit_columns).toList diff prim_key_columns).toList\n    // if (Config.hash_cols != \"None\") {\n    //   dup_check_column = (dup_check_column diff hash_cols).toList\n    // }\n    if (\n      Config.temp_output_flag == \"false\" && Config.target_write_type == \"append\" && Config.skip_delta_synapse_write_common_dimension == \"false\"\n    ) {\n      println(\"write_target TYPE: Append\")\n      in0.write\n        .format(\"delta\")\n        .option(\n          \"optimizeWrite\",\n          true\n        ) // https://docs.databricks.com/optimizations/auto-optimize.html\n        .option(\n          \"delta.enableChangeDataFeed\",\n          true\n        ) // https://docs.databricks.com/delta/delta-change-data-feed.html\n        .mode(\"append\")\n        .saveAsTable(target_tbl)\n    } else if (\n      Config.temp_output_flag == \"false\" && Config.target_write_type != \"scd2\" && Config.target_write_type != \"scd0\" && Config.skip_delta_synapse_write_common_dimension == \"false\"\n    ) {\n      println(\"write_target TYPE: SCD1\")\n      println(\"prim_key_columns: \")\n      prim_key_columns.foreach(println)\n\n      DeltaTable\n        .forName(target_tbl)\n        .as(\"target\")\n        .merge(\n          in0.as(\"source\"),\n          prim_key_columns\n            .map(x => col(\"source.\" + x).eqNullSafe(col(\"target.\" + x)))\n            .reduce(_ && _)\n        ) // merge is done on primary key of table\n        .whenMatched(\n          !(dup_check_column\n            .map(x => col(\"source.\" + x).eqNullSafe(col(\"target.\" + x)))\n            .reduce(_ && _)) && (col(\"source.rec_stat_cd\") <= col(\"target.rec_stat_cd\"))\n        ) // update will only happen if any of the underlying column is updated\n        .update(\n          (List(\n            \"insert_ts\" → col(\"target.insert_ts\"),\n            \"insert_uid\" → col(\"target.insert_uid\"),\n            \"update_ts\" → col(\"source.insert_ts\"),\n            \"update_uid\" → col(\"source.insert_uid\"),\n            \"run_id\" → col(\"source.run_id\"),\n            \"rec_stat_cd\" → col(\"source.rec_stat_cd\")\n          ) ++ dup_check_column.map(x => x → col(\"source.\" + x))).toMap\n        ) // update timestamp and uid columns for audit purpose\n        .whenNotMatched()\n        .insertAll()\n        .execute()\n    } else if (\n      Config.temp_output_flag == \"false\" && Config.target_write_type == \"scd2\" && Config.skip_delta_synapse_write_common_dimension == \"false\"\n    ) {\n      println(\"write_target TYPE: SCD2\")\n\n      case class SCD2Columns(scd2StartDt: String, scd2EndDt: String)\n      val scd2StartDt = Config.scd2_columns\n        .split(\",\")\n        .filter(_.contains(\"startTimestamp\"))\n        .array(0)\n        .split(\":\")\n        .array(1)\n        .trim\n      println(\"scd2StartDt: \" + scd2StartDt)\n      val scd2EndDt = Config.scd2_columns\n        .split(\",\")\n        .filter(_.contains(\"endTimestamp\"))\n        .array(0)\n        .split(\":\")\n        .array(1)\n        .trim\n      println(\"scd2EndDt: \" + scd2EndDt)\n      val SCD2ColumnNames = SCD2Columns(scd2StartDt, scd2EndDt)\n\n      val target_df = if (Config.scd2_exclude_source_list == \"None\") {\n        spark.read\n          .table(target_tbl)\n          .filter(\n            to_date(col(SCD2ColumnNames.scd2EndDt)) === lit(\"9999-12-31\")\n          )\n      } else {\n        val scd2_exclude_source_list = Config.scd2_exclude_source_list\n          .split(\",\")\n          .map(x => (\"'\" + x.trim + \"'\"))\n          .mkString(\",\")\n        spark.read\n          .table(target_tbl)\n          .filter(\n            to_date(col(SCD2ColumnNames.scd2EndDt)) === lit(\"9999-12-31\")\n          )\n          .where(s\"src_env_sk not in (${scd2_exclude_source_list})\")\n      }\n\n      def getSCD2PKCols(prefix: String): Seq[Column] = {\n        prim_key_columns\n          .filterNot(_ == SCD2ColumnNames.scd2StartDt)\n          .map(x => col(s\"$prefix.$x\"))\n      }\n\n      def buildSCD2PKCondition(prefix: String): Column = {\n        concat_ws(\"-\", getSCD2PKCols(prefix): _*)\n      }\n\n      import org.apache.spark.sql.functions._\n      val window = Window\n        .partitionBy(\n          prim_key_columns\n            .filterNot(_ == SCD2ColumnNames.scd2StartDt)\n            .map(x => col(x)): _*\n        )\n        .orderBy(desc(SCD2ColumnNames.scd2StartDt))\n      val inp_df =\n        in0.withColumn(\"rn\", row_number().over(window)).where(\"rn=1\").drop(\"rn\")\n\n      val insForExistRecDFJoin = inp_df\n        .as(\"source\")\n        .join(\n          target_df.as(\"target\"),\n          buildSCD2PKCondition(\"source\") === buildSCD2PKCondition(\"target\"),\n          \"left\"\n        )\n        .filter(\n          !dup_check_column\n            .map(x => col(s\"source.$x\").eqNullSafe(col(s\"target.$x\")))\n            .reduce(_ && _)\n        )\n        .where(\n          s\"to_date(source.${SCD2ColumnNames.scd2StartDt}) >= to_date(nvl(target.${SCD2ColumnNames.scd2StartDt}, '1900-01-01'))\"\n        )\n\n      val insRecords = insForExistRecDFJoin\n        .selectExpr(\"source.*\")\n\n      val updRecords = insForExistRecDFJoin\n        .where(\n          s\"source.${SCD2ColumnNames.scd2StartDt} <> target.${SCD2ColumnNames.scd2StartDt}\"\n        )\n        .drop(expr(s\"target.${SCD2ColumnNames.scd2EndDt}\"))\n        .selectExpr(\n          \"target.*\",\n          s\"cast(source.${SCD2ColumnNames.scd2StartDt} - INTERVAL 1 day as timestamp) as ${SCD2ColumnNames.scd2EndDt}\"\n        )\n        .withColumn(\"run_id\", lit(run_id_for_data))\n        .withColumn(\"insert_ts\", current_ts_val)\n        .withColumn(\n          \"insert_uid\",\n          substring(uid_col, 0, 20)\n        ) // audit_cols for updates\n\n      val inactiveRecordsDF = target_df\n        .as(\"target\")\n        .join(\n          inp_df.as(\"source\"),\n          buildSCD2PKCondition(\"source\") === buildSCD2PKCondition(\"target\"),\n          \"left_anti\"\n        )\n        .filter(to_date(col(SCD2ColumnNames.scd2EndDt)) === lit(\"9999-12-31\"))\n        .withColumn(SCD2ColumnNames.scd2EndDt, current_timestamp)\n        .withColumn(\"run_id\", lit(run_id_for_data))\n        .withColumn(\"insert_ts\", current_ts_val)\n        .withColumn(\n          \"insert_uid\",\n          substring(uid_col, 0, 20)\n        ) // audit_cols for updates\n\n      // new logic\n      val inactiveRecordsNormDF = target_df\n        .as(\"target\")\n        .join(\n          insRecords.as(\"source\"),\n          buildSCD2PKCondition(\"source\") === buildSCD2PKCondition(\"target\"),\n          \"inner\"\n        )\n        .filter(\n          insRecords.col(SCD2ColumnNames.scd2StartDt) =!= target_df.col(\n            SCD2ColumnNames.scd2StartDt\n          )\n        )\n        .drop(expr(s\"target.${SCD2ColumnNames.scd2EndDt}\"))\n        .selectExpr(\n          \"target.*\",\n          s\"cast(source.${SCD2ColumnNames.scd2StartDt} - INTERVAL 1 day as timestamp) as ${SCD2ColumnNames.scd2EndDt}\"\n        )\n        .withColumn(\"run_id\", lit(run_id_for_data))\n        .withColumn(\"insert_ts\", current_ts_val)\n        .withColumn(\n          \"insert_uid\",\n          substring(uid_col, 0, 20)\n        ) // audit_cols for updates\n\n      val stagedUpdates = insRecords\n        .unionByName(updRecords)\n        .unionByName(inactiveRecordsDF)\n        .unionByName(inactiveRecordsNormDF)\n        .filter(col(Config.sk_service_col).isNotNull)\n        .distinct\n\n      DeltaTable\n        .forName(target_tbl)\n        .as(\"target\")\n        .merge(\n          stagedUpdates.as(\"source\"),\n          prim_key_columns\n            .map(x => col(\"source.\" + x).eqNullSafe(col(\"target.\" + x)))\n            .reduce(_ && _)\n        ) // merge is done on primary key of table\n        .whenMatched(\n          !(dup_check_column\n            .map(x => col(\"source.\" + x).eqNullSafe(col(\"target.\" + x)))\n            .reduce(_ && _)) && (col(\"source.rec_stat_cd\") <= col(\"target.rec_stat_cd\"))\n        ) // update will only happen if any of the underlying column is updated\n        .update(\n          (List(\n            \"insert_ts\" → col(\"target.insert_ts\"),\n            \"insert_uid\" → col(\"target.insert_uid\"),\n            \"update_ts\" → col(\"source.insert_ts\"),\n            \"update_uid\" → col(\"source.insert_uid\"),\n            \"run_id\" → col(\"source.run_id\"),\n            \"rec_stat_cd\" → col(\"source.rec_stat_cd\")\n          ) ++ dup_check_column.map(x => x → col(\"source.\" + x))).toMap\n        ) // update timestamp and uid columns for audit purpose\n        .whenNotMatched()\n        .insertAll()\n        .execute()\n\n    } else if (\n      Config.temp_output_flag == \"false\" && Config.target_write_type == \"scd0\" && Config.skip_delta_synapse_write_common_dimension == \"false\"\n    ) {\n      println(\"write_target TYPE: SCD0\")\n      DeltaTable\n        .forName(target_tbl)\n        .as(\"target\")\n        .merge(\n          in0.as(\"source\"),\n          prim_key_columns\n            .map(x => col(\"source.\" + x).eqNullSafe(col(\"target.\" + x)))\n            .reduce(_ && _)\n        ) // merge is done on primary key of table\n        .whenNotMatched()\n        .insertAll()\n        .execute()\n    } else if (Config.temp_output_flag == \"true\") {\n      try {\n        spark.sql(\"truncate table \" + target_tbl)\n      } catch {\n        case e: Exception => {\n          spark.sql(\"drop table IF EXISTS \" + target_tbl)\n        }\n      }\n      in0.write\n        .format(\"delta\")\n        .option(\n          \"optimizeWrite\",\n          true\n        ) // https://docs.databricks.com/optimizations/auto-optimize.html\n        .option(\n          \"delta.enableChangeDataFeed\",\n          true\n        ) // https://docs.databricks.com/delta/delta-change-data-feed.html\n        .mode(\"overwrite\")\n        .saveAsTable(target_tbl)\n    }\n\n  } else { // creation of delta table if it does not exist\n    in0.write\n      .format(\"delta\")\n      .option(\n        \"optimizeWrite\",\n        true\n      ) // https://docs.databricks.com/optimizations/auto-optimize.html\n      .option(\n        \"delta.enableChangeDataFeed\",\n        true\n      ) // https://docs.databricks.com/delta/delta-change-data-feed.html\n      .mode(\"overwrite\")\n      .saveAsTable(target_tbl)\n  }\n} else {\n  if (\n    spark.conf.get(\n      \"new_data_flag\"\n    ) == \"false\" && Config.temp_output_flag == \"true\"\n  ) {\n    try {\n      spark.sql(\"truncate table \" + target_tbl)\n    } catch {\n      case e: Exception => {\n        spark.sql(\"drop table IF EXISTS \" + target_tbl)\n      }\n    }\n  }\n}\nval out0 =\n  if (\n    Config.skip_delta_synapse_write_common_dimension == \"true\" && spark.conf\n      .get(\"new_data_flag\") == \"true\"\n  ) {\n    in0\n      .as(\"source\")\n      .join(\n        spark.read.table(target_tbl).as(\"target\"),\n        prim_key_columns\n          .map(x => col(\"source.\" + x).eqNullSafe(col(\"target.\" + x)))\n          .reduce(_ && _),\n        \"left\"\n      )\n      .filter(\n        prim_key_columns\n          .map(x => col(\"target.\" + x).isNull)\n          .reduce(_ && _)\n      )\n      .select(\"source.*\")\n  } else {\n    spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))\n  }\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "VMN3zOsqy8XcvcScQl2KW$$tIbzBsKedXk0HnTmrZHWm" : {
      "id" : "VMN3zOsqy8XcvcScQl2KW$$tIbzBsKedXk0HnTmrZHWm",
      "component" : "Script",
      "metadata" : {
        "label" : "create_delta_file",
        "slug" : "create_delta_file",
        "x" : 6020,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "3Z-gE7d3EJP4IlhuiWKEr$$Zu-94wvFGyI0fA1_5k25x",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "Kr52eTVRp2Wj-qyMV9EZH$$pHCc2SwO5DP44eppo0hWb",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"######################################\")\nprintln(\"#####Step name: create_delta_file#####\")\nprintln(\"######################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval target_tbl = Config.target_table_db + \".\" + Config.target_table\nvar maxVersion = -1\nvar maxVersionPostSQLUpdate = -1\n\nif (\n  (Config.generate_load_ready_files == \"true\" && spark.conf.get(\n    \"new_data_flag\"\n  ) == \"true\" && Config.skip_delta_synapse_write_common_dimension == \"false\" && Config.skip_main_table_load_ready_files == \"false\") || (Config.generate_only_load_ready_files == \"true\")\n) {\n  maxVersion = spark\n    .sql(\n      \"select max(version) from (DESCRIBE HISTORY \" + target_tbl + \") temp where lower(temp.operation) <> 'optimize'\"\n    )\n    .collect()(0)\n    .getLong(0)\n    .toInt\n}\nmaxVersionPostSQLUpdate = maxVersion\nif (Config.post_delta_sql_update != \"None\") {\n  println(\"Running post SQL Query\")\n  spark.sql(Config.post_delta_sql_update)\n  maxVersionPostSQLUpdate = spark\n    .sql(\n      \"select max(version) from (DESCRIBE HISTORY \" + target_tbl + \") temp where lower(temp.operation) <> 'optimize'\"\n    )\n    .collect()(0)\n    .getLong(0)\n    .toInt\n\n}\n\nval temp_out0 =\n  if (\n    (Config.generate_load_ready_files == \"true\" && spark.conf.get(\n      \"new_data_flag\"\n    ) == \"true\" && Config.skip_delta_synapse_write_common_dimension == \"false\" && Config.skip_main_table_load_ready_files == \"false\") || (Config.generate_only_load_ready_files == \"true\")\n  ) {\n\n// read cdc data from delta table\n    if (Config.post_delta_sql_update != \"None\") {\n      import org.apache.spark.sql.expressions.Window\n      println(\"post sql deduplicate\")\n\n      val isPKSet = spark.conf.getAll.contains(\"primary_key\")\n      val prim_key_columns = if (isPKSet) {\n        spark.conf\n          .get(\"primary_key\")\n          .split(\",\")\n          .map(x => x.trim())\n      } else {\n        Config.primary_key\n          .split(\",\")\n          .map(x => x.trim())\n      }\n\n      val temp_df = spark.read\n        .format(\"delta\")\n        .option(\"readChangeData\", true)\n        .option(\"startingVersion\", maxVersion)\n        .option(\"endingVersion\", maxVersionPostSQLUpdate)\n        .table(target_tbl)\n        .where(\"_change_type in ('update_postimage', 'insert')\")\n\n      val window = Window\n        .partitionBy(prim_key_columns.map(x => col(x)): _*)\n        .orderBy(\n          col(\"_commit_version\").desc_nulls_last,\n          col(\"update_ts\").desc_nulls_last\n        )\n      temp_df\n        .withColumn(\"dedup_row_num\", row_number().over(window))\n        .where(\"dedup_row_num == 1\")\n        .drop(\"dedup_row_num\")\n\n    } else {\n      spark.read\n        .format(\"delta\")\n        .option(\"readChangeData\", true)\n        .option(\"startingVersion\", maxVersion)\n        .option(\"endingVersion\", maxVersionPostSQLUpdate)\n        .table(target_tbl)\n        .where(\"_change_type in ('update_postimage', 'insert')\")\n    }\n  } else if (\n    Config.skip_delta_synapse_write_common_dimension == \"true\" && spark.conf\n      .get(\n        \"new_data_flag\"\n      ) == \"true\"\n  ) {\n    in0\n  } else {\n    val target_tbl = Config.target_table_db + \".\" + Config.target_table\n    if (\n      Config.generate_load_ready_files == \"true\" && Config.skip_main_table_load_ready_files == \"false\"\n    ) {\n      spark.read.table(target_tbl).where(\"1 == 2\")\n    } else {\n      in0\n    }\n  }\n\nval out0 = if (Config.remove_partition_cols_in_load_ready != \"None\") {\n  println(\n    \"dropping partition cols: \" + Config.remove_partition_cols_in_load_ready\n  )\n  val partition_cols_in_delta =\n    Config.remove_partition_cols_in_load_ready.split(\",\").map(x => x.trim())\n  temp_out0.drop(partition_cols_in_delta: _*)\n} else {\n  temp_out0\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "b5iLKRgznTuJnTToFoHFs$$10R6GPkgJMTqSZvlQno0S" : {
      "id" : "b5iLKRgznTuJnTToFoHFs$$10R6GPkgJMTqSZvlQno0S",
      "component" : "Script",
      "metadata" : {
        "label" : "put_src_env_sk",
        "slug" : "put_src_env_sk",
        "x" : 1620,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "3tAwEeSMIk_yCnqS2V89c$$HPmwhn_vGngoazfhOrcZs",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "Mldt1wmYMOpv6nAc8aBl3$$8ZTts6QCo4sXxcPR69xCq",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"###################################\")\nprintln(\"#####Step name: put_src_env_sk#####\")\nprintln(\"###################################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    ) \n\nval out0 = if (spark.conf.get(\"new_data_flag\") == \"true\") {\n  if (in0.columns.contains(\"dxf_src_sys_id\")) {\n    in0\n  } else if (in0.columns.contains(\"src_env_sk\")) {\n    in0.withColumn(\"dxf_src_sys_id\", col(\"src_env_sk\"))\n  } else {\n    in0.withColumn(\"dxf_src_sys_id\", lit(Config.src_env_sk).cast(\"string\"))\n  }\n} else {\n  in0\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "WPwvwUYSyzyGJwGrxog7E$$dila5inO1kk-iAWbfki_0" : {
      "id" : "WPwvwUYSyzyGJwGrxog7E$$dila5inO1kk-iAWbfki_0",
      "component" : "Script",
      "metadata" : {
        "label" : "spark_configs",
        "slug" : "spark_configs",
        "x" : 20,
        "y" : 220,
        "phase" : -2,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ ],
        "outputs" : [ {
          "id" : "D05MewfEUW-_zQJPLI796$$51yytXgldVB6xUX-l0A0r",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"##################################\")\r\nprintln(\"#####Step name: spark_configs#####\")\r\nprintln(\"##################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    ) \r\nval sparkConfigs = Config.spark_configs.split(\",\")\r\n\r\nval sparkSession = sparkConfigs\r\n  .foldLeft(SparkSession.builder().appName(\"Prophecy Pipeline\")) {\r\n    case (s, configStr) =>\r\n      val Array(key, value) = configStr.split(\"=\").map(_.trim)\r\n      s.config(key, value)\r\n  }\r\n  .enableHiveSupport()\r\n  .getOrCreate()\r\n  .newSession()\r\n\r\nval out0 = spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))",
        "scriptMethodHeader" : "def apply(spark: SparkSession): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "1w2Y-FhwG2D2tTokmhCdC$$rxXAwHre2rqCfbqgrQA33" : {
      "id" : "1w2Y-FhwG2D2tTokmhCdC$$rxXAwHre2rqCfbqgrQA33",
      "component" : "Script",
      "metadata" : {
        "label" : "write_audit_log",
        "slug" : "write_audit_log",
        "x" : 7420,
        "y" : 220,
        "phase" : 2,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "NAnmHEhSKy-Kmd02t1ix2$$r7De1FHtI2JLdA5Qkp8I1",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "// this writes the audit summary for the run\nprintln(\"####################################\")\nprintln(\"#####Step name: write_audit_log#####\")\nprintln(\"####################################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    )\n\nif (Config.disable_audit_summary == \"false\") {\n  println(\"Writing audit summary for pipeline run\")\n  val target_tbl = Config.target_table_db + \".\" + Config.target_table\n\n  val run_id = spark.conf.get(\"run_id\").toLong\n  val source_count = spark.conf.get(\"source_count\").toLong\n  val source_filter_count =\n    source_count - spark.conf.get(\"source_filter_count\").toLong\n  val reject_record_count = spark.conf.get(\"reject_record_count\").toLong\n  val duplicate_filtered_record_count =\n    spark.conf.get(\"source_filter_count\").toLong - reject_record_count - spark.conf\n      .get(\"duplicate_filtered_record_count\")\n      .toLong\n  val post_filtered_record_count = spark.conf.get(\"duplicate_filtered_record_count\").toLong - spark.conf\n    .get(\"post_filtered_record_count\")\n    .toLong\n\n  val insert_load_ready_count = spark.conf.get(\"insert_load_ready_count\").toLong\n  val update_load_ready_count = spark.conf.get(\"update_load_ready_count\").toLong\n\n  val min_processed_file_timestamp =\n    spark.conf.get(\"min_processed_file_timestamp\")\n  val max_processed_file_timestamp =\n    spark.conf.get(\"max_processed_file_timestamp\")\n  val processed_file_count = spark.conf.get(\"processed_file_count\").toInt\n\n  val auditColumns = Seq(\n    \"data_mart\",\n    \"pipeline_name\",\n    \"run_id\",\n    \"source_count\",\n    \"pre_filter_record_count\",\n    \"reject_record_count\",\n    \"deduplicate_record_count\",\n    \"post_filter_record_count\",\n    \"insert_load_ready_count\",\n    \"update_load_ready_count\",\n    \"min_processed_file_timestamp\",\n    \"max_processed_file_timestamp\",\n    \"processed_file_count\"\n  )\n\n  val auditDf = spark\n    .createDataFrame(\n      Seq(\n        (\n          Config.data_mart,\n          Config.pipeline_name,\n          run_id,\n          source_count,\n          source_filter_count,\n          reject_record_count,\n          duplicate_filtered_record_count,\n          post_filtered_record_count,\n          insert_load_ready_count,\n          update_load_ready_count,\n          min_processed_file_timestamp,\n          max_processed_file_timestamp,\n          processed_file_count\n        )\n      )\n    )\n    .toDF(auditColumns: _*)\n\n  // write audit summary output to delta table\n  auditDf.write\n    .format(\"delta\")\n    .option(\"delta.enableChangeDataFeed\", true)\n    .partitionBy(\"pipeline_name\")\n    .mode(\"append\")\n    .saveAsTable(Config.audit_summary)\n\n  println(\"Process completed for table:\" + Config.target_table)\n}\nspark.sqlContext.clearCache()\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): Unit = {",
        "scriptMethodFooter" : "    \n}"
      }
    },
    "SP3GIx64LeRuO7vfX5COW$$zswxLvkA2uhHN3zr6zrKE" : {
      "id" : "SP3GIx64LeRuO7vfX5COW$$zswxLvkA2uhHN3zr6zrKE",
      "component" : "Script",
      "metadata" : {
        "label" : "select_final_cols",
        "slug" : "select_final_cols",
        "x" : 5420,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "Mwjm-Lp3o9_XY1sYIUfPJ$$0e0wh5rpsnlMa4ejbXbMd",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "v6k1wg79g1kZlq64irXUB$$-njhhKtYpyz3mTFOaKAHD",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "// parse config json of key-value pair where key is target column name and value is data type\r\n// select required fields as per target schema\r\nprintln(\"######################################\")\r\nprintln(\"#####Step name: select_final_cols#####\")\r\nprintln(\"######################################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\nval out0 =\r\n  if (\r\n    Config.final_table_schema != \"None\" && spark.conf.get(\r\n      \"new_data_flag\"\r\n    ) == \"true\"\r\n  ) {\r\n    try {\r\n      import org.json4s._\r\n      import org.json4s.jackson.JsonMethods._\r\n      import scala.collection.mutable.ListBuffer\r\n      import spark.sqlContext.implicits._\r\n      import play.api.libs.json._\r\n\r\n      val json = Json.parse(Config.final_table_schema)\r\n\r\n      val col_values = json.as[JsObject].keys.toSeq\r\n\r\n      in0.select(col_values.map { x =>\r\n        if (x.endsWith(\"_sk\") && x != \"src_env_sk\") {\r\n          when(col(x) === \"-9090\" || col(x) === \"-9191\", lit(-1).cast(\"long\")).otherwise(col(x)).as(x)\r\n        } else { col(x) }\r\n      }: _*)\r\n    } catch {\r\n      case e: Exception => {\r\n        println(\r\n          s\"\"\"Process failed while selecting final columns\"\"\"\r\n        )\r\n\r\n        if (spark.conf.get(\"main_table_api_type\") == \"NON-API\"){\r\n          import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n          println(s\" Removing lock from: ${Config.sk_table_name_override}\")\r\n          dbutils.fs.rm(Config.sk_lock_file_path + Config.sk_table_name_override + \".txt\")\r\n        }\r\n        throw e\r\n      }\r\n    }\r\n  } else {\r\n    in0 // if no target schema is provided\r\n  }\r\n\r\nif(Config.debug_flag) {\r\n  var printDf = out0\r\n  if(Config.debug_filter.toLowerCase() != \"none\"){\r\n    printDf = printDf.where(Config.debug_filter)\r\n  }  \r\n  if(Config.debug_col_list.toLowerCase() != \"none\"){\r\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\r\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\r\n  } else {\r\n    printDf.show(truncate=true)\r\n  }\r\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "x6-e9o7PmYi74M-VxDFOS$$1twtczQfqc0UZllW4Pive" : {
      "id" : "x6-e9o7PmYi74M-VxDFOS$$1twtczQfqc0UZllW4Pive",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_normalize",
        "slug" : "optional_normalize",
        "x" : 2420,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "DamBQ0RtEhBQNhYA2sEW7$$gLySIy4zuro9EWaXJDUOU",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "EmpZkrGIn8sEN1ZGk0blC$$J8YVSku8UXBfXdOvGyUvt",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#######################################\")\r\nprintln(\"#####Step name: optional_normalize#####\")\r\nprintln(\"#######################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n\r\nval out0 = if (Config.normalize_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\") {\r\n  in0.withColumn(\"normalized_col\", explode_outer(expr(Config.normalize_rules)))\r\n} else {\r\n  in0\r\n}\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "w2MZMOsj39JbiRhPXWShH$$8PJNM3348yL9FIrwlHOzn" : {
      "id" : "w2MZMOsj39JbiRhPXWShH$$8PJNM3348yL9FIrwlHOzn",
      "component" : "Script",
      "metadata" : {
        "label" : "fetch_config_from_metadata",
        "slug" : "fetch_config_from_metadata",
        "x" : 420,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "Nbt8gOmE6jSSW7MqNOMcF$$LfdD-GNyE8RjtedVBDT7b",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "wtOaygbQx_I-1K3OJz20d$$JO7l1k_pO3TPQeOQa-gRF",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ {
              "name" : "_1",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "_2",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            } ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : true
      },
      "properties" : {
        "script" : "import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\nprintln(\"###############################################\")\r\nprintln(\"#####Step name: fetch_config_from_metadata#####\")\r\nprintln(\"###############################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    ) \r\n\r\nif (Config.pk_sk_col_from_metadata_table) {\r\n  val metadata_db_name =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_dbname_key\r\n    )\r\n\r\n    val metadata_url =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_url_key\r\n    )\r\n\r\n    val metadata_user =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_user_key\r\n    )\r\n\r\n    val metadata_password =\r\n    dbutils.secrets.get(\r\n      scope = Config.metadata_scope,\r\n      key = Config.metadata_password_key\r\n    )\r\n\r\n  val fs_access_key =\r\n    dbutils.secrets.get(\r\n      scope = Config.synapse_scope,\r\n      key = Config.synapse_fs_access_key\r\n    )\r\n\r\n  val synapse_fs_access_url =\r\n    dbutils.secrets.get(\r\n      scope = Config.synapse_scope,\r\n      key = Config.synapse_fs_access_url_key\r\n    )\r\n\r\n  val synapse_temp_dir =\r\n    dbutils.secrets.get(\r\n      scope = Config.synapse_scope,\r\n      key = Config.synapse_temp_dir_key\r\n    )\r\n\r\n  spark.conf.set(\r\n    synapse_fs_access_url,\r\n    fs_access_key\r\n  )\r\n  val metadata_query =\r\n    s\"select A.TableName, NaturalKeys, SurrKey  from ${Config.sql_server_lookup_table_detail} A inner join ${Config.sql_server_sk_metadata_table} B on A.TableDetail_Id = B.TableDetail_Id where LOWER(tableName)=LOWER('${Config.target_table}')\"\r\n\r\n  println(\"metadata_query: \" + metadata_query)\r\n\r\n  val sk_metadata_df = spark.read\r\n    .format(\"jdbc\")\r\n    .option(\r\n      \"url\",\r\n      \"jdbc:sqlserver://\" + metadata_url + \";database=\" + metadata_db_name\r\n    )\r\n    .option(\"user\", metadata_user)\r\n    .option(\"password\", metadata_password)\r\n    .option(\r\n      \"query\",\r\n      metadata_query\r\n    )\r\n    .option(\"useAzureMSI\", \"true\")\r\n    .option(\"tempDir\", synapse_temp_dir)\r\n    .load()\r\n  val prim_key_columns = sk_metadata_df\r\n    .selectExpr(\"nvl(NaturalKeys, '-')\")\r\n    .first\r\n    .getAs[String](0)\r\n    .replace(\";\", \",\")\r\n    .toLowerCase\r\n    .trim\r\n  val sk_service_col = sk_metadata_df\r\n    .selectExpr(\"nvl(SurrKey, '-')\")\r\n    .first\r\n    .getAs[String](0)\r\n    .toLowerCase\r\n    .trim\r\n\r\n  if (prim_key_columns != \"-\") {\r\n    spark.conf.set(\"primary_key\", prim_key_columns)\r\n  }\r\n  if (sk_service_col != \"-\") {\r\n    spark.conf.set(\"sk_service_col\", sk_service_col)\r\n  }\r\n  println(\"prim_key_columns: \"+prim_key_columns)\r\n  println(\"prim_key_columns from json: \" + Config.primary_key)\r\n\r\n  println(\"sk_service_col: \"+sk_service_col)\r\n  println(\"sk_service_col from json: \" + Config.sk_service_col)\r\n}\r\n\r\nval out0 = spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "0XsmhQ5Tbs3kFP8nEhjFc$$Zpo0i3bzAR2ZaT2BxsSFu" : {
      "id" : "0XsmhQ5Tbs3kFP8nEhjFc$$Zpo0i3bzAR2ZaT2BxsSFu",
      "component" : "Script",
      "metadata" : {
        "label" : "apply_source_default",
        "slug" : "apply_source_default",
        "x" : 1820,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "LtreNnG93P2tD1LmCJtUg$$onBfpshEgIE1aPAaM_lYW",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "y0SaPNgkT3YVa_4sLO6B3$$ayOdWPH96KRhBMRU4gb34",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#########################################\")\nprintln(\"#####Step name: apply_source_default#####\")\nprintln(\"#########################################\")\n\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    ) \n//defaulting decimal types so that correct joins can happen\nval out0 =\n  if (\n    spark.conf.get(\n      \"new_data_flag\"\n    ) == \"true\"\n  ) {\n\n    var temp_df = in0\n\n    // default columns used in joins/lookups\n    import pureconfig._\n    import pureconfig.generic.auto._\n    import scala.language.implicitConversions\n\n    case class JoinDef(\n        fmt: String,\n        src: String,\n        joinCond: String,\n        valCols: List[String],\n        joinType: Option[String] = Some(\"left\"),\n        filterCond: Option[String] = Some(\"true\"),\n        joinHint: Option[String] = None\n    )\n\n    case class Joins(\n        joins: List[JoinDef]\n    )\n\n    var joinsConfig =\n      ConfigSource\n        .string(Config.reformat_rules_join.replace(\"\\n\", \" \"))\n        .loadOrThrow[Joins]\n\n    import scala.collection.mutable.ListBuffer\n    var outputList = new ListBuffer[String]()\n    joinsConfig.joins.foreach { joinDef =>\n      val cond = joinDef.joinCond\n      val regex = \"\"\"in0\\.(\\w+)\"\"\".r\n      val matches = regex.findAllIn(cond)\n      val colNames = matches.map(_.split('.')(1)).toList\n      outputList ++= colNames\n    }\n    val lookup_cols = outputList.toSet.toList\n    temp_df = temp_df.select(in0.dtypes.map { x =>\n      if (lookup_cols.contains(x._1)) {\n        if (x._2.toLowerCase().endsWith(\",0)\")) {\n          coalesce(col(x._1), lit(0)).cast(\"long\").as(x._1)\n        } else if (x._2.toLowerCase().startsWith(\"long\")) {\n          coalesce(col(x._1), lit(0)).cast(\"long\").as(x._1)\n        } else if (x._2.toLowerCase().startsWith(\"int\")) {\n          coalesce(col(x._1), lit(0)).cast(\"int\").as(x._1)\n        } else if (\n          x._2\n            .toLowerCase()\n            .startsWith(\"decimal\") || x._2.toLowerCase().startsWith(\"double\")\n        )\n          coalesce(col(x._1), lit(0)).cast(\"double\").as(x._1)\n        else if (x._2.toLowerCase().startsWith(\"date\")) {\n          coalesce(col(x._1), lit(\"1900-01-01\")).cast(\"date\").as(x._1)\n        } else if (x._2.toLowerCase().startsWith(\"timestamp\")) {\n          coalesce(col(x._1), lit(\"1900-01-01 00:00:00\"))\n            .cast(\"timestamp\")\n            .as(x._1)\n        } else\n          when(\n            col(x._1).isNull || (col(x._1).cast(\"string\") === lit(\"\")),\n            lit(\"-\")\n          )\n            .otherwise(trim(col(x._1)))\n            .as(x._1)\n      } else col(x._1)\n    }: _*)\n\n    // defaulting decimal types so that correct joins can happen\n    if (Config.default_decimal_types != \"false\") {\n      temp_df = temp_df.select(in0.dtypes.map { x =>\n        if (x._2.toLowerCase().endsWith(\",0)\")) {\n          col(x._1).cast(\"long\").as(x._1)\n        } else if (\n          x._2\n            .toLowerCase()\n            .startsWith(\"decimal\") || x._2.toLowerCase().startsWith(\"double\")\n        )\n          coalesce(\n            expr(\n              \"format_number(cast(`\" + x._1 + \"` as decimal(38,15)), '0.###############')\"\n            )\n          ).as(x._1)\n        else if (\n          x._2\n            .toLowerCase()\n            .startsWith(\"timestamp\") || x._2.toLowerCase().contains(\"time\")\n        ) {\n          date_format(col(x._1), \"yyyy-MM-dd HH:mm:ss.SSS\").as(x._1)\n        } else if (\n          x._2\n            .toLowerCase()\n            .startsWith(\"date\")\n        ) {\n          col(x._1).cast(\"string\").as(x._1)\n        } else col(x._1)\n      }: _*)\n    }\n    temp_df\n  } else {\n    in0\n  }\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "y-m6OkI8CIOyv0kGlSaK9$$Ss9JYcUtFkU6YmYdrBy9z" : {
      "id" : "y-m6OkI8CIOyv0kGlSaK9$$Ss9JYcUtFkU6YmYdrBy9z",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_rollup",
        "slug" : "optional_rollup",
        "x" : 4420,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "qlVSwiDXeHFWhSokqkn3a$$QnWNsRGJyWfBLUdzsIzOI",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "R7SZMrxnlUmfuv5CfTykl$$BnxjBoiTjiwZ8t0vWDvmO",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"####################################\")\r\nprintln(\"#####Step name: optional_rollup#####\")\r\nprintln(\"####################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n\r\nval out0 = if (Config.rollup_agg_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\") {\r\n  import org.json4s._\r\n  import org.json4s.jackson.JsonMethods._\r\n  import scala.collection.mutable.ListBuffer\r\n\r\n  def jsonStrToMap(jsonStr: String): Map[String, String] = {\r\n      implicit val formats: DefaultFormats.type = org.json4s.DefaultFormats\r\n      parse(jsonStr).extract[Map[String, String]]\r\n    }\r\n\r\n  val groupby_values = jsonStrToMap(Config.rollup_groupby_rules)\r\n  val agg_values = jsonStrToMap(Config.rollup_agg_rules)\r\n\r\n  var groupby_cols = new ListBuffer[org.apache.spark.sql.Column]()\r\n  var agg_cols =  new ListBuffer[org.apache.spark.sql.Column]()\r\n\r\n  groupby_values.foreach {case(key, value) => groupby_cols += expr(value).as(key)}\r\n\r\n  agg_values.foreach {case(key, value) => agg_cols += expr(value).as(key)}\r\n\r\n  in0.groupBy(groupby_cols:_*).agg(agg_cols.head, agg_cols.tail:_*)\r\n} else {\r\n  in0\r\n}\r\n\r\nif(Config.debug_flag) {\r\n  var printDf = out0\r\n  if(Config.debug_filter.toLowerCase() != \"none\"){\r\n    printDf = printDf.where(Config.debug_filter)\r\n  }  \r\n  if(Config.debug_col_list.toLowerCase() != \"none\"){\r\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\r\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\r\n  } else {\r\n    printDf.show(truncate=true)\r\n  }\r\n}\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "sLG-1tclvkxeUSOB_ZyrA$$5e57JAQoONa1dWQ6TtbhJ" : {
      "id" : "sLG-1tclvkxeUSOB_ZyrA$$5e57JAQoONa1dWQ6TtbhJ",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_default_rules",
        "slug" : "optional_default_rules",
        "x" : 2820,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "iNJgROMqIBiZb1KSqNYDf$$zLubz5RFwq7iQmui9pWkk",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "YY_5_Kq6tWxVKYlpDqE_e$$L4JjlXOd8DBkfCG6hAk7J",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import org.json4s._\r\nimport org.json4s.jackson.JsonMethods._\r\nimport scala.collection.mutable.ListBuffer\r\nimport spark.sqlContext.implicits._\r\n\r\nprintln(\"###########################################\")\r\nprintln(\"#####Step name: optional_default_rules#####\")\r\nprintln(\"###########################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n    \r\nval out0 =\r\n  if (\r\n    Config.default_source_blank_nulls != \"None\" && spark.conf.get(\r\n      \"new_data_flag\"\r\n    ) == \"true\"\r\n  ) {\r\n\r\n    val default_columns = Config.default_source_blank_nulls\r\n      .split(\",\")\r\n      .map(x => x.trim())\r\n\r\n    var outputList = new ListBuffer[org.apache.spark.sql.Column]()\r\n\r\n    for (col_name <- in0.columns) {\r\n      if (default_columns.contains(col_name)) {\r\n        val expression =\r\n          \"case when \" + col_name + \" is not null and trim(\" + col_name + \") != '' then trim(\" + col_name + \") else '-' end\"\r\n        outputList += expr(expression).as(col_name)\r\n      } else {\r\n        outputList += col(col_name)\r\n      }\r\n    }\r\n\r\n    in0.select(outputList: _*)\r\n  } else if (\r\n    Config.default_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\r\n  ) {\r\n    def jsonStrToMap(jsonStr: String): Map[String, String] = {\r\n      implicit val formats: DefaultFormats.type = org.json4s.DefaultFormats\r\n      parse(jsonStr).extract[Map[String, String]]\r\n    }\r\n\r\n    val col_map = jsonStrToMap(Config.default_rules.replace(\"\\n\", \" \"))\r\n\r\n    val col_values = col_map.keySet.toList\r\n\r\n    var outputList = new ListBuffer[org.apache.spark.sql.Column]()\r\n\r\n    for (col_name <- in0.columns) {\r\n      if (col_values.contains(col_name)) {\r\n        outputList += expr(col_map.get(col_name).get.replace(\"SSSZ\", \"SSS\"))\r\n          .as(col_name)\r\n      } else {\r\n        outputList += expr(col_name).as(col_name)\r\n      }\r\n    }\r\n\r\n    in0.select(outputList: _*)\r\n  } else {\r\n    in0 // if no default rules is provided\r\n  }\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "J6fjz0hQY29uv10LLQ6hU$$CK_Nl_BZsSbAUx2nAW4Ia" : {
      "id" : "J6fjz0hQY29uv10LLQ6hU$$CK_Nl_BZsSbAUx2nAW4Ia",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_filter_and_audit_fields",
        "slug" : "optional_filter_and_audit_fields",
        "x" : 1220,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "F9LCxXb0Zu1m06NR2LRk0$$NaoHMtfnd1cOpFHdRWhu6",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "7MSjWnqrvS41z-ANpwKXN$$ZSIzXduIWL9VMya6IsZpB",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#####################################################\")\nprintln(\"#####Step name: optional_filter_and_audit_fields#####\")\nprintln(\"#####################################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval out0 = if (spark.conf.get(\"new_data_flag\") == \"true\") {\n\n  val audit_values =\n    if (Config.incremental_watermark_max_ts_expr != \"None\") {\n      in0\n        .selectExpr(\n          \"coalesce(min(file_name_timestamp), '-') as min_ts\",\n          s\"coalesce((${Config.incremental_watermark_max_ts_expr}), '-') as max_ts\",\n          \"count(*) as row_count\",\n          \"count(distinct file_name_timestamp) as processed_file_count\"\n        )\n        .collect()(0)\n    } else {\n      in0\n        .selectExpr(\n          \"coalesce(min(file_name_timestamp), '-') as min_ts\",\n          \"coalesce(max(file_name_timestamp), '-') as max_ts\",\n          \"count(*) as row_count\",\n          \"count(distinct file_name_timestamp) as processed_file_count\"\n        )\n        .collect()(0)\n    }\n\n  println(\"source record count: \" + audit_values(2).toString)\n  println(s\"Next Watermark Value will be : ${audit_values(1).toString} \")\n\n  spark.conf.set(\"source_count\", audit_values(2).toString)\n  spark.conf.set(\"min_processed_file_timestamp\", \"-\")\n  spark.conf.set(\"max_processed_file_timestamp\", \"-\")\n  spark.conf.set(\"processed_file_count\", \"0\")\n\n  if (Config.read_incremental_files_flag == \"true\") {\n\n    spark.conf.set(\"min_processed_file_timestamp\", audit_values(0).toString)\n    spark.conf.set(\"max_processed_file_timestamp\", audit_values(1).toString)\n    spark.conf.set(\"processed_file_count\", audit_values(3).toString)\n  }\n  val res = if (Config.source_filter.toLowerCase().contains(\"partition by\")) {\n    val where_cond = Config.source_filter\n    val window_arr = where_cond.split(\"==\")\n    in0\n      .withColumn(\"window_temp_col\", expr(window_arr(0).trim()))\n      .where(\"window_temp_col == \" + window_arr(1).trim())\n      .drop(\"window_temp_col\")\n  } else if (Config.source_filter != \"None\") {\n    in0.where(Config.source_filter)\n  } else {\n    in0.drop(\"ids_create_run_id\")\n  }\n\n  if (Config.source_filter != \"None\") {\n    spark.conf.set(\n      \"source_filter_count\",\n      (res.count()).toString\n    )\n  } else {\n    spark.conf.set(\"source_filter_count\", audit_values(2).toString)\n  }\n  res\n} else {\n  spark.conf.set(\"min_processed_file_timestamp\", \"-\")\n  spark.conf.set(\"max_processed_file_timestamp\", \"-\")\n  spark.conf.set(\"source_count\", \"0\")\n  spark.conf.set(\"processed_file_count\", \"0\")\n  spark.conf.set(\"source_filter_count\", \"0\")\n\n  in0\n\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "MLZ2fgQ3msTF0GOvhUqD-$$86vbeJZRRg8AtLTmckeHR" : {
      "id" : "MLZ2fgQ3msTF0GOvhUqD-$$86vbeJZRRg8AtLTmckeHR",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_repartition",
        "slug" : "optional_repartition",
        "x" : 1420,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "hM6P1_SabsHNK3ilBw48X$$SV-7ht0iCEht6GCJAf7SP",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "7FRJJQKjJ2ecBLF_Rrqmd$$uTZDNjbmEbiRgoVJwiljO",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#########################################\")\r\nprintln(\"#####Step name: optional_repartition#####\")\r\nprintln(\"#########################################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\nval out0 =\r\n  if (\r\n    Config.repartition_flag == \"true\" && spark.conf.get(\r\n      \"new_data_flag\"\r\n    ) == \"true\"\r\n  ) {\r\n    in0.repartitionByRange(\r\n      Config.repartition_cols.split(\",\").map(x => col(x.trim())): _*\r\n    )\r\n  } else if (\r\n    Config.enable_round_robin_partioning && Config.spark_configs != \"None\" && spark.conf\r\n      .get(\"new_data_flag\") == \"true\"\r\n  ) {\r\n    val numPartitions = Config.spark_configs\r\n      .split(\",\")\r\n      .filter(_.contains(\"shuffle\"))\r\n      .array(0)\r\n      .split(\"=\")\r\n      .array(1).toInt\r\n    println(\r\n      \"round_robin_partioning is enabled, number of partitions: \" + numPartitions\r\n    )\r\n    in0.repartition(numPartitions)\r\n  } else {\r\n    in0\r\n  }\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "iLCBWNwlU4rZ-zrD39NlO$$pYmIZTlRyNwK02U3cBgEU" : {
      "id" : "iLCBWNwlU4rZ-zrD39NlO$$pYmIZTlRyNwK02U3cBgEU",
      "component" : "Script",
      "metadata" : {
        "label" : "add_audit_cols",
        "slug" : "add_audit_cols",
        "x" : 5020,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "J6pukRNozYFXrh6hu11L7$$zaaOECgzJt-rray0ckm47",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "axHoGnKhBJMWoNsL09Tt6$$w9jbV3OuwMXovL9OT62yH",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n// this method adds audit column to the existing dataframe\nprintln(\"###################################\")\nprintln(\"#####Step name: add_audit_cols#####\")\nprintln(\"###################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval run_id = spark.conf.get(\"run_id\")\n\nvar run_id_for_data = run_id\nif (Config.custom_run_id_suffix != \"None\") {\n  run_id_for_data = run_id.substring(0, 8) + Config.custom_run_id_suffix\n}\n\nprintln(\"run id for data: \" + run_id_for_data)\n\nval current_ts_val = to_timestamp(\n  from_utc_timestamp(current_timestamp(), \"America/Chicago\").cast(\"string\"),\n  \"yyyy-MM-dd HH:mm:ss\"\n)\nval uid_col = Config.get_insert_uid_from_source_df match {\n      case true if in0.columns.contains(\"insert_uid\") => col(\"insert_uid\")\n      case _ => Config.uid.toLowerCase match {\n        case \"none\"  => {\n          import java.nio.charset.StandardCharsets\n          // Hex string\n          val hexString = \"E281A3\"\n          // Convert hex string to byte array\n          val byteArray = hexString.sliding(2, 2).toArray.map(Integer.parseInt(_, 16).toByte)\n          // Decode byte array to UTF-8 string\n          val invisibleSep = new String(byteArray, StandardCharsets.UTF_8)\n          val secret = dbutils.secrets.get(scope = Config.api_scope, key = Config.update_uid_key)\n          val plaintextSecret = secret.replace(\"\", invisibleSep)\n          lit(plaintextSecret)\n        }\n        case _ => lit(Config.uid)\n      }\n    }\n\nval out0 =\n  if (\n    Config.disable_audit == \"false\" && spark.conf.get(\"new_data_flag\") == \"true\"\n  ) {\n    try {\n      val insert_uid_flg = in0.columns.contains(\"insert_uid\") && Config.get_insert_uid_from_source_df\n      var temp_df = insert_uid_flg match {\n            case true => {\n              val df = in0\n                .select(\n                  col(\"*\"),\n                  current_ts_val.as(\"insert_ts\"),\n                  current_ts_val.as(\"update_ts\"),\n                  lit(null).cast(\"string\").as(\"update_uid\"),\n                  lit(run_id_for_data).cast(\"long\").as(\"run_id\"),\n                  lit(Config.rec_stat_cd).cast(\"short\").as(\"rec_stat_cd\")\n                )\n                .drop(\"reject_record\")\n              df.withColumn(\"insert_uid\", substring(uid_col, 0, 20))\n            }\n            case false => in0\n              .select(\n                col(\"*\"),\n                current_ts_val.as(\"insert_ts\"),\n                current_ts_val.as(\"update_ts\"),\n                substring(uid_col, 0, 20).as(\"insert_uid\"),\n                lit(null).cast(\"string\").as(\"update_uid\"),\n                lit(run_id_for_data).cast(\"long\").as(\"run_id\"),\n                lit(Config.rec_stat_cd).cast(\"short\").as(\"rec_stat_cd\")\n              )\n              .drop(\"reject_record\")\n          }\n      if (in0.columns.contains(\"src_env_sk\")) {\n        temp_df = temp_df.withColumn(\"src_env_sk\", col(\"src_env_sk\"))\n      } else if (in0.columns.contains(\"dxf_src_sys_id\")) {\n        temp_df = temp_df.withColumn(\"src_env_sk\", col(\"dxf_src_sys_id\"))\n      } else {\n        temp_df = temp_df.withColumn(\"src_env_sk\", lit(Config.src_env_sk))\n      }\n\n      if (Config.run_id_from_filename == \"true\") {\n        temp_df =\n          temp_df.withColumn(\"run_id\", col(\"file_name_timestamp\").cast(\"long\"))\n      }\n\n      if (Config.add_audit_sec_flag == \"true\") {\n        temp_df = temp_df.withColumn(\"sec_flg\", lit(0).cast(\"short\"))\n      }\n      temp_df\n    } catch {\n      case e: Exception => {\n        println(\n          s\"\"\"Process failed while generating audit columns. Removing lock from: ${Config.target_table}\"\"\"\n        )\n\n        if (spark.conf.get(\"main_table_api_type\") == \"NON-API\"){\n          import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n          println(s\" Removing lock from: ${Config.sk_table_name_override}\")\n          dbutils.fs.rm(Config.sk_lock_file_path + Config.sk_table_name_override + \".txt\")\n        }\n        throw e\n      }\n    }\n  } else {\n    in0\n  }\n\nif(Config.debug_flag) {\n  var printDf = out0\n  if(Config.debug_filter.toLowerCase() != \"none\"){\n    printDf = printDf.where(Config.debug_filter)\n  }  \n  if(Config.debug_col_list.toLowerCase() != \"none\"){\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\n  } else {\n    printDf.show(truncate=true)\n  }\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "-HWhIbTJNxycZcwI9Sq3w$$SJAnEcpJS0Q5NicBUqB7i" : {
      "id" : "-HWhIbTJNxycZcwI9Sq3w$$SJAnEcpJS0Q5NicBUqB7i",
      "component" : "Script",
      "metadata" : {
        "label" : "dedup_filter",
        "slug" : "dedup_filter",
        "x" : 3820,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "l6hjs83kiCN1VK2MHw6b4$$Xev6hnuVBNicgBBQF2hRe",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "uQN5Yif5jsAbvJLCqTiDd$$bgDUlmyNmNfxFtcBzgrhY",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#################################\")\r\nprintln(\"#####Step name: dedup_filter#####\")\r\nprintln(\"#################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n\r\nval out0 =\r\n  if (\r\n    Config.dedup_filter != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\r\n  ) {\r\n    in0.where(Config.dedup_filter)\r\n  } else {\r\n    in0\r\n  }\r\n  \r\nif(Config.debug_flag) {\r\n  var printDf = out0\r\n  if(Config.debug_filter.toLowerCase() != \"none\"){\r\n    printDf = printDf.where(Config.debug_filter)\r\n  }  \r\n  if(Config.debug_col_list.toLowerCase() != \"none\"){\r\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\r\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\r\n  } else {\r\n    printDf.show(truncate=true)\r\n  }\r\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "Hgf5DAS1R-FD_dKZeOdmd$$CrPq1G1VlHpSx5GydDGbO" : {
      "id" : "Hgf5DAS1R-FD_dKZeOdmd$$CrPq1G1VlHpSx5GydDGbO",
      "component" : "Script",
      "metadata" : {
        "label" : "create_single_file",
        "slug" : "create_single_file",
        "x" : 6820,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "alpnzSF7dUPbXG7Xst0jB$$LyysmNJFH-FWMfI750r9U",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        }, {
          "id" : "Qd9HB2dEg82pCZuOga3rP$$o3slPbUooq96dzyaxltC7",
          "slug" : "in1",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "oeiXY8In7xKIhLXAOa7SR$$0MAFaMNZvGZ_HeuaGL9gk",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{FSDataInputStream, FileSystem, Path}\r\nimport org.apache.hadoop.io.IOUtils\r\nimport scala.io.Source\r\nimport scala.sys.process.Process\r\nimport scala.util.Try\r\nimport java.io._\r\nimport java.text.SimpleDateFormat\r\n//read parquets in  folder into single dataframe and write back as single parquet\r\nprintln(\"#######################################\")\r\nprintln(\"#####Step name: create_single_file#####\")\r\nprintln(\"#######################################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\nif (Config.skip_main_table_load_ready_files == \"false\") {\r\n  if (\r\n    Config.generate_load_ready_files == \"true\" || (Config.generate_only_load_ready_files == \"true\")\r\n  ) {\r\n    try {\r\n      println(\"Renaming and moving insert and update load ready file\")\r\n      val load_ready_insert_path =\r\n        Config.load_ready_insert_path\r\n      val baseFilePath =\r\n        Config.single_load_ready_path\r\n      val tableName = Config.target_table\r\n      val run_id = spark.conf.get(\"run_id\")\r\n\r\n      val hadoopConfig = new Configuration()\r\n      val hdfs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\r\n\r\n      val finalInsertPath = new Path(\r\n        baseFilePath + \"/\" + tableName + \"/\" + run_id.substring(\r\n          0,\r\n          4\r\n        ) + \"/\" + tableName + \".insert.000.\" + run_id + \".parquet\"\r\n      )\r\n      val finalUpdatePath = new Path(\r\n        baseFilePath + \"/\" + tableName + \"/\" + run_id.substring(\r\n          0,\r\n          4\r\n        ) + \"/\" + tableName + \".update.000.\" + run_id + \".parquet\"\r\n      )\r\n\r\n      val tempInsertPath =\r\n        new Path(\r\n          Config.load_ready_insert_path + \"/\" + Config.target_table + \"/insert_files/\" + spark.conf\r\n            .get(\"run_id\") + \"/\"\r\n        )\r\n      val tempUpdatePath =\r\n        new Path(\r\n          Config.load_ready_update_path + \"/\" + Config.target_table + \"/update_files/\" + spark.conf\r\n            .get(\"run_id\") + \"/\"\r\n        )\r\n\r\n      copyMerge(hdfs, tempInsertPath, hdfs, finalInsertPath, true, hadoopConfig)\r\n      copyMerge(hdfs, tempUpdatePath, hdfs, finalUpdatePath, true, hadoopConfig)\r\n\r\n      def copyMerge(\r\n          srcFS: FileSystem,\r\n          srcDir: Path,\r\n          dstFS: FileSystem,\r\n          dstFile: Path,\r\n          deleteSource: Boolean,\r\n          conf: Configuration\r\n      ): Boolean = {\r\n        if (dstFS.exists(dstFile))\r\n          throw new IOException(s\"Target $dstFile already exists\")\r\n\r\n        // Source path is expected to be a directory:\r\n        if (srcFS.getFileStatus(srcDir).isDirectory()) {\r\n\r\n          val outputFile = dstFS.create(dstFile)\r\n          Try {\r\n            srcFS\r\n              .listStatus(srcDir)\r\n              .sortBy(_.getPath.getName)\r\n              .filter(_.getPath.getName.endsWith(\".parquet\"))\r\n              .collect {\r\n                case status if status.isFile() =>\r\n                  val inputFile = srcFS.open(status.getPath())\r\n                  Try(IOUtils.copyBytes(inputFile, outputFile, conf, false))\r\n                  inputFile.close()\r\n              }\r\n          }\r\n          outputFile.close()\r\n\r\n          if (deleteSource) srcFS.delete(srcDir, true) else true\r\n        } else false\r\n      }\r\n    } catch {\r\n      case e: Exception => {\r\n        if (spark.conf.get(\"main_table_api_type\") == \"NON-API\") {\r\n          val update_sql = spark.conf.get(\"main_table_max_sk_update_sql\")\r\n          println(\r\n            s\"\"\"Updating max sk in metadata table failed. Please update max sk manually and remove lock to proceed.\r\n            Update metadata table run this query in metadata db: ${update_sql}\r\n            To remove lock for ${Config.target_table}. Run below command in databricks: \r\n            dbutils.fs.rm(${Config.sk_lock_file_path}${Config.sk_table_name_override}.txt\")\r\n            \"\"\"\r\n          )\r\n        }\r\n        // dbutils.fs.rm(Config.sk_lock_file_path + Config.target_table + \".txt\")\r\n        throw e\r\n      }\r\n    }\r\n  }\r\n}\r\nval out0 = in0\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame, in1: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "w1ePGHe-uHbRy_LYBrwd6$$5s455UdOQK7Vg-Nu7ikG5" : {
      "id" : "w1ePGHe-uHbRy_LYBrwd6$$5s455UdOQK7Vg-Nu7ikG5",
      "component" : "Script",
      "metadata" : {
        "label" : "reject_records",
        "slug" : "reject_records",
        "x" : 4420,
        "y" : 320,
        "phase" : -1,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "qZgrKbEc5IiKlvzeOqQXn$$dQrxno8sqqyko9eLe13FX",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "// this method write rejected records to the path provided by config as well as write the reject record summary\n\nprintln(\"###################################\")\nprintln(\"#####Step name: reject_records#####\")\nprintln(\"###################################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    )\n\nif (\n  Config.disable_reject_record_summary == \"false\" && spark.conf.get(\n    \"new_data_flag\"\n  ) == \"true\"\n  && spark.conf.get(\"reject_record_count\") != \"0\"\n) {\n\n  val run_id = spark.conf.get(\"run_id\")\n  // write reject records in delta table\n  in0\n    .withColumn(\n      \"run_id\",\n      lit(run_id)\n    )\n    .write\n    .format(\"delta\")\n    //.option(\"optimizeWrite\", true)\n    .option(\"delta.enableChangeDataFeed\", true)\n    .partitionBy(\"run_id\")\n    .mode(\"append\")\n    .saveAsTable(Config.error_table_prefix + Config.target_table)\n\n  // write reject record summary in delta table\n  in0\n    .withColumn(\n      \"run_id\",\n      lit(run_id)\n    )\n    .groupBy(\n      lit(Config.pipeline_name).as(\"pipeline_name\"),\n      col(\"run_id\"),\n      col(\"reject_record\")\n    ) // group by reject record type\n    .agg(count(lit(1)).as(\"reject_count\"))\n    .withColumn(\n      \"run_id\",\n      lit(run_id)\n    )\n    .write\n    .format(\"delta\")\n    //.option(\"optimizeWrite\", true)\n    .option(\"delta.enableChangeDataFeed\", true)\n    .partitionBy(\"pipeline_name\", \"run_id\")\n    .mode(\"append\")\n    .saveAsTable(Config.error_summary_table)\n\n    val reject_record_count = spark.conf.get(\"reject_record_count\").toInt\n\n    if (reject_record_count > Config.reject_record_threshold && Config.length_rules_from_metadata_table != \"true\"){\n    throw new Exception(\n        \"Reject record count (\" + reject_record_count.toString + \") is greater than the defined reject record threshold (\"\n          + Config.reject_record_threshold.toString + \") for the pipeline\"\n      )\n  }\n\n} else{\n  val reject_record_count = spark.conf.get(\"reject_record_count\").toInt\n\n    if (reject_record_count > Config.reject_record_threshold && Config.length_rules_from_metadata_table != \"true\"){\n    throw new Exception(\n        \"Reject record count (\" + reject_record_count.toString + \") is greater than the defined reject record threshold (\"\n          + Config.reject_record_threshold.toString + \") for the pipeline\"\n      )\n  }\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): Unit = {",
        "scriptMethodFooter" : "    \n}"
      }
    },
    "5I9w6UESsmDlhfmUL07UY$$mWCyl0Zqbe_HbwHuPg6F2" : {
      "id" : "5I9w6UESsmDlhfmUL07UY$$mWCyl0Zqbe_HbwHuPg6F2",
      "component" : "Script",
      "metadata" : {
        "label" : "update_inc_metadata_table",
        "slug" : "update_inc_metadata_table",
        "x" : 7420,
        "y" : 20,
        "phase" : 1,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "u9KR5hzmm6pWnQWNQ0vw0$$mJSG7SX8fv4v1apaWi7Ly",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"##############################################\")\nprintln(\"#####Step name: update_inc_metadata_table#####\")\nprintln(\"##############################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval accumulated_process_flag = if (Config.accumulated_process_flag == \"true\") true else false\n\nif (\n  Config.read_incremental_files_flag == \"true\" && spark.conf.get(\n    \"new_data_flag\"\n  ) == \"true\"\n) {\n  if (!accumulated_process_flag) {\n    println(\"Updating incremental metadata table\")\n    import _root_.io.delta.tables._\n    import spark.implicits._\n    val metaColumns = Seq(\n      \"data_mart\",\n      \"pipeline_name\",\n      \"source\",\n      \"target\",\n      \"last_process_timestamp\"\n    )\n\n    val metaDataDf = spark\n      .createDataFrame(\n        Seq(\n          (\n            Config.data_mart,\n            Config.pipeline_name,\n            Config.source_table,\n            Config.target_table,\n            spark.conf.get(\"max_processed_file_timestamp\")\n          )\n        )\n      )\n      .toDF(metaColumns: _*)\n    // val metaDataDf = in0\n    //   .groupBy(lit(Config.pipeline_name), col(\"source_file_base_path\"))\n    //   .agg(\n    //     max(\"file_name_timestamp\")\n    //       .alias(\"last_process_timestamp\")\n    //   )\n    //   .toDF(metaColumns: _*)\n\n    if (!spark.catalog.tableExists(Config.incremental_load_metadata_table)) {\n      metaDataDf.write\n        .format(\"delta\")\n        .mode(\"overwrite\")\n        .partitionBy(\"pipeline_name\")\n        .saveAsTable(Config.incremental_load_metadata_table)\n    } else {\n      DeltaTable\n        .forName(Config.incremental_load_metadata_table)\n        .as(\"target\")\n        .merge(\n          metaDataDf.as(\"source\"),\n          (col(\"target.`pipeline_name`\") === lit(Config.pipeline_name)) &&\n            (col(\"source.`target`\") === col(\"target.`target`\"))\n        )\n        .whenMatched()\n        .updateAll()\n        .whenNotMatched()\n        .insertAll()\n        .execute()\n    }\n  } else {\n    if (accumulated_process_flag) {\n      val run_id = spark.conf.get(\"run_id\")\n      val current_ts_val = to_timestamp(\n        from_utc_timestamp(current_timestamp(), \"America/Chicago\")\n          .cast(\"string\"),\n        \"yyyy-MM-dd HH:mm:ss\"\n      )\n      val new_files =\n        spark.conf.get(\"accoumulated_process_files\").split(\",\").toList\n      import spark.implicits._\n      val write_df = new_files\n        .toDF(\"processed_file\")\n        .withColumn(\"data_mart\", lit(Config.data_mart))\n        .withColumn(\"pipeline_name\", lit(Config.pipeline_name))\n        .withColumn(\"target\", lit(Config.target_table))\n        .withColumn(\"run_id\", lit(run_id).cast(\"long\"))\n        .withColumn(\"insert_ts\", current_ts_val)\n        .select(\"data_mart\",\"pipeline_name\", \"target\", \"run_id\", \"processed_file\", \"insert_ts\")\n\n      write_df.write\n        .format(\"delta\")\n        .mode(\"append\")\n        .partitionBy(\"pipeline_name\")\n        .option(\n          \"optimizeWrite\",\n          true\n        )\n        .option(\n          \"delta.enableChangeDataFeed\",\n          true\n        )\n        .saveAsTable(Config.accumulated_processed_files_table)\n    }\n\n  }\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): Unit = {",
        "scriptMethodFooter" : "    \n}"
      }
    },
    "8TQl4BrnklSyziDM1pDq2$$PJh89RDre0QIipfFMy8HU" : {
      "id" : "8TQl4BrnklSyziDM1pDq2$$PJh89RDre0QIipfFMy8HU",
      "component" : "Script",
      "metadata" : {
        "label" : "ht2",
        "slug" : "ht2",
        "x" : 820,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "vk3mHrosPWyw6g8Y4uDKR$$2bNECW62AENwSllT1JBwL",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "5Jsn5eN4EgAVCoL1Er62w$$w4D3a1k7yp86S2DFj4jwJ",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ {
              "name" : "dxf_src_dataset_id",
              "type" : "long",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_rec_cnt",
              "type" : "integer",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_sys_id",
              "type" : "short",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "dxf_src_file_name",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "hold_id",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "name",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "type_code",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "description",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "start_date_active",
              "type" : "timestamp",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "end_date_active",
              "type" : "timestamp",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "order_holds_update_ts",
              "type" : "timestamp",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "newline",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_full_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "file_name_timestamp",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "source_file_base_path",
              "type" : "string",
              "nullable" : false,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            } ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : true
      },
      "properties" : {
        "script" : "import org.apache.spark.sql.{SparkSession}\r\nimport org.apache.spark.sql.types.StructType\r\nimport org.apache.spark.sql.functions._\r\n\r\nprintln(\"########################\")\r\nprintln(\"#####Step name: ht2#####\")\r\nprintln(\"########################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    ) \r\n\r\nvar out0 = in0\r\n\r\nif (Config.ht2_flag && (spark.conf.get(\"new_data_flag\") == \"true\" )) {\r\n  def flattenStructSchema(\r\n      schema: StructType,\r\n      prefix: String = null\r\n  ): Array[Column] = {\r\n    schema.fields.flatMap(f => {\r\n      val columnName = if (prefix == null) f.name else s\"$prefix.${f.name}\"\r\n\r\n      f.dataType match {\r\n        case st: StructType => flattenStructSchema(st, columnName)\r\n        case _ => Array(col(columnName).as(columnName.replace(\".\", \"-\")))\r\n      }\r\n    })\r\n  }\r\n\r\n  val flattenedSchemaDf = in0.select(flattenStructSchema(in0.schema): _*)\r\n\r\n  var exprStringBuilder = \"\"\r\n  val tempSet = scala.collection.mutable.Set[String]()\r\n\r\n  flattenedSchemaDf.columns.foreach { x =>\r\n    val childName = x.split('-').reverse.array(0)\r\n    // exprStringBuilder ++= s\"`$x`${if(tempSet.contains(childName)) \"\" else \" as \" + childName}, \"\r\n    if (tempSet.contains(childName) & (x.split('-').size > 1) ) {\r\n      exprStringBuilder =  exprStringBuilder + \"`\" + x + \"`\" + \", \"\r\n    } else if (tempSet.contains(childName)){\r\n      exprStringBuilder = exprStringBuilder + \"`\" + x + \"`\" + \" as `outer_struct-\" + x + \"`\" + \", \"\r\n    } else {\r\n      exprStringBuilder = exprStringBuilder + \"`\" + x + \"`\" + \" as \" + \"`\" + childName + \"`\" + \", \"\r\n    }\r\n\r\n    tempSet += childName\r\n  }\r\n\r\n  val finalColumnsList = exprStringBuilder.dropRight(2).toString.split(\",\")\r\n\r\n  val targetTableColumns =\r\n    spark.read.table(s\"${Config.target_table_db}.${Config.target_table.replace(\"ht2_\",\"\")}\").columns\r\n\r\n  val filteredColumnsList = \r\n    targetTableColumns.diff(Config.audit_cols.split(\",\").map(_.trim))\r\n\r\n  out0 = flattenedSchemaDf\r\n    .select(finalColumnsList.map(expr): _*)\r\n    .withColumn(\"file_name_timestamp\", coalesce(col(\"file_name_timestamp\"), lit(\"19000101000000\"))).withColumn(\"rec_stat_cd\", lit(\"1\"))\r\n}\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "O_ToGlviXaR1cWzi7yApg$$WOHgnFUDO0ao0YqcmToJd" : {
      "id" : "O_ToGlviXaR1cWzi7yApg$$WOHgnFUDO0ao0YqcmToJd",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_filter_rules",
        "slug" : "optional_filter_rules",
        "x" : 2220,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "E04uKZEjgrvasB6AULnNS$$U4TK3tkLAsbbGzfD37CFY",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "tnAiG_eIFfzBuCehhJVWb$$3mV4034A2tWaprUruZz_K",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"##########################################\")\r\nprintln(\"#####Step name: optional_filter_rules#####\")\r\nprintln(\"##########################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    ) \r\n\r\nval out0 =\r\n  if (\r\n    Config.optional_filter_rules != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\r\n  ) {\r\n    in0.where(Config.optional_filter_rules)\r\n  } else {\r\n    in0\r\n  }",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "vKNfiHSvwz-ASGB0rmEVI$$LZPS24kdDgRr1_JaOpRjw" : {
      "id" : "vKNfiHSvwz-ASGB0rmEVI$$LZPS24kdDgRr1_JaOpRjw",
      "component" : "Script",
      "metadata" : {
        "label" : "decryption",
        "slug" : "decryption",
        "x" : 6220,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "w5AdwZt6673vq5yWaH6AN$$UeK7vbFBKiyE0aUgkIJgN",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "aAdBpJZJITnhqLUVsr3Yy$$eD_sDGuwD33lYZE354ZkV",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import java.security.MessageDigest\r\nimport java.util\r\nimport javax.crypto.Cipher\r\nimport javax.crypto.spec.{IvParameterSpec, SecretKeySpec}\r\nimport org.apache.commons.codec.binary.Hex\r\nimport org.apache.spark.sql.functions._\r\n\r\nprintln(\"###############################\")\r\nprintln(\"#####Step name: decryption#####\")\r\nprintln(\"###############################\")\r\nprintln(\r\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n)\r\n\r\nvar out0 = in0\r\nif (\r\n  (Config.decrypt_cols != \"None\" && spark.conf.get(\r\n    \"new_data_flag\"\r\n  ) == \"true\" && Config.skip_main_table_load_ready_files == \"false\") || (Config.decrypt_cols != \"None\" && Config.generate_only_load_ready_files == \"true\" && Config.skip_main_table_load_ready_files == \"false\")\r\n) {\r\n\r\n//Decryption method for obfuscating PHI / PII fields defined in config encryptColumns\r\n  import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n  println(\"Decrypting columns\")\r\n  // loading db secrets\r\n  val decrypt_key =\r\n    dbutils.secrets.get(\r\n      scope = Config.decrypt_scope,\r\n      key = Config.decrypt_EncKey\r\n    )\r\n\r\n  val decrypt_iv =\r\n    dbutils.secrets.get(\r\n      scope = Config.decrypt_scope,\r\n      key = Config.decrypt_InitVec\r\n    )\r\n\r\n  val pii_cols = Config.decrypt_cols.split(\",\").map(x => x.trim().toLowerCase())\r\n  val non_pii_cols =\r\n    (in0.columns.map(x => x.toLowerCase()).toList diff pii_cols).toList\r\n\r\n  def decrypt(key: String, ivString: String, encryptedValue: String): String =\r\n    {\r\n      if (encryptedValue != null) {\r\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\r\n        cipher.init(Cipher.DECRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\r\n        new String(cipher.doFinal(Hex.decodeHex(encryptedValue.toCharArray())))\r\n      } else {\r\n        null\r\n      }\r\n    }\r\n\r\n    def keyToSpec(key: String): SecretKeySpec = {\r\n      var keyBytes: Array[Byte] = (key).getBytes(\"UTF-8\")\r\n      // val sha: MessageDigest = MessageDigest.getInstance(\"MD5\")\r\n      keyBytes = Hex.decodeHex(key)\r\n      keyBytes = util.Arrays.copyOf(keyBytes, 16)\r\n      new SecretKeySpec(keyBytes, \"AES\")\r\n    }\r\n\r\n    def getIVSpec(IVString: String) = {\r\n      new IvParameterSpec(\r\n        IVString.getBytes() ++ Array.fill[Byte](16 - IVString.length)(\r\n          0x00.toByte\r\n        )\r\n      )\r\n    }\r\n\r\n    val decryptUDF = udf(decrypt _)\r\n\r\n    spark.udf.register(\"decrypt\", decryptUDF)\r\n\r\n    val str = if (Config.encryption_to_uppercase) {\r\n      pii_cols\r\n        .map(x =>\r\n          s\"decrypt('$decrypt_key','$decrypt_iv',\" + x.toUpperCase + \") as \" + x\r\n        )\r\n        .mkString(\",\")\r\n    } else {\r\n      pii_cols\r\n        .map(x => s\"decrypt('$decrypt_key','$decrypt_iv',\" + x + \") as \" + x)\r\n        .mkString(\",\")\r\n    }\r\n\r\n    val all_cols = str + \",\" + non_pii_cols.mkString(\",\")\r\n    in0.createOrReplaceTempView(\r\n      s\"temp_tbl_${Config.pipeline_name.replace(\"-\", \"\")}\"\r\n    )\r\n\r\n    out0 = spark\r\n      .sql(\r\n        s\"select $all_cols from temp_tbl_${Config.pipeline_name.replace(\"-\", \"\")}\"\r\n      )\r\n      .select(in0.columns.map(x => col(x)): _*)\r\n\r\n} else {\r\n  out0 = in0\r\n}\r\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "qCyo_ZpxGMT3wM4FUCkB8$$6S2ukC40_BY5laPMcQtRv" : {
      "id" : "qCyo_ZpxGMT3wM4FUCkB8$$6S2ukC40_BY5laPMcQtRv",
      "component" : "Script",
      "metadata" : {
        "label" : "read_source",
        "slug" : "read_source",
        "x" : 620,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "Yz_3iARfXg-AZTNaFbI-l$$-sEP0R9kFpBPw7N4BbHFk",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ {
              "name" : "_1",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "_2",
              "type" : "string",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            } ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "wdW6ALIS3lvrkMvsH4ILh$$vMH750-tJ2F1tJXizUWIC",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "// case class to define source\nimport pureconfig._\nimport pureconfig.generic.auto._\nimport scala.language.implicitConversions\n\nimport java.time._\nimport java.time.format._\n\nprintln(\"################################\")\nprintln(\"#####Step name: read_source#####\")\nprintln(\"################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval curr_time = Instant.now().atZone(ZoneId.of(s\"${Config.source_zoneId}\"))\nval run_id =\n  Instant\n    .now()\n    .atZone(ZoneId.of(\"America/Chicago\"))\n    .format(DateTimeFormatter.ofPattern(\"yyyyMMddHHmmss\"))\n    .toString\nprintln(\"run_id: \" + run_id)\nspark.conf.set(\"run_id\", run_id)\n\nval load_filter_time =\n  curr_time.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")).toString\nval load_filter_date = load_filter_time.substring(0, 10)\n\nprintln(\"read source started\")\n\ncase class SrcDef(\n    fmt: String,\n    src: String,\n    delimiter: Option[String] = Some(\",\")\n)\n\n// Workaround until pureconfig 0.17.1 hits prod\ncase class Sources(\n    srcs: List[SrcDef]\n)\n\n// SingleOrList is a special class that accepts either a single SrcDef\n// or a list of SrcDef, and returns a List[SrcDef] either way.\nval sourceConfig =\n  ConfigSource\n    .string(Config.source_path.replace(\"\\n\", \" \"))\n    .loadOrThrow[Sources]\n\nval accumulated_process_flag =\n  if (Config.accumulated_process_flag == \"true\") true else false\n\n// creating source dataframe based on source format and path\nval allDFs = sourceConfig.srcs\n  .map(srcDef => {\n    val df = if (Config.read_incremental_files_flag == \"true\") {\n\n      import _root_.io.delta.tables._\n      import org.apache.spark.sql.functions._\n      import spark.implicits._\n\n      import scala.util.Try\n      println(\"read_incremental_files_flag is true\")\n      def allFiles(path: String): List[String] = {\n        import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n        try {\n          dbutils.fs\n            .ls(path)\n            .map(file => {\n              // Work around double encoding bug\n              val path = file.path.replace(\"%25\", \"%\").replace(\"%25\", \"%\")\n              if (file.isDir) allFiles(path)\n              else List(path)\n            })\n            .reduce(_ ++ _)\n        } catch {\n          case e: Exception => {\n            val empty_list: List[String] = Nil\n            empty_list\n          }\n        }\n      }\n\n      val cond_arr = Config.incremental_condition_strategy\n        .split(\",\")\n        .map(x => x.trim().toLowerCase())\n      val upper_cond = cond_arr(1)\n      val lower_cond = cond_arr(0)\n\n      var lastLoadTimestamp: String =\n        if (srcDef.fmt == \"query\" || (srcDef.fmt == \"table\")) {\n          if (Config.incremental_table_cutoff_date != None) {\n            Config.incremental_table_cutoff_date.toString\n          } else {\n            \"1900-01-01\"\n          }\n        } else {\n          \"19000101000000\"\n        }\n      if (\n        spark.catalog.tableExists(\n          Config.incremental_load_metadata_table\n        ) && !accumulated_process_flag\n      ) {\n        val lastLoadTimestamp_values = spark\n          .sql(\n            \"SELECT last_process_timestamp from \" + Config.incremental_load_metadata_table + \" where pipeline_name = '\" + Config.pipeline_name + \"' and target = '\" + Config.target_table + \"'\"\n          )\n          .collect()\n        if (lastLoadTimestamp_values.length > 0) {\n          lastLoadTimestamp = lastLoadTimestamp_values(0)\n            .getString(0)\n          if (srcDef.fmt != \"table\" && srcDef.fmt != \"query\") {\n            if (lastLoadTimestamp == \"-\") {\n              lastLoadTimestamp = \"19000101000000\"\n            }\n          } else {\n            if (lastLoadTimestamp == \"-\") {\n              lastLoadTimestamp = \"1900-01-01\"\n            }\n          }\n        }\n      }\n\n      println(\"lastLoadTimestamp: \" + lastLoadTimestamp)\n\n      if (srcDef.fmt != \"table\" && srcDef.fmt != \"query\") {\n        val all_files = allFiles(srcDef.src)\n          .filter(x => x.endsWith(\".\" + srcDef.fmt))\n\n        val all_file_names = all_files.map(x => x.split(\"//\").last)\n        println(\"all_file_names: \", all_file_names)\n\n        var incrementalFiles =\n          if (!accumulated_process_flag) {\n            all_files.filter { x =>\n              val regex = \"(.*)/.*(\\\\d{4}\\\\d{2}\\\\d{2}\\\\d{2}\\\\d{2}\\\\d{2}).*\".r\n              val regex(dir, timestamp) = x\n              timestamp.toLong > lastLoadTimestamp.toLong\n            }\n          } else {\n            println(\"accumulated_process_flag loop\")\n            if (\n              spark.catalog\n                .tableExists(Config.accumulated_processed_files_table)\n            ) {\n              val processed_files = spark.read\n                .table(Config.accumulated_processed_files_table)\n                .where(\n                  \" pipeline_name = '\" + Config.pipeline_name + \"' and target = '\" + Config.target_table + \"'\"\n                )\n                .select(\"processed_file\")\n                .collect()\n                .map(x => x.getString(0))\n\n              val new_files = (all_files diff processed_files).toList\n              println(\"new_files: \" + new_files.mkString(\",\"))\n              spark.conf\n                .set(\"accoumulated_process_files\", new_files.mkString(\",\"))\n              new_files\n            } else {\n              spark.conf\n                .set(\"accoumulated_process_files\", all_files.mkString(\",\"))\n              all_files\n            }\n\n          }\n\n        if (incrementalFiles.length > 0) {\n          if (\n            srcDef.fmt == \"csv\" || srcDef.fmt == \"txt\" || srcDef.fmt == \"dat\"\n          ) {\n            spark.read\n              .format(\"csv\")\n              .option(\"header\", true)\n              .option(\"sep\", srcDef.delimiter.getOrElse(\",\"))\n              .load(incrementalFiles: _*)\n          } else {\n            if (!Config.enable_read_schema_from_target_table) {\n              spark.read\n                .option(\"mergeSchema\", \"true\")\n                .format(\"parquet\")\n                .load(incrementalFiles: _*)\n            } else {\n              val targetTblSchema =\n                spark.read.table(s\"${Config.target_table_db}.${Config.target_table}\").schema\n\n              spark.read\n                .schema(targetTblSchema)\n                .format(\"parquet\")\n                .load(incrementalFiles: _*)\n            }\n          }\n        } else {\n          null\n        }\n      } else if (srcDef.fmt == \"table\") {\n\n        if (Config.incremental_table_watermark_col == \"None\") {\n          throw new Exception(\n            \"Please define incremental_table_strategy and incremental_table_watermark_col in case you want to read \" +\n              \"data incrementally from table. Alternatively you can put read_incremental_files_flag  as false in case incremetnal \" +\n              \"read is not requried.\"\n          )\n        }\n\n        if (Config.incremental_table_strategy == \"now\") {\n          val inc_cond =\n            Config.incremental_table_watermark_col + \" \" + lower_cond + \" '\" + lastLoadTimestamp +\n              \"' and \" + Config.incremental_table_watermark_col + \" \" + upper_cond + \" '\" + load_filter_time + \"'\"\n          println(\"incrmental_condition now: \" + inc_cond)\n          spark.read\n            .table(srcDef.src)\n            .where(\n              inc_cond\n            )\n            .withColumn(\n              \"file_name_timestamp\",\n              expr(Config.incremental_table_watermark_col).cast(\"string\")\n            )\n        } else if (Config.incremental_table_strategy.startsWith(\"today\")) {\n\n          val delta_days =\n            if (Config.incremental_table_strategy.contains(\"-\")) {\n              Config.incremental_table_strategy.split(\"-\")(1).trim().toInt\n            } else {\n              0\n            }\n\n          val date_filter = LocalDate\n            .parse(load_filter_date, DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))\n            .minusDays(delta_days)\n            .toString\n\n          var end_ts_filter =\n            if (Config.incremental_table_strategy_delta_hours == \"None\") {\n              date_filter\n            } else {\n              date_filter + \" \" + Config.incremental_table_strategy_delta_hours\n            }\n\n          val inc_cond =\n            Config.incremental_table_watermark_col + \" \" + lower_cond + \" '\" + lastLoadTimestamp + \"' and \" + Config.incremental_table_watermark_col + \" \" + upper_cond + \" '\" + end_ts_filter + \"'\"\n          println(\"incrmental_condition today: \" + inc_cond)\n          spark.read\n            .table(srcDef.src)\n            .where(\n              inc_cond\n            )\n            .withColumn(\n              \"file_name_timestamp\",\n              expr(Config.incremental_table_watermark_col).cast(\"string\")\n            )\n        } else {\n          throw new Exception(\n            \"Please define incremental_table_strategy and incremental_table_watermark_col in case you want to read \" +\n              \"data incrementally from table. Alternatively you can put read_incremental_files_flag  as false in case incremental \" +\n              \"read is not requried.\"\n          )\n\n          null\n        }\n\n      } else if (srcDef.fmt == \"query\") {\n        if (Config.incremental_table_watermark_col == \"None\") {\n          throw new Exception(\n            \"Please define incremental_table_strategy and incremental_table_watermark_col in case you want to read \" +\n              \"data incrementally from query. Alternatively you can put read_incremental_files_flag  as false in case incremental \" +\n              \"read is not requried.\"\n          )\n        }\n\n        val loadEndTimestamp = if (Config.incremental_table_strategy == \"now\") {\n          load_filter_time\n        } else if (Config.incremental_table_strategy.startsWith(\"today\")) {\n          val delta_days =\n            if (Config.incremental_table_strategy.contains(\"-\")) {\n              Config.incremental_table_strategy.split(\"-\")(1).trim().toInt\n            } else {\n              0\n            }\n          val date_filter = LocalDate\n            .parse(load_filter_date, DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))\n            .minusDays(delta_days)\n            .toString\n          var end_ts_filter =\n            if (Config.incremental_table_strategy_delta_hours == \"None\") {\n              date_filter\n            } else {\n              date_filter + \" \" + Config.incremental_table_strategy_delta_hours\n            }\n          end_ts_filter\n        } else if (Config.incremental_table_strategy.startsWith(\"ts\")) {\n          // TODO: Check if Config.incremental_table_strategy is of actual Timestamp form\n          val possibleTs =\n            Config.incremental_table_strategy.split(\"ts\")(1).trim()\n          val dateFormatPattern = \"yyyy-MM-dd HH:mm:ss\"\n          val formatter = DateTimeFormatter.ofPattern(dateFormatPattern)\n\n          Try(formatter.parse(possibleTs)) match {\n            case scala.util.Success(_) =>\n              // Date parsing successful\n              println(s\"$possibleTs is in the correct format.\")\n            case scala.util.Failure(_) =>\n              // Date parsing failed\n              throw new Exception(\n                s\"Failed in read_source. $possibleTs is not in the correct format.\"\n              )\n          }\n          possibleTs\n        } else {\n          throw new Exception(\n            \"Please define incremental_table_strategy in case you want to read \" +\n              \"data incrementally from query. Alternatively you can put read_incremental_files_flag  as false in case incremental \" +\n              \"read is not requried.\"\n          )\n          \"\"\n        }\n\n        val where_query =\n          if (Config.incremental_query_replace_strategy == \"query_replace\") {\n            srcDef.src\n              .replace(Config.start_ts_query_repr, s\"'${lastLoadTimestamp}'\")\n              .replace(Config.end_ts_query_repr, s\"'${loadEndTimestamp}'\")\n          } else {\n\n            val initial_query = srcDef.src.replace(\" WHERE \", \" where \")\n\n            val updated_query = if (!initial_query.contains(\" where \")) {\n              initial_query + \" where \"\n            } else {\n              initial_query\n            }\n\n            val where_index = updated_query.lastIndexOf(\" where \")\n\n            val clause_to_insert =\n              Config.incremental_table_watermark_col + \" \" + lower_cond + \" '\" + lastLoadTimestamp +\n                \"' and \" + Config.incremental_table_watermark_col + \" \" + upper_cond + \" '\" + loadEndTimestamp + \"'\"\n\n            val clause_to_insert_final =\n              if (initial_query.contains(\" where \")) {\n                clause_to_insert + \" and \"\n              } else {\n                clause_to_insert\n              }\n            updated_query.substring(\n              0,\n              where_index + 7\n            ) + clause_to_insert_final + \" \" + updated_query.substring(\n              where_index + 7\n            )\n\n          }\n\n        val final_query =\n          \"select cast(\" + Config.incremental_table_watermark_col + \" as string) as file_name_timestamp, \" + where_query\n            .substring(7, where_query.length)\n\n        println(\"final query for reading incremental data: \")\n        println(final_query)\n        spark.sql(final_query)\n      } else {\n        null\n      }\n\n    } else if (\n      srcDef.fmt == \"csv\" || srcDef.fmt == \"txt\" || srcDef.fmt == \"dat\"\n    ) {\n      spark.read\n        .format(\"csv\")\n        .option(\"header\", true)\n        .option(\"sep\", srcDef.delimiter.getOrElse(\",\"))\n        .load(srcDef.src)\n    } else if (srcDef.fmt == \"parquet\") {\n      try {\n        spark.read.format(\"parquet\").load(srcDef.src)\n      } catch {\n        case e: Exception => {\n          null\n        }\n      }\n    } else if (srcDef.fmt == \"query\") {\n      println(\"query for fetching input data: \")\n      println(srcDef.src)\n      spark.sql(srcDef.src)\n    } else {\n      if (Config.read_from_multiple_temp_tables == \"true\") {\n        try {\n          spark.read.table(srcDef.src)\n        } catch {\n          case e: Exception => {\n            null\n          }\n        }\n      } else {\n        println(\"Source table used for fetching input data: \")\n        println(srcDef.src)\n        spark.read.table(srcDef.src)\n      }\n    }\n\n    if (df != null) {\n      var output = df\n      if (srcDef.fmt != \"table\" && srcDef.fmt != \"query\") {\n        output = output\n          .withColumn(\n            \"source_file_full_path\",\n            input_file_name()\n          )\n          .withColumn(\n            \"file_name_timestamp\",\n            regexp_extract(\n              col(\"source_file_full_path\"),\n              \"\\\\d{4}\\\\d{2}\\\\d{2}\\\\d{2}\\\\d{2}\\\\d{2}\",\n              0\n            )\n          )\n          .withColumn(\"source_file_base_path\", lit(srcDef.src))\n\n      } else {\n        if (!output.columns.contains(\"file_name_timestamp\")) {\n          output = output.withColumn(\"file_name_timestamp\", lit(\"-\"))\n        }\n        output = output\n          .withColumn(\"source_file_base_path\", lit(\"-\"))\n          .withColumn(\"source_file_full_path\", lit(\"-\"))\n      }\n      output\n    } else {\n      null\n    }\n\n  })\n  .filter(x => x != null)\n\n// Union all input dataframes if more than one is specified\nval out0 = if (allDFs.length > 0) {\n  val final_df = allDFs.tail.foldLeft(allDFs.head)((df1, df2) => df1.union(df2))\n  if (final_df.rdd.isEmpty) {\n    println(\"read_source: No new data to process. Got empty df.\")\n    spark.conf.set(\"new_data_flag\", \"false\")\n    spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))\n  } else {\n    spark.conf.set(\"new_data_flag\", \"true\")\n    final_df\n  }\n} else {\n  println(\"read_source: No new data to process.\")\n  spark.conf.set(\"new_data_flag\", \"false\")\n  spark.createDataFrame(Seq((\"1\", \"1\"), (\"2\", \"2\")))\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "gztr7dxOl6-dGaE-1at_i$$C8lVxU9VogSJWJJBYwdnn" : {
      "id" : "gztr7dxOl6-dGaE-1at_i$$C8lVxU9VogSJWJJBYwdnn",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_post_filter",
        "slug" : "optional_post_filter",
        "x" : 5220,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "Ij5oZsTkmcpLnUUhZY70w$$y87DSOFfSsOR0-KJGFbE-",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "X__5bzL0HmZxu0HGq7eF3$$So6w5lsV8BsSq1lwQaI03",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"#########################################\")\r\nprintln(\"#####Step name: optional_post_filter#####\")\r\nprintln(\"#########################################\")\r\nprintln(\r\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\r\n    )\r\n\r\nval out0 =\r\n  if (\r\n    Config.target_filter != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\r\n  ) {\r\n    try{\r\n      in0.where(Config.target_filter)\r\n    }catch {\r\n      case e: Exception => {\r\n        println(\r\n          s\"\"\"Process failed while applying post filter\"\"\"\r\n        )\r\n\r\n        if (spark.conf.get(\"main_table_api_type\") == \"NON-API\"){\r\n          import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\r\n          println(s\" Removing lock from: ${Config.sk_table_name_override}\")\r\n          dbutils.fs.rm(Config.sk_lock_file_path + Config.sk_table_name_override + \".txt\")\r\n        }\r\n        throw e\r\n      }\r\n    }\r\n  } else {\r\n    in0\r\n  }\r\n\r\nif (\r\n  Config.target_filter != \"None\" && spark.conf.get(\"new_data_flag\") == \"true\"\r\n) {\r\n  spark.conf.set(\"post_filtered_record_count\", out0.count().toString)\r\n} else {\r\n  spark.conf.set(\"post_filtered_record_count\", spark.conf.get(\"source_count\"))\r\n}\r\n\r\nif(Config.debug_flag) {\r\n  var printDf = out0\r\n  if(Config.debug_filter.toLowerCase() != \"none\"){\r\n    printDf = printDf.where(Config.debug_filter)\r\n  }  \r\n  if(Config.debug_col_list.toLowerCase() != \"none\"){\r\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\r\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\r\n  } else {\r\n    printDf.show(truncate=true)\r\n  }\r\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "tw8EKT2W2iowee5qrJ_JH$$NAdCUULfzOI44Hkq_jAYL" : {
      "id" : "tw8EKT2W2iowee5qrJ_JH$$NAdCUULfzOI44Hkq_jAYL",
      "component" : "Script",
      "metadata" : {
        "label" : "optional_joins",
        "slug" : "optional_joins",
        "x" : 2020,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "BcE7nKlVBYdQlyl0maTaH$$gB-qQZ-PobSe4gjBbXh5Q",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "d5DR6BWsfXSRiM2NF07RD$$PvSPv3sfyXEAeHXxyl6Tc",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "import java.time._\nimport java.time.format._\n\nprintln(\"#####S#############################\")\nprintln(\"#####Step name: optional_joins#####\")\nprintln(\"#####S#############################\")\nprintln(\n      \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n    ) \nval out0 =\n  if (\n    (Config.reformat_rules_join != \"None\" || Config.optional_filter_rules_join != \"None\") && spark.conf\n      .get(\n        \"new_data_flag\"\n      ) == \"true\"\n  ) {\n    import _root_.io.prophecy.abinitio.ScalaFunctions._\n    import _root_.io.prophecy.libs._\n    import pureconfig._\n    import pureconfig.generic.auto._\n    import scala.language.implicitConversions\n    import org.apache.spark.storage.StorageLevel\n    import scala.collection.mutable.ListBuffer\n\n    registerAllUDFs(spark: SparkSession)\n\n    var encrypt_key = \"Secret not defined\"\n    var encrypt_iv = \"Secret not defined\"\n    var decrypt_key = \"Secret not defined\"\n    var decrypt_iv = \"Secret not defined\"\n\n    import java.security.MessageDigest\n    import java.util\n    import javax.crypto.Cipher\n    import javax.crypto.spec.{IvParameterSpec, SecretKeySpec}\n    import org.apache.commons.codec.binary.Hex\n    import org.apache.spark.sql.functions._\n\n    try {\n\n//Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\n      import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n      // loading db secrets\n      encrypt_key = dbutils.secrets.get(\n        scope = Config.encrypt_scope,\n        key = Config.encrypt_EncKey\n      )\n\n      encrypt_iv = dbutils.secrets.get(\n        scope = Config.encrypt_scope,\n        key = Config.encrypt_InitVec\n      )\n\n      decrypt_key = dbutils.secrets.get(\n        scope = Config.decrypt_scope,\n        key = Config.decrypt_EncKey\n      )\n\n      decrypt_iv = dbutils.secrets.get(\n        scope = Config.decrypt_scope,\n        key = Config.decrypt_InitVec\n      )\n      println(\"Found secrets successfully for encrypt decrypt UDFs.\")\n    } catch {\n      case e: Exception => {\n        println(\n          \"Please define databricks secrets for encrypt_key, decrypt_key, envrypt_iv and decrypt_iv on your cluster to use encrypt decrypt functions as udf\"\n        )\n      }\n    }\n\n    def encrypt(key: String, ivString: String, plainValue: String): String = {\n      if (plainValue != null) {\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n        cipher.init(Cipher.ENCRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\n        var encrypted_str =\n          Hex.encodeHexString(cipher.doFinal(plainValue.getBytes(\"UTF-8\")))\n        encrypted_str = encrypted_str\n        return encrypted_str\n      } else {\n        null\n      }\n    }\n\n    def decrypt(\n        key: String,\n        ivString: String,\n        encryptedValue: String\n    ): String = {\n      if (encryptedValue != null) {\n        val cipher: Cipher = Cipher.getInstance(\"AES/OFB/PKCS5Padding\")\n        cipher.init(Cipher.DECRYPT_MODE, keyToSpec(key), getIVSpec(ivString))\n        new String(cipher.doFinal(Hex.decodeHex(encryptedValue.toCharArray())))\n      } else {\n        null\n      }\n    }\n\n    def keyToSpec(key: String): SecretKeySpec = {\n      var keyBytes: Array[Byte] = (key).getBytes(\"UTF-8\")\n      keyBytes = Hex.decodeHex(key)\n      keyBytes = util.Arrays.copyOf(keyBytes, 16)\n      new SecretKeySpec(keyBytes, \"AES\")\n    }\n\n    def getIVSpec(IVString: String) = {\n      new IvParameterSpec(IVString.getBytes() ++ Array.fill[Byte](16-IVString.length)(0x00.toByte))\n\n    }\n\n    val encryptUDF = udf(encrypt _)\n    val decryptUDF = udf(decrypt _)\n    spark.udf.register(\"aes_encrypt_udf\", encryptUDF)\n    spark.udf.register(\"aes_decrypt_udf\", decryptUDF)\n\n    println(\"Registered encrypt and decrypt UDFs in joins.\")\n\n    var ff3_encrypt_key = \"Secret not defined\"\n    var ff3_encrypt_tweak = \"Secret not defined\"\n\n    try {\n\n      // Encryption method for obfuscating PHI / PII fields defined in config encryptColumns\n      import com.databricks.dbutils_v1.DBUtilsHolder.dbutils\n\n      // loading db secrets\n      ff3_encrypt_key = dbutils.secrets.get(\n        scope = Config.ff3_encrypt_scope,\n        key = Config.ff3_encrypt_key\n      )\n\n      ff3_encrypt_tweak = dbutils.secrets.get(\n        scope = Config.ff3_encrypt_scope,\n        key = Config.ff3_encrypt_tweak\n      )\n    } catch {\n      case e: Exception => {\n        println(\n          \"Please define databricks secrets for ff3_encrypt_key and ff3_encrypt_tweak on your cluster to use ff3_encrypt_idwdata as udf\"\n        )\n      }\n    }\n\n    // case class to define source\n    case class JoinDef(\n        fmt: String,\n        src: String,\n        joinCond: String,\n        valCols: List[String],\n        joinType: Option[String] = Some(\"left\"),\n        filterCond: Option[String] = Some(\"true\"),\n        joinHint: Option[String] = None,\n        delimiter: Option[String] = Some(\",\")\n    )\n\n    case class Joins(\n        joins: List[JoinDef]\n    )\n\n    var joinsConfig = List[JoinDef]()\n\n    if (Config.reformat_rules_join != \"None\") {\n      joinsConfig = joinsConfig ++\n        ConfigSource\n          .string(Config.reformat_rules_join.replace(\"\\n\", \" \"))\n          .loadOrThrow[Joins].joins\n    }\n\n    if (Config.optional_filter_rules_join != \"None\") {\n      joinsConfig = joinsConfig ++\n        ConfigSource\n          .string(Config.optional_filter_rules_join.replace(\"\\n\", \" \"))\n          .loadOrThrow[Joins].joins\n    }\n\n    var res1 = if (Config.source_join_hint != \"None\") {\n      in0.hint(Config.source_join_hint)\n    } else {\n      in0\n    }\n    var res2 = if (Config.source_join_hint != \"None\") {\n      in0.hint(Config.source_join_hint)\n    } else {\n      in0\n    }\n    val joinDf = joinsConfig.foreach { joinDef =>\n      println(\"optional_join: \" + joinDef.src)\n      println(\n        Instant\n          .now()\n          .atZone(ZoneId.of(\"America/Chicago\"))\n          .format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n          .toString\n      )\n      var df =\n        if (joinDef.fmt == \"csv\") {\n          spark.read\n            .format(\"csv\")\n            .option(\"header\", true)\n            .option(\"sep\", joinDef.delimiter.getOrElse(\",\"))\n            .load(joinDef.src)\n        } else if (joinDef.fmt == \"parquet\") {\n          spark.read.format(\"parquet\").load(joinDef.src)\n        } else if (joinDef.fmt == \"delta\") {\n          spark.read.format(\"delta\").load(joinDef.src)\n        } else if (joinDef.fmt == \"query\") {\n          spark.sql(joinDef.src)\n        } else {\n          spark.read.table(joinDef.src)\n        }\n      val final_cols =\n        List(\"in0.*\") ++ joinDef.valCols.map(x =>\n          x.replace(\"SSSZ\", \"SSS\")\n            .replace(\n              \"STRING), 1, 20), 'T', -1), ' '), 'yyyy-MM-dd HH:mm:ss.'), \",\n              \"STRING), 1, 20), 'T', -1), ' '), 'yyyy-MM-dd HH:mm:ss'), \"\n            )\n            .replace(\n              \"aes_encrypt_udf(\",\n              \"aes_encrypt_udf('\" + encrypt_key + \"', '\" + encrypt_iv + \"', \"\n            )\n            .replace(\n              \"aes_decrypt_udf(\",\n              \"aes_decrypt_udf('\" + decrypt_key + \"', '\" + decrypt_iv + \"', \"\n            )\n            .replace(\n              \"ff3_encrypt_idwdata(\",\n              \"ff3_encrypt_idwdata_new('\" + ff3_encrypt_key + \"', '\" + ff3_encrypt_tweak + \"', \"\n            )\n        )\n      val join_condition = expr(\n        joinDef.joinCond\n          .replace(\"SSSZ\", \"SSS\")\n          .replace(\n            \"STRING), 1, 20), 'T', -1), ' '), 'yyyy-MM-dd HH:mm:ss.'), \",\n            \"STRING), 1, 20), 'T', -1), ' '), 'yyyy-MM-dd HH:mm:ss'), \"\n          )\n          .replace(\n            \"aes_encrypt_udf(\",\n            \"aes_encrypt_udf('\" + encrypt_key + \"', '\" + encrypt_iv + \"', \"\n          )\n          .replace(\n            \"aes_decrypt_udf(\",\n            \"aes_decrypt_udf('\" + decrypt_key + \"', '\" + decrypt_iv + \"', \"\n          )\n          .replace(\n            \"ff3_encrypt_idwdata(\",\n            \"ff3_encrypt_idwdata_new('\" + ff3_encrypt_key + \"', '\" + ff3_encrypt_tweak + \"', \"\n          )\n      )\n\n      if (joinDef.filterCond.get.toLowerCase().contains(\"partition by\")) {\n        val where_cond = joinDef.filterCond.get\n        val window_arr = where_cond.split(\"==\")\n        df = df\n          .withColumn(\"window_temp_col\", expr(window_arr(0).trim()))\n          .where(\"window_temp_col == \" + window_arr(1).trim())\n          .drop(\"window_temp_col\")\n      } else {\n        df = df.where(joinDef.filterCond.get)\n      }\n      if (joinDef.joinHint != None) {\n        res1 = res2\n          .as(\"in0\")\n          .join(\n            df.as(\"in1\")\n              .hint(joinDef.joinHint.get),\n            join_condition,\n            joinDef.joinType.get\n          )\n          .selectExpr(final_cols: _*)\n      } else {\n        res1 = res2\n          .as(\"in0\")\n          .join(\n            df.as(\"in1\"),\n            join_condition,\n            joinDef.joinType.get\n          )\n          .selectExpr(final_cols: _*)\n\n        if (Config.join_persist_flag == \"true\") {\n          res1 = res1.persist(StorageLevel.DISK_ONLY)\n          res1.count()\n          res2.unpersist()\n        }\n        res2 = res1\n        if(Config.debug_flag) {\n          println(s\"For Join with ${joinDef.src}, COUNT = ${res2.count()}\")\n        }\n      }\n      println(\n        Instant\n          .now()\n          .atZone(ZoneId.of(\"America/Chicago\"))\n          .format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n          .toString\n      )\n\n    }\n    val join_output_count = res2.count()\n    println(\"join_output_count: \" + join_output_count)\n    spark.conf.get(\"join_output_count\", join_output_count.toString)\n    val source_filter_out_count = spark.conf.get(\"source_filter_count\")\n    println(\"source_filter_out_count: \" + source_filter_out_count)\n    if (join_output_count != source_filter_out_count.toLong){\n      println(\"Please check your join conditions / lookup data for duplicates on joining condition. Some join is not happening 1 to 1.\")\n    }\n    res2\n  } else {\n    in0\n  }\n\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "RBBqUr4dpZNVnm5tWNW8D$$NdUDYQv2OrgDNHD2VJ0cB" : {
      "id" : "RBBqUr4dpZNVnm5tWNW8D$$NdUDYQv2OrgDNHD2VJ0cB",
      "component" : "Script",
      "metadata" : {
        "label" : "split_load_ready_files",
        "slug" : "split_load_ready_files",
        "x" : 6420,
        "y" : 120,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "fFKjdmCxFbuszWWfZTXvU$$CoidSZAlGQa1bqsxjKP5-",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "JSu35un4kuT_9Yc2pJAJG$$X6YMlCwsxv3WQNiZmzgJr",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        }, {
          "id" : "huMv9i577INmS1q__fkI3$$KSngA5cXQhw-iN-A_l6E0",
          "slug" : "out1",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "println(\"###########################################\")\nprintln(\"#####Step name: split_load_ready_files#####\")\nprintln(\"###########################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nvar out0 = in0\nvar out1 = in0\n\nspark.conf.set(\"insert_load_ready_count\", \"0\")\nspark.conf.set(\"update_load_ready_count\", \"0\")\nif (\n  (Config.skip_delta_synapse_write_common_dimension == \"true\" && spark.conf\n    .get(\n      \"new_data_flag\"\n    ) == \"true\" && Config.generate_load_ready_files == \"true\" && Config.skip_main_table_load_ready_files == \"false\")\n) {\n  out0 = in0\n  spark.conf.set(\"insert_load_ready_count\", out0.count().toString)\n  out1 = in0.where(\"1==2\")\n} else if (\n  (Config.generate_load_ready_files == \"true\" && spark.conf.get(\n    \"new_data_flag\"\n  ) == \"true\" && Config.skip_main_table_load_ready_files == \"false\") || (Config.generate_only_load_ready_files == \"true\" && Config.skip_main_table_load_ready_files == \"false\")\n) {\n  out0 = in0.filter(col(\"_change_type\") === lit(\"insert\"))\n  spark.conf.set(\"insert_load_ready_count\", out0.count().toString)\n  out1 = in0.filter(col(\"_change_type\") === lit(\"update_postimage\"))\n  spark.conf.set(\"update_load_ready_count\", out1.count().toString)\n}\n",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): (DataFrame, DataFrame) = {",
        "scriptMethodFooter" : "    (out0, out1)\n}"
      }
    },
    "Ga5tBdfNKuRKxtlMvtMkO$$dvthpkrQFi1kYd2YNtkET" : {
      "id" : "Ga5tBdfNKuRKxtlMvtMkO$$dvthpkrQFi1kYd2YNtkET",
      "component" : "Script",
      "metadata" : {
        "label" : "apply_dedup_rules",
        "slug" : "apply_dedup_rules",
        "x" : 4020,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "awF0Yf57r8e6_EKB_p2xy$$UACVQwkS7qDkKXnsE0E0V",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "4GEbpr4gEFd-MvrzVvSu5$$GnuKRlJI-N_fm-Dmx9hVX",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true,
        "autoUpdateOnRun" : false
      },
      "properties" : {
        "script" : "// deduplicate on natural keys / adhoc configuration\nprintln(\"######################################\")\nprintln(\"#####Step name: apply_dedup_rules#####\")\nprintln(\"######################################\")\nprintln(\n  \"step start time: \" + Instant.now().atZone(ZoneId.of(\"America/Chicago\"))\n)\n\nval out0 =\n  if (\n    Config.disable_dedup_rules != \"true\" && spark.conf.get(\n      \"new_data_flag\"\n    ) == \"true\"\n  ) {\n    val isPKSet = spark.conf.getAll.contains(\"primary_key\")\n    val prim_key_columns = if (isPKSet) {\n      spark.conf\n        .get(\"primary_key\")\n        .split(\",\")\n        .map(x => x.trim())\n    } else {\n      Config.primary_key\n        .split(\",\")\n        .map(x => x.trim())\n    }\n\n    var col_values = List[String]()\n\n    var col_map = Map[String, String]()\n\n    if (Config.final_table_schema != \"None\") {\n\n      import org.json4s._\n      import org.json4s.jackson.JsonMethods._\n      import scala.collection.mutable.ListBuffer\n      import spark.sqlContext.implicits._\n\n      def jsonStrToMap(jsonStr: String): Map[String, String] = {\n        implicit val formats: DefaultFormats.type = org.json4s.DefaultFormats\n        parse(jsonStr).extract[Map[String, String]]\n      }\n\n      col_map = jsonStrToMap(Config.final_table_schema)\n\n      col_values = col_map.keySet.toList\n\n    }\n\n// default null natural keys column to blank to avoid duplicates\n    val new_columns = in0.columns.map { x =>\n      if (prim_key_columns.contains(x) && col_values.contains(x)) {\n        if ((!x.endsWith(\"_dt_sk\")) && (x.endsWith(\"_sk\"))) {\n          when(col(x).isNull, lit(\"-9090\"))\n            .otherwise(col(x))\n            .cast(col_map.get(x).get)\n            .as(x)\n        } else if (col_map.get(x).get.startsWith(\"decimal\")) {\n          when(col(x).isNull, lit(\"0\"))\n            .otherwise(col(x))\n            .cast(col_map.get(x).get)\n            .as(x)\n        } else if (col_map.get(x).get.startsWith(\"timestamp\")) {\n          when(\n            col(x).isNull,\n            lit(\"1900-01-01 00:00:00\")\n          )\n            .otherwise(col(x))\n            .cast(col_map.get(x).get)\n            .as(x)\n        } else if (col_map.get(x).get.startsWith(\"date\")) {\n          when(col(x).isNull, lit(\"1900-01-01\"))\n            .otherwise(col(x))\n            .cast(col_map.get(x).get)\n            .as(x)\n        } else if (col_map.get(x).get.startsWith(\"string\")) {\n          when(\n            col(x).isNull || (col(x).cast(\"string\") === lit(\"\")),\n            lit(\"-\")\n          )\n            .otherwise(trim(col(x)))\n            .as(x)\n        } else {\n          when(col(x).isNull, lit(\"0\").cast(col_map.get(x).get))\n            .otherwise(col(x))\n            .as(x)\n        }\n      } else { col(x) }\n    }\n\n    val temp_df = in0.select(new_columns: _*)\n    import org.apache.spark.sql.expressions.Window\n    if (Config.dedup_rules != \"None\") {\n      import org.json4s._\n      import org.json4s.jackson.JsonMethods._\n      import scala.collection.mutable.ListBuffer\n\n      def jsonStrToMap(jsonStr: String): Map[String, String] = {\n        implicit val formats: DefaultFormats.type = org.json4s.DefaultFormats\n        parse(jsonStr).extract[Map[String, String]]\n      }\n\n      val map_values = jsonStrToMap(Config.dedup_rules)\n\n      val typeToKeep = map_values.get(\"dedupType\").get\n      val groupByColumns =\n        map_values.get(\"dedupColumns\").get.split(',').map(x => x.trim()).toList\n\n      if (typeToKeep == \"first\" && map_values.contains(\"orderByColumns\")){\n        import scala.collection.mutable.ListBuffer\n        println(\"Deduplicate strategy: \", typeToKeep)\n        var outputList = new ListBuffer[org.apache.spark.sql.Column]()\n        val order_by_rules = map_values.get(\"orderByColumns\").get.split(',').map(x => x.trim())\n\n        order_by_rules.foreach { rule =>\n          if (rule.toLowerCase().contains(\" asc\")) {\n            if (rule.toLowerCase().contains(\" asc nulls\")) {\n              if (rule.toLowerCase().contains(\"nulls first\")) {\n                outputList += asc_nulls_first(rule.split(\" \")(0).trim())\n              } else {\n                outputList += asc_nulls_last(rule.split(\" \")(0).trim())\n              }\n            } else {\n              outputList += asc(rule.split(\" \")(0).trim())\n\n            }\n          } else {\n            if (rule.toLowerCase().contains(\" desc nulls\")) {\n              if (rule.toLowerCase().contains(\"nulls first\")) {\n                outputList += desc_nulls_first(rule.split(\" \")(0).trim())\n              } else {\n                outputList += desc_nulls_last(rule.split(\" \")(0).trim())\n              }\n            } else {\n              outputList += desc(rule.split(\" \")(0).trim())\n            }\n\n          }\n        }\n\n        outputList += col(\"file_name_timestamp\").desc_nulls_last\n        val window = Window\n          .partitionBy(groupByColumns.map(x => col(x)): _*)\n          .orderBy(\n            outputList: _*\n          )\n        temp_df\n          .withColumn(\"dedup_row_num\", row_number().over(window))\n          .where(\"dedup_row_num == 1\")\n          .drop(\"dedup_row_num\")\n      } else{\n        println(\"Deduplicate strategy: \", typeToKeep)\n        temp_df.dropDuplicates(groupByColumns)\n      }   \n    } else {\n      if (prim_key_columns.length > 0 && Config.dedup_columns != \"None\") {\n        import scala.collection.mutable.ListBuffer\n        var outputList = new ListBuffer[org.apache.spark.sql.Column]()\n        val order_by_rules = Config.dedup_columns.split(',').map(x => x.trim())\n\n        order_by_rules.foreach { rule =>\n          if (rule.toLowerCase().contains(\" asc\")) {\n            if (rule.toLowerCase().contains(\" asc nulls\")) {\n              if (rule.toLowerCase().contains(\"nulls first\")) {\n                outputList += asc_nulls_first(rule.split(\" \")(0).trim())\n              } else {\n                outputList += asc_nulls_last(rule.split(\" \")(0).trim())\n              }\n            } else {\n              outputList += asc(rule.split(\" \")(0).trim())\n\n            }\n          } else {\n            if (rule.toLowerCase().contains(\" desc nulls\")) {\n              if (rule.toLowerCase().contains(\"nulls first\")) {\n                outputList += desc_nulls_first(rule.split(\" \")(0).trim())\n              } else {\n                outputList += desc_nulls_last(rule.split(\" \")(0).trim())\n              }\n            } else {\n              outputList += desc(rule.split(\" \")(0).trim())\n            }\n\n          }\n        }\n\n        outputList += col(\"file_name_timestamp\").desc_nulls_last\n        val window = Window\n          .partitionBy(prim_key_columns.map(x => col(x)): _*)\n          .orderBy(\n            outputList: _*\n          )\n        temp_df\n          .withColumn(\"dedup_row_num\", row_number().over(window))\n          .where(\"dedup_row_num == 1\")\n          .drop(\"dedup_row_num\")\n      } else if (prim_key_columns.length > 0) {\n        if (in0.columns.contains(\"src_env_rnk\")) {\n          val window = Window\n            .partitionBy(prim_key_columns.map(x => col(x)): _*)\n            .orderBy(\n              col(\"src_env_rnk\").cast(LongType).asc_nulls_last,\n              col(\"file_name_timestamp\").desc_nulls_last\n            )\n          temp_df\n            .withColumn(\"dedup_row_num\", row_number().over(window))\n            .where(\"dedup_row_num == 1\")\n            .drop(\"dedup_row_num\")\n        } else {\n          val window = Window\n            .partitionBy(prim_key_columns.map(x => col(x)): _*)\n            .orderBy(col(\"file_name_timestamp\").desc_nulls_last)\n          temp_df\n            .withColumn(\"dedup_row_num\", row_number().over(window))\n            .where(\"dedup_row_num == 1\")\n            .drop(\"dedup_row_num\")\n        }\n      } else {\n        temp_df\n      }\n\n    }\n  } else {\n    in0\n  }\n\nif (spark.conf.get(\"new_data_flag\") == \"true\") {\n  val duplicate_filtered_record_count = out0.count().toString\n  println(\"duplicate_filtered_record_count: \" + duplicate_filtered_record_count)\n  spark.conf.set(\n    \"duplicate_filtered_record_count\",\n    duplicate_filtered_record_count\n  )\n} else {\n  spark.conf.set(\"duplicate_filtered_record_count\", \"0\")\n}\n\nif(Config.debug_flag) {\n  var printDf = out0\n  if(Config.debug_filter.toLowerCase() != \"none\"){\n    printDf = printDf.where(Config.debug_filter)\n  }  \n  if(Config.debug_col_list.toLowerCase() != \"none\"){\n    val print_cols = Config.debug_col_list.split(\",\").map(x => x.trim())\n    printDf.selectExpr(print_cols : _*).show(truncate=false)\n  } else {\n    printDf.show(truncate=true)\n  }\n}",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    }
  },
  "ports" : {
    "inputs" : [ ],
    "outputs" : [ ],
    "selectedInputFields" : [ ],
    "isCustomOutputSchema" : false,
    "autoUpdateOnRun" : false
  },
  "diagnostics" : [ {
    "property" : "$.workflow.processes.WPwvwUYSyzyGJwGrxog7E$$dila5inO1kk-iAWbfki_0.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.jZeb21Pxoz4Lv3giVtLPX$$zkvp8Lr4YXAN4xK-fN_To.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.qCyo_ZpxGMT3wM4FUCkB8$$6S2ukC40_BY5laPMcQtRv.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.YNgpozNBwPuffrvW5YU9j$$raZHITPDu9IJ8awefPWUu.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.J6fjz0hQY29uv10LLQ6hU$$CK_Nl_BZsSbAUx2nAW4Ia.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.MLZ2fgQ3msTF0GOvhUqD-$$86vbeJZRRg8AtLTmckeHR.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.b5iLKRgznTuJnTToFoHFs$$10R6GPkgJMTqSZvlQno0S.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.0XsmhQ5Tbs3kFP8nEhjFc$$Zpo0i3bzAR2ZaT2BxsSFu.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.tw8EKT2W2iowee5qrJ_JH$$NAdCUULfzOI44Hkq_jAYL.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.O_ToGlviXaR1cWzi7yApg$$WOHgnFUDO0ao0YqcmToJd.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.x6-e9o7PmYi74M-VxDFOS$$1twtczQfqc0UZllW4Pive.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.U97KPDJIcP5qKUhQ9njTG$$J84QZgcLBgqq5JdAvGh-S.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.sLG-1tclvkxeUSOB_ZyrA$$5e57JAQoONa1dWQ6TtbhJ.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.IVOE4eqMDCX0iAK57gJbn$$zeEdlgdzuuniA3bfd6F6A.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.EnfQ7uWDAR9PQgbLcfbip$$eizUFBt06Keq1uCYDN22P.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.19Es0MHx1pfrBprQj5hG7$$S64p_J0adIrQbxOJnJ8fO.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.-HWhIbTJNxycZcwI9Sq3w$$SJAnEcpJS0Q5NicBUqB7i.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.Ga5tBdfNKuRKxtlMvtMkO$$dvthpkrQFi1kYd2YNtkET.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.JB32GmgXxUskZ_V-wAb81$$faMNpiQq5PZsq8jK-xAbo.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.y-m6OkI8CIOyv0kGlSaK9$$Ss9JYcUtFkU6YmYdrBy9z.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.QHWoT3ZPQuf1QukUU12ho$$z-PZk030XyvJVf9kka-AN.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.i2C9DDNannmhRMhfhU5AE$$oKDJLFTWId6o45uYOIzFi.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.iLCBWNwlU4rZ-zrD39NlO$$pYmIZTlRyNwK02U3cBgEU.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.gztr7dxOl6-dGaE-1at_i$$C8lVxU9VogSJWJJBYwdnn.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.SP3GIx64LeRuO7vfX5COW$$zswxLvkA2uhHN3zr6zrKE.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.MZXjVgsTlFrCOsgbrh6tI$$bgaHPzaXnVAHqudoIFwIt.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.bQ1XrnLqKirtJc3djEyNV$$mRnIsUCOUdF2KfuRJI0XC.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.VMN3zOsqy8XcvcScQl2KW$$tIbzBsKedXk0HnTmrZHWm.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.vKNfiHSvwz-ASGB0rmEVI$$LZPS24kdDgRr1_JaOpRjw.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.RBBqUr4dpZNVnm5tWNW8D$$NdUDYQv2OrgDNHD2VJ0cB.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.Ij_U4Axq1dHCVC-2pIWvC$$BAYL6HtkUYT8c0-H07jNw.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.uBR1aQ01XBKZLCyNC-N0I$$yXxFyuwiwRcKUmfdRF6xY.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.Hgf5DAS1R-FD_dKZeOdmd$$CrPq1G1VlHpSx5GydDGbO.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.btQqSKid5gvTBDzik0I7b$$t_OXMCoK5vibXnH2FDC8D.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  }, {
    "property" : "$.workflow.processes.nJN6hRRk2RrSF8R-1W3i1$$EeMkGvgQAf4igaTg_5TtH.ports.outputs",
    "range" : {
      "start" : {
        "line" : 0,
        "character" : 0
      },
      "end" : {
        "line" : 0,
        "character" : 0
      }
    },
    "severity" : 3,
    "message" : "Please infer output schema from the cluster as described [here](https://docs.prophecy.io/low-code-spark/gems/custom/script/)",
    "tags" : [ ],
    "relatedInformation" : [ ],
    "diagnosticType" : 2
  } ]
}